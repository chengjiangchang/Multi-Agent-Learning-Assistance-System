import pandas as pd
import numpy as np
import json
import random
import math
import os
import sys
import asyncio
import argparse
import subprocess
from collections import defaultdict

# --- 0. åŠ¨æ€å®‰è£…ç¼ºå¤±çš„ä¾èµ– (å¦‚æœéœ€è¦) ---
try:
    import grpc
except ImportError:
    print("æœªæ£€æµ‹åˆ° grpcioï¼Œæ­£åœ¨å°è¯•è‡ªåŠ¨å®‰è£…...")
    try:
        subprocess.check_call([
            sys.executable, "-m", "pip", "install", 
            "grpcio==1.62.2", 
            "--index-url", "https://pypi.tuna.tsinghua.edu.cn/simple",
            "--trusted-host", "pypi.tuna.tsinghua.edu.cn"
        ])
        print("grpcio å®‰è£…æˆåŠŸï¼")
    except subprocess.CalledProcessError as e:
        print(f"è‡ªåŠ¨å®‰è£… grpcio å¤±è´¥: {e}")
        print("è¯·æ‰‹åŠ¨åœ¨è™šæ‹Ÿç¯å¢ƒä¸­è¿è¡Œ 'pip install grpcio==1.62.2'ã€‚")
        sys.exit(1)


from tqdm import tqdm
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, f1_score, classification_report, log_loss
from rouge_score import rouge_scorer
import matplotlib.pyplot as plt


# --- Agent Model Config ---
MODEL_NAME = "qwen-plus"  # ä½¿ç”¨ Qwen-Plus æ¨¡å‹


# --- 1. è®¾ç½®é¡¹ç›®è·¯å¾„ ---
def setup_project_path():
    """
    è·å–é¡¹ç›®æ ¹ç›®å½•ï¼ˆå»æ‰äº†æ—§çš„ yl_data_process æŸ¥æ‰¾é€»è¾‘ï¼‰
    """
    try:
        current_path = os.path.dirname(os.path.abspath(__file__))
    except NameError:
        current_path = os.getcwd()
    
    # é¡¹ç›®æ ¹ç›®å½•æ˜¯ Code æ–‡ä»¶å¤¹çš„çˆ¶ç›®å½•
    project_root = os.path.dirname(current_path)
    print(f"é¡¹ç›®æ ¹ç›®å½•: {project_root}")
    return project_root

# è®¾ç½®è·¯å¾„
PROJECT_ROOT = setup_project_path()

# å¯¼å…¥ LLM å·¥å…·å‡½æ•°
from llms.qwen import user_sys_call as user_sys_call_with_model

# å»¶è¿Ÿå¯¼å…¥æŒæ¡åº¦è¯„ä¼°ç»“æœï¼ˆåœ¨ç¡®ä¿è·¯å¾„è®¾ç½®ä¹‹åï¼‰
ASSESSMENT_RESULTS_PATH = None
if PROJECT_ROOT:
    ASSESSMENT_RESULTS_PATH = os.path.join(
        PROJECT_ROOT,
        'results/mastery_assessment_results.csv'
    )


# --- 2. æ•°æ®åŠ è½½ä¸é¢„å¤„ç† ---
def load_and_preprocess_data(project_root):
    """
    åŠ è½½æ‰€æœ‰CSVæ•°æ®å¹¶å°†å…¶é¢„å¤„ç†ä¸ºæŒ‰å­¦ç”ŸIDåˆ†ç»„çš„æ—¥å¿—ã€‚
    """
    print("\n" + "="*80)
    print("ğŸ”„ é˜¶æ®µ 1/3: æ•°æ®åŠ è½½ä¸é¢„å¤„ç†".center(80))
    print("="*80)
    data_path = os.path.join(project_root, 'data/')
    
    try:
        questions_df = pd.read_csv(os.path.join(data_path, "Questions.csv"))
        question_choices_df = pd.read_csv(os.path.join(data_path, "Question_Choices.csv"))
        question_kc_relationships_df = pd.read_csv(os.path.join(data_path, "Question_KC_Relationships.csv"))
        transactions_df = pd.read_csv(os.path.join(data_path, "Transaction.csv"))
        kcs_df = pd.read_csv(os.path.join(data_path, "KCs.csv"))
        kc_relationships_df = pd.read_csv(os.path.join(data_path, "KC_Relationships.csv"))
        print("æ‰€æœ‰æ•°æ®æ–‡ä»¶åŠ è½½æˆåŠŸï¼")
    except FileNotFoundError as e:
        print(f"åŠ è½½æ–‡ä»¶æ—¶å‡ºé”™: {e}")
        sys.exit(1)

    # åˆå¹¶æ•°æ®
    merged_df = pd.merge(transactions_df, questions_df[['id', 'question_text']], left_on='question_id', right_on='id', how='left')
    merged_df = merged_df.rename(columns={'question_text': 'exer_content', 'answer_state': 'score'}).drop(columns=['id_y'])
    
    kc_id_to_name_map = kcs_df.set_index('id')['name']
    question_to_kc_map = question_kc_relationships_df.drop_duplicates(subset=['question_id']).copy()
    question_to_kc_map['know_name'] = question_to_kc_map['knowledgecomponent_id'].map(kc_id_to_name_map)

    full_question_kc_map = question_kc_relationships_df.copy()
    full_question_kc_map['know_name'] = full_question_kc_map['knowledgecomponent_id'].map(kc_id_to_name_map)
    kc_to_questions_map = (
        full_question_kc_map.dropna(subset=['know_name'])
        .groupby('know_name')['question_id']
        .apply(lambda x: list(dict.fromkeys(x.tolist())))
        .to_dict()
    )
    question_text_map = questions_df.set_index('id')['question_text'].fillna('').to_dict()
    
    student_logs_df = pd.merge(merged_df, question_to_kc_map[['question_id', 'know_name']], on='question_id', how='left')
    student_logs_df['score'] = student_logs_df['score'].astype(int)

    # æŒ‰å­¦ç”Ÿåˆ†ç»„
    all_student_records = {
        sid: recs.sort_values(by='start_time').reset_index(drop=True)
        for sid, recs in student_logs_df.groupby('student_id')
    }
    
    min_records = 10
    all_student_records = {sid: recs for sid, recs in all_student_records.items() if len(recs) >= min_records}
    print(f"âœ… æ•°æ®é¢„å¤„ç†å®Œæˆ")
    print(f"   ğŸ“Š ç­›é€‰åå­¦ç”Ÿæ•°: {len(all_student_records)} å")
    print(f"   ğŸ“ æœ€å°è®°å½•æ•°é˜ˆå€¼: {min_records} æ¡")

    kc_descriptions = kcs_df.set_index('name')['description'].fillna('').to_dict()
    
    return all_student_records, kcs_df, kc_relationships_df, question_to_kc_map, questions_df, kc_to_questions_map, question_text_map, kc_descriptions, question_choices_df


def load_mastery_assessment_results(results_path, target_student_ids=None):
    """
    åŠ è½½æŒæ¡åº¦è¯„ä¼°ç»“æœï¼Œå¹¶æ ¹æ®ç›®æ ‡å­¦ç”Ÿç­›é€‰ã€‚
    è¿”å›ç»“æ„: {student_id: {kc_name: {"mastery_level": str, "rationale": str, "suggestions": str}}}
    """
    if not results_path or not os.path.exists(results_path):
        print("æœªæ‰¾åˆ°æŒæ¡åº¦è¯„ä¼°ç»“æœæ–‡ä»¶ï¼Œå°†è·³è¿‡æŒæ¡åº¦ä¸Šä¸‹æ–‡å¢å¼ºå®éªŒã€‚")
        return {}

    try:
        mastery_df = pd.read_csv(results_path)
    except Exception as e:
        print(f"åŠ è½½æŒæ¡åº¦è¯„ä¼°ç»“æœå¤±è´¥: {e}")
        return {}

    if 'student_id' not in mastery_df.columns or 'kc_name' not in mastery_df.columns:
        print("æŒæ¡åº¦è¯„ä¼°ç»“æœç¼ºå°‘å¿…è¦åˆ— (student_id, kc_name)ï¼Œå°†è·³è¿‡æŒæ¡åº¦ä¸Šä¸‹æ–‡å¢å¼ºå®éªŒã€‚")
        return {}

    if target_student_ids is not None:
        mastery_df = mastery_df[mastery_df['student_id'].isin(target_student_ids)]

    mastery_lookup = defaultdict(dict)
    for _, row in mastery_df.iterrows():
        sid = row['student_id']
        kc_name = row['kc_name']
        mastery_lookup[sid][kc_name] = {
            'mastery_level': row.get('mastery_level', 'N/A'),
            'rationale': row.get('rationale', ''),
            'suggestions': row.get('suggestions', '')
        }

    if not mastery_lookup:
        print("æŒæ¡åº¦è¯„ä¼°ç»“æœä¸­æ²¡æœ‰ä¸é€‰å®šå­¦ç”ŸåŒ¹é…çš„è®°å½•ï¼Œå°†è·³è¿‡æŒæ¡åº¦ä¸Šä¸‹æ–‡å¢å¼ºå®éªŒã€‚")

    return mastery_lookup


def load_tutoring_content_results(results_path, target_student_ids=None):
    """
    åŠ è½½è¾…å¯¼å†…å®¹ç»“æœï¼Œå¹¶æ ¹æ®ç›®æ ‡å­¦ç”Ÿç­›é€‰ã€‚
    è¿”å›ç»“æ„: {student_id: {kc_name: {"tutoring_content": str, "example_question_ids": list}}}
    """
    if not results_path or not os.path.exists(results_path):
        print("æœªæ‰¾åˆ°è¾…å¯¼å†…å®¹ç»“æœæ–‡ä»¶ï¼Œå°†è·³è¿‡è¾…å¯¼å†…å®¹å¢å¼ºå®éªŒã€‚")
        return {}

    try:
        tutoring_df = pd.read_csv(results_path)
    except Exception as e:
        print(f"åŠ è½½è¾…å¯¼å†…å®¹ç»“æœå¤±è´¥: {e}")
        return {}

    if 'student_id' not in tutoring_df.columns or 'kc_name' not in tutoring_df.columns:
        print("è¾…å¯¼å†…å®¹ç»“æœç¼ºå°‘å¿…è¦åˆ— (student_id, kc_name)ï¼Œå°†è·³è¿‡è¾…å¯¼å†…å®¹å¢å¼ºå®éªŒã€‚")
        return {}

    if target_student_ids is not None:
        tutoring_df = tutoring_df[tutoring_df['student_id'].isin(target_student_ids)]

    tutoring_lookup = defaultdict(dict)
    for _, row in tutoring_df.iterrows():
        sid = row['student_id']
        kc_name = row['kc_name']
        
        # è§£æ example_question_idsï¼ˆJSONå­—ç¬¦ä¸²ï¼‰
        example_q_ids = []
        if pd.notna(row.get('example_question_ids')):
            try:
                example_q_ids = json.loads(row['example_question_ids'])
            except:
                pass
        
        tutoring_lookup[sid][kc_name] = {
            'tutoring_content': row.get('tutoring_content', ''),
            'example_question_ids': example_q_ids,
            'llm_raw_response': row.get('llm_raw_response', ''),
            'prompt_system': row.get('prompt_system', ''),
            'prompt_user': row.get('prompt_user', '')
        }

    if not tutoring_lookup:
        print("è¾…å¯¼å†…å®¹ç»“æœä¸­æ²¡æœ‰ä¸é€‰å®šå­¦ç”ŸåŒ¹é…çš„è®°å½•ï¼Œå°†è·³è¿‡è¾…å¯¼å†…å®¹å¢å¼ºå®éªŒã€‚")

    return tutoring_lookup


def calculate_expected_tutoring_pairs(student_ids, all_student_records, mastery_lookup=None):
    """
    è®¡ç®—æ¯ä¸ªå­¦ç”Ÿåº”è¯¥ç”Ÿæˆè¾…å¯¼å†…å®¹çš„çŸ¥è¯†ç‚¹åˆ—è¡¨ã€‚
    
    Args:
        student_ids: å­¦ç”ŸIDåˆ—è¡¨
        all_student_records: æ‰€æœ‰å­¦ç”Ÿçš„åšé¢˜è®°å½• {student_id: DataFrame}
        mastery_lookup: æŒæ¡åº¦è¯„ä¼°æ•°æ® {student_id: {kc_name: {...}}}
    
    Returns:
        dict: {
            'expected_pairs': set of (student_id, kc_name),
            'student_weak_kcs': {student_id: [kc_names]}
        }
    """
    from sklearn.model_selection import train_test_split
    
    expected_pairs = set()
    student_weak_kcs = {}
    
    for student_id in student_ids:
        if student_id not in all_student_records:
            continue
        
        student_records_df = all_student_records[student_id]
        
        # æ•°æ®åˆ’åˆ†ï¼ˆä¸ generate_tutoring_content.py ä¿æŒä¸€è‡´ï¼‰
        if len(student_records_df) > 10:
            train_df, _ = train_test_split(
                student_records_df, 
                test_size=0.1, 
                random_state=42, 
                shuffle=True
            )
        else:
            train_df = student_records_df
        
        # è¯†åˆ«è–„å¼±çŸ¥è¯†ç‚¹ï¼ˆä¸ generate_tutoring_content.py é€»è¾‘ä¸€è‡´ï¼‰
        weak_kcs = []
        
        # æ–¹å¼1: åŸºäºæŒæ¡åº¦è¯„ä¼°
        if mastery_lookup and student_id in mastery_lookup:
            for kc_name, info in mastery_lookup[student_id].items():
                level = (info or {}).get('mastery_level', '')
                if isinstance(level, str) and level.strip() in ['Novice', 'Developing']:
                    weak_kcs.append(kc_name)
        
        # æ–¹å¼2: åŸºäºé”™é¢˜ç»Ÿè®¡
        if not weak_kcs:
            wrong_df = train_df[train_df['score'] == 0]
            if not wrong_df.empty:
                kc_order = wrong_df['know_name'].value_counts().index.tolist()
                weak_kcs = kc_order  # ä¸é™åˆ¶æ•°é‡
        
        # è®°å½•è¯¥å­¦ç”Ÿçš„è–„å¼±çŸ¥è¯†ç‚¹
        student_weak_kcs[student_id] = weak_kcs
        
        # æ„å»ºæœŸæœ›çš„ (student_id, kc_name) å¯¹
        for kc_name in weak_kcs:
            expected_pairs.add((student_id, kc_name))
    
    return {
        'expected_pairs': expected_pairs,
        'student_weak_kcs': student_weak_kcs
    }


def build_related_kc_map(all_kc_names, kcg_edges):
    """æ ¹æ®çŸ¥è¯†ç‚¹å…³ç³»æ„å»ºé‚»æ¥æ˜ å°„ã€‚"""
    related_map = {kc: {kc} for kc in all_kc_names}
    for pre_kc, post_kc in kcg_edges:
        if pre_kc in related_map:
            related_map[pre_kc].add(post_kc)
        else:
            related_map[pre_kc] = {pre_kc, post_kc}
        if post_kc in related_map:
            related_map[post_kc].add(pre_kc)
        else:
            related_map[post_kc] = {post_kc, pre_kc}
    return related_map


def build_mastery_summary(student_id, target_kc, related_map, mastery_lookup, kc_descriptions=None):
    """
    ä¸ºæŒ‡å®šå­¦ç”Ÿå’Œç›®æ ‡çŸ¥è¯†ç‚¹æ„å»ºæŒæ¡åº¦æ‘˜è¦ï¼ŒåŒ…å«çŸ¥è¯†ç‚¹æè¿°ã€æŒæ¡ç­‰çº§ä¸ç†ç”±æ‘˜è¦ã€‚
    ä¼˜åŒ–ç‰ˆï¼šæ›´æ¸…æ™°åœ°å±•ç¤ºå½“å‰çŸ¥è¯†ç‚¹çš„æŒæ¡æƒ…å†µï¼Œå¸®åŠ©æ¨¡å‹åšå‡ºæ›´å‡†ç¡®çš„é¢„æµ‹ã€‚
    """
    if not mastery_lookup or student_id not in mastery_lookup:
        return None

    related_kcs = related_map.get(target_kc, {target_kc})
    student_mastery = mastery_lookup[student_id]

    # ä¼˜å…ˆå±•ç¤ºç›®æ ‡çŸ¥è¯†ç‚¹
    target_info = student_mastery.get(target_kc)
    if not target_info:
        return None
    
    lines = []
    
    # 1. çªå‡ºæ˜¾ç¤ºå½“å‰é¢˜ç›®çš„çŸ¥è¯†ç‚¹æŒæ¡æƒ…å†µ
    level = target_info.get('mastery_level', 'N/A')
    rationale = target_info.get('rationale', '') or ''
    
    # æ¸…ç† mastery_levelï¼šå»é™¤ç©ºæ ¼å’Œ Markdown æ ‡è®°
    if isinstance(level, str):
        level = level.strip()  # å»é™¤é¦–å°¾ç©ºæ ¼
        level = level.replace('**', '')  # å»é™¤ Markdown åŠ ç²—æ ‡è®°
    
    # æ ¹æ®æŒæ¡ç­‰çº§ç»™å‡ºæ˜ç¡®çš„è‡ªä¿¡åº¦æç¤º
    confidence_map = {
        'Novice': 'âš ï¸ Low Confidence - You are still learning this concept',
        'Developing': 'âš¡ Moderate Confidence - You have basic understanding but may struggle',
        'Proficient': 'âœ“ Good Confidence - You have solid grasp of this concept',
        'Mastered': 'â˜… High Confidence - You have mastered this concept',
        'Advanced': 'â˜… High Confidence - You have mastered this concept'
    }
    confidence_hint = confidence_map.get(level, '? Uncertain')
    
    lines.append(f"ğŸ“Œ Target Concept: {target_kc}")
    lines.append(f"   Mastery Level: {level}")
    lines.append(f"   Confidence: {confidence_hint}")
    lines.append(f"   Analysis: {rationale}")
    
    # ç›¸å…³çŸ¥è¯†ç‚¹çš„æŒæ¡ä¸å†ä½œä¸ºè¯„ä¼°è¾“å…¥ï¼Œå·²ç§»é™¤
    
    return "\n".join(lines)


def _truncate_text(text, limit=180):
    if not isinstance(text, str):
        return ""
    text = text.strip()
    return text if len(text) <= limit else text[:limit] + "..."


def prepare_recommendation_inputs(student_records_df, kc_to_questions_map, question_text_map, max_wrong_questions=5, max_recommendations_per_kc=3):
    wrong_records = student_records_df[student_records_df['score'] == 0].copy()
    if wrong_records.empty:
        return [], []

    wrong_records = wrong_records.sort_values(by='start_time')
    wrong_details = []
    for _, row in wrong_records.head(max_wrong_questions).iterrows():
        wrong_details.append({
            'question_id': row['question_id'],
            'know_name': row['know_name'],
            'question_preview': _truncate_text(row.get('exer_content', '')),
            'start_time': row.get('start_time', '')
        })

    attempted_question_ids = set(student_records_df['question_id'].tolist())

    kc_candidates = []
    wrong_kc_counts = wrong_records['know_name'].value_counts()
    for kc_name in wrong_kc_counts.index:
        associated_questions = kc_to_questions_map.get(kc_name, [])
        candidate_ids = [qid for qid in associated_questions if qid not in attempted_question_ids]
        if not candidate_ids:
            continue
        recs = []
        for qid in candidate_ids[:max_recommendations_per_kc]:
            recs.append({
                'question_id': qid,
                'question_preview': _truncate_text(question_text_map.get(qid, ''))
            })
        if recs:
            kc_candidates.append({
                'kc_name': kc_name,
                'recommendations': recs
            })

    return wrong_details, kc_candidates


def build_recommendation_agent_prompt(student_id, wrong_details, kc_candidates):
    system_prompt = (
        "You are a Personalized Exercise Recommendation Agent. "
        "Analyze the student's mistakes and recommend targeted practice exercises."
    )

    user_lines = [f"Student ID: {student_id}"]
    user_lines.append("\n--- Incorrect Attempts ---")
    if wrong_details:
        for item in wrong_details:
            user_lines.append(
                f"Question ID: {item['question_id']} | KC: {item['know_name']} | Preview: {item['question_preview']}"
            )
    else:
        user_lines.append("No incorrect attempts found.")

    user_lines.append("\n--- Candidate Exercises by Knowledge Component ---")
    if kc_candidates:
        for candidate in kc_candidates:
            user_lines.append(f"KC: {candidate['kc_name']}")
            for rec in candidate['recommendations']:
                user_lines.append(
                    f"  - Recommend Question ID {rec['question_id']}: {rec['question_preview']}"
                )
    else:
        user_lines.append("No candidate exercises available.")

    user_lines.append(
        "\nTask: Select up to 3 high-priority exercises to recommend. "
        "For each recommendation, explain briefly why it is helpful."
    )
    user_lines.append(
        "\nOutput format:\n"
        "Recommendation 1: <Question ID> | KC: <KC Name> | Reason: <text>\n"
        "Recommendation 2: ..."
    )

    return system_prompt, "\n".join(user_lines)


def parse_recommendation_response(text):
    if not isinstance(text, str):
        return ""
    return text.strip()


def parse_tutoring_by_kc(llm_response, weak_kc_list):
    """
    è§£æLLMçš„è¾…å¯¼å“åº”ï¼ŒæŒ‰çŸ¥è¯†ç‚¹åˆ†æ®µå­˜å‚¨ã€‚
    
    æ”¹è¿›ç‰ˆï¼šå¢åŠ å®¹é”™æœºåˆ¶å’Œå¤‡ç”¨ç­–ç•¥ï¼Œè§£å†³çŸ¥è¯†ç‚¹åç§°ä¸åŒ¹é…é—®é¢˜
    
    Args:
        llm_response: LLMè¿”å›çš„å®Œæ•´è¾…å¯¼å†…å®¹
        weak_kc_list: è–„å¼±çŸ¥è¯†ç‚¹åˆ—è¡¨
    
    Returns:
        dict: {kc_name: tutoring_content_for_that_kc}
    """
    if not isinstance(llm_response, str) or not weak_kc_list:
        return {}
    
    parsed = {}
    import re
    
    # ç­–ç•¥1ï¼šç²¾ç¡®åŒ¹é…ï¼ˆæ”¯æŒ Markdown åŠ ç²—æ ‡è®° **ï¼‰
    for kc_name in weak_kc_list:
        # å…è®¸çŸ¥è¯†ç‚¹åç§°å‰åæœ‰ ** æ ‡è®°ï¼ˆMarkdownåŠ ç²—ï¼‰
        # åŒ¹é…æ¨¡å¼ï¼šConcept: **XXX** æˆ– Concept: XXX
        pattern = rf'Concept:\s*\*?\*?\s*{re.escape(kc_name)}\s*\*?\*?(.*?)(?=Concept:\s*\*?\*?|\Z)'
        match = re.search(pattern, llm_response, re.DOTALL | re.IGNORECASE)
        
        if match:
            content = match.group(1).strip()
            if content:
                parsed[kc_name] = f"Concept: {kc_name}\n{content}"
    
    # ç­–ç•¥2ï¼šå¦‚æœç²¾ç¡®åŒ¹é…å¤±è´¥ï¼Œå°è¯•æ¨¡ç³ŠåŒ¹é…
    if not parsed:
        for kc_name in weak_kc_list:
            # æ¨¡ç³ŠåŒ¹é…ï¼šæŸ¥æ‰¾åŒ…å«éƒ¨åˆ†çŸ¥è¯†ç‚¹åç§°çš„æ®µè½
            sections = llm_response.split('Concept:')
            for section in sections[1:]:  # è·³è¿‡ç¬¬ä¸€ä¸ªç©ºæ®µ
                section = section.strip()
                if not section:
                    continue
                # æå–ç¬¬ä¸€è¡Œä½œä¸ºLLMè¿”å›çš„çŸ¥è¯†ç‚¹åç§°
                first_line = section.split('\n')[0].strip().strip('*').strip()
                # æ£€æŸ¥æ˜¯å¦åŒ…å«æœŸæœ›çš„çŸ¥è¯†ç‚¹å…³é”®è¯
                if kc_name.lower() in first_line.lower() or first_line.lower() in kc_name.lower():
                    content_lines = section.split('\n', 1)
                    content = content_lines[1] if len(content_lines) > 1 else ''
                    parsed[kc_name] = f"Concept: {kc_name}\n{content}"
                    break
    
    # ç­–ç•¥3ï¼šå¦‚æœæ‰€æœ‰çŸ¥è¯†ç‚¹éƒ½è§£æå¤±è´¥ï¼Œé‡‡ç”¨é¡ºåºåˆ†é…ï¼ˆæœ€åå¤‡ç”¨æ–¹æ¡ˆï¼‰
    if not parsed and weak_kc_list:
        sections = re.split(r'Concept:', llm_response, flags=re.IGNORECASE)
        sections = [s.strip() for s in sections[1:] if s.strip()]  # è·³è¿‡ç¬¬ä¸€ä¸ªç©ºæ®µ
        
        # æŒ‰é¡ºåºå°†æ®µè½åˆ†é…ç»™çŸ¥è¯†ç‚¹
        for i, kc_name in enumerate(weak_kc_list):
            if i < len(sections):
                # æå–æ®µè½çš„ç¬¬ä¸€è¡Œä½œä¸ºLLMè¿”å›çš„çŸ¥è¯†ç‚¹åç§°
                first_line = sections[i].split('\n')[0].strip().strip('*').strip()
                # ä½¿ç”¨åŸå§‹çŸ¥è¯†ç‚¹åç§°ä½œä¸ºkeyï¼Œä½†ä¿ç•™LLMè¿”å›çš„å®Œæ•´å†…å®¹
                content_lines = sections[i].split('\n', 1)
                content = content_lines[1] if len(content_lines) > 1 else sections[i]
                parsed[kc_name] = f"Concept: {kc_name}\n{content}"
                
                # è®°å½•è­¦å‘Šï¼šLLMè¿”å›çš„åç§°ä¸æœŸæœ›ä¸ç¬¦
                if first_line.lower() != kc_name.lower():
                    import logging
                    logging.warning(f"çŸ¥è¯†ç‚¹åç§°ä¸åŒ¹é… - æœŸæœ›: '{kc_name}', LLMè¿”å›: '{first_line}' (å·²ä½¿ç”¨é¡ºåºåˆ†é…)")
    
    return parsed


def _select_three_questions_for_kc(kc_name, kc_to_questions_map, test_question_ids, question_text_map, question_choices_df, max_num=3):
    """
    ä¸ºæŒ‡å®šçŸ¥è¯†ç‚¹æŒ‘é€‰æœ€å¤š3é“é¢˜ï¼Œå¹¶é™„å¸¦é€‰é¡¹ä¸æ­£ç¡®ç­”æ¡ˆæ–‡æœ¬ã€‚
    
    é¢˜åº“é€‰æ‹©é€»è¾‘ï¼š
    - ä»è¯¥çŸ¥è¯†ç‚¹çš„æ‰€æœ‰é¢˜ç›®ä¸­æ’é™¤æµ‹è¯•é›†çš„é¢˜ç›®
    - å¯ä»¥åŒ…å«è®­ç»ƒé›†åšè¿‡çš„é¢˜ï¼ˆç”¨äºå¤ä¹ è®²è§£ï¼‰
    - ç¡®ä¿ä¸æ³„éœ²æµ‹è¯•é›†ç­”æ¡ˆ
    
    Args:
        test_question_ids: æµ‹è¯•é›†é¢˜ç›®IDé›†åˆï¼ˆéœ€è¦æ’é™¤çš„ï¼‰
    """
    question_ids = kc_to_questions_map.get(kc_name, []) or []
    if not question_ids:
        return []

    # æ’é™¤æµ‹è¯•é›†é¢˜ç›®ï¼ˆå¯ä»¥åŒ…å«è®­ç»ƒé›†é¢˜ç›®ï¼‰
    available = [qid for qid in question_ids if qid not in test_question_ids]
    pool = available

    picked = []
    candidates = list(pool)
    random.shuffle(candidates)
    for qid in candidates:
        if len(picked) >= max_num:
            break
        # é¢˜å¹²
        q_text = _truncate_text(question_text_map.get(qid, '') or '')
        # é€‰é¡¹ä¸æ­£ç¡®ç­”æ¡ˆ
        choices = get_question_choices(qid, question_choices_df)
        if not choices:
            picked.append({
                'question_id': qid,
                'question_text': q_text,
                'choices': [],
                'correct_letter': None,
                'correct_text': None
            })
            continue
        letters = [chr(65 + i) for i in range(len(choices))]
        correct_letter = None
        correct_text = None
        rendered_choices = []
        for idx, ch in enumerate(choices):
            letter = letters[idx]
            rendered_choices.append({'letter': letter, 'text': ch.get('choice_text', '')})
            if ch.get('is_correct'):
                correct_letter = letter
                correct_text = ch.get('choice_text', '')

        picked.append({
            'question_id': qid,
            'question_text': q_text,
            'choices': rendered_choices,
            'correct_letter': correct_letter,
            'correct_text': correct_text
        })

    return picked


def build_tutoring_prompt_single_kc(student_id, kc_name, kc_description, example_questions):
    """
    æ„å»ºå•ä¸ªçŸ¥è¯†ç‚¹çš„è¾…å¯¼æç¤ºè¯ï¼ˆä¼˜åŒ–ç‰ˆï¼šæ¯æ¬¡åªå¤„ç†1ä¸ªçŸ¥è¯†ç‚¹ + 3é“ä¾‹é¢˜ï¼‰
    
    ä» generate_tutoring_content.py å¯¼å…¥ï¼Œé¿å…é‡å¤ä»£ç 
    
    Args:
        student_id: å­¦ç”ŸID
        kc_name: çŸ¥è¯†ç‚¹åç§°
        kc_description: çŸ¥è¯†ç‚¹æè¿°
        example_questions: 3é“ä¾‹é¢˜åˆ—è¡¨ï¼ˆåŒ…å«é¢˜å¹²ã€é€‰é¡¹ã€ç­”æ¡ˆï¼‰
    
    Returns:
        tuple: (system_prompt, user_prompt)
    """
    system_prompt = (
        "You are an experienced tutoring teacher. Your task:\n"
        "1. Explain the concept clearly - what it is and why it matters\n"
        "2. For each example, show step-by-step solution AND explicitly connect it to the concept\n"
        "\n"
        "Output format:\n"
        "Concept Explanation:\n"
        "[Clear explanation of the key ideas and their importance]\n"
        "\n"
        "Example 1 (Question ID: X):\n"
        "Solution: [Step-by-step process]\n"
        "Connection: [How this example demonstrates the concept]\n"
        "\n"
        "Example 2 (Question ID: Y):\n"
        "Solution: [Step-by-step process]\n"
        "Connection: [How this example demonstrates the concept]\n"
        "\n"
        "Example 3 (Question ID: Z):\n"
        "Solution: [Step-by-step process]\n"
        "Connection: [How this example demonstrates the concept]"
    )

    lines = []
    lines.append(f"Student ID: {student_id}")
    lines.append(f"Concept: {kc_name}")
    
    if kc_description:
        lines.append(f"\nConcept Description: {kc_description}")
    
    lines.append(f"\nPractice Examples:")
    for idx, ex in enumerate(example_questions, start=1):
        lines.append(f"\nExample {idx} (Question ID: {ex['question_id']}):")
        lines.append(f"{ex['question_text']}")
        if ex['choices']:
            for ch in ex['choices']:
                lines.append(f"{ch['letter']}. {ch['text']}")
        if ex['correct_letter']:
            lines.append(f"Correct Answer: {ex['correct_letter']}")

    return system_prompt, "\n".join(lines)


def build_tutoring_agent_prompt(student_id, weak_kc_list, kc_descriptions, kc_to_questions_map, question_text_map, question_choices_df, student_records_df, test_question_ids=None):
    """
    æ„å»ºä¸ªæ€§åŒ–è¾…å¯¼æ™ºèƒ½ä½“æç¤ºè¯ï¼ˆæ‰¹å¤„ç†ç‰ˆï¼šå…¼å®¹å¤šçŸ¥è¯†ç‚¹ï¼‰
    
    æ³¨æ„ï¼šå»ºè®®ä½¿ç”¨ build_tutoring_prompt_single_kc() é€ä¸ªç”Ÿæˆï¼Œé¿å…è§£æé—®é¢˜
    
    Args:
        test_question_ids: æµ‹è¯•é›†é¢˜ç›®IDé›†åˆï¼Œç”¨äºæ’é™¤ï¼ˆé¿å…æ³„éœ²ç­”æ¡ˆï¼‰ã€‚å¦‚æœä¸ºNoneåˆ™ä¸æ’é™¤ä»»ä½•é¢˜ç›®ã€‚
    
    Returns:
        tuple: (system_prompt, user_prompt, actual_kcs)
    """
    # ä½¿ç”¨ä¼ å…¥çš„æµ‹è¯•é›†é¢˜ç›®IDï¼ˆé¿å…é‡å¤æ•°æ®åˆ’åˆ†ï¼‰
    if test_question_ids is None:
        test_question_ids = set()

    tutoring_items = []
    for kc_name in weak_kc_list:
        desc = (kc_descriptions or {}).get(kc_name, '') or ''
        picked = _select_three_questions_for_kc(
            kc_name,
            kc_to_questions_map,
            test_question_ids,  # â† åªæ’é™¤æµ‹è¯•é›†
            question_text_map,
            question_choices_df,
            max_num=3
        )
        if not picked:
            continue
        tutoring_items.append({'kc': kc_name, 'desc': desc, 'examples': picked})

    if not tutoring_items:
        return None, None, []
    
    # ä½¿ç”¨ç»Ÿä¸€çš„å•çŸ¥è¯†ç‚¹å‡½æ•°æ„å»ºï¼ˆæ‰¹å¤„ç†æ—¶æ‹¼æ¥å¤šä¸ªï¼‰
    all_lines = []
    all_lines.append(f"Student ID: {student_id}")
    all_lines.append("\nKey Knowledge Points to Review:")
    
    system_prompt = None
    for item in tutoring_items:
        sys_prompt, user_section = build_tutoring_prompt_single_kc(
            student_id,
            item['kc'],
            item['desc'],
            item['examples']
        )
        if system_prompt is None:
            # ä¿®æ”¹ç³»ç»Ÿæç¤ºè¯ä»¥æ”¯æŒå¤šçŸ¥è¯†ç‚¹
            system_prompt = sys_prompt.replace(
                "Output format:",
                "Output format for EACH concept:\nConcept: <exact_concept_name>\n"
            )
        
        # æ·»åŠ åˆ†éš”ç¬¦å’ŒçŸ¥è¯†ç‚¹å†…å®¹
        all_lines.append(f"\n{'='*60}")
        # è·³è¿‡ "Student ID: xxx" è¡Œï¼Œåªä¿ç•™çŸ¥è¯†ç‚¹å†…å®¹
        kc_section = '\n'.join(user_section.split('\n')[1:])
        all_lines.append(kc_section)
    
    actual_kcs = [item['kc'] for item in tutoring_items]
    return system_prompt, "\n".join(all_lines), actual_kcs


async def run_tutoring_agent(student_id, student_records_df, kc_to_questions_map, question_text_map, kc_descriptions, question_choices_df, prompt_log_path, mastery_lookup=None, test_question_ids=None):
    """
    è¿è¡Œä¸ªæ€§åŒ–è¾…å¯¼æ™ºèƒ½ä½“ï¼Œè¿”å›ç»“æ„åŒ–çš„è¾…å¯¼å†…å®¹å­—å…¸ï¼ˆæŒ‰çŸ¥è¯†ç‚¹ç»„ç»‡ï¼‰ã€‚
    
    Args:
        test_question_ids: æµ‹è¯•é›†é¢˜ç›®IDé›†åˆï¼Œç”¨äºæ’é™¤ï¼ˆé¿å…æ³„éœ²ç­”æ¡ˆï¼‰
    """
    # 1) è¯†åˆ«è–„å¼±çŸ¥è¯†ç‚¹ï¼šä¼˜å…ˆç”¨æŒæ¡åº¦ï¼Œå…¶æ¬¡ç”¨é”™é¢˜ï¼ˆä¸é™åˆ¶æ•°é‡ï¼‰
    weak_kcs = []
    if mastery_lookup and student_id in mastery_lookup:
        for kc_name, info in mastery_lookup[student_id].items():
            level = (info or {}).get('mastery_level', '')
            if isinstance(level, str) and level.strip() in ['Novice', 'Developing']:
                weak_kcs.append(kc_name)
        # ç§»é™¤æ•°é‡é™åˆ¶ï¼Œè¯„ä¼°æ‰€æœ‰è–„å¼±çŸ¥è¯†ç‚¹

    if not weak_kcs:
        wrong_df = student_records_df[student_records_df['score'] == 0]
        if not wrong_df.empty:
            kc_order = wrong_df['know_name'].value_counts().index.tolist()
            weak_kcs = kc_order  # ä¸é™åˆ¶æ•°é‡

    if not weak_kcs:
        return None

    # 2) æ„å»ºæç¤ºè¯ï¼ˆè¿”å›å®é™…ä½¿ç”¨çš„çŸ¥è¯†ç‚¹åˆ—è¡¨ï¼‰
    system_prompt, user_prompt, actual_kcs = build_tutoring_agent_prompt(
        student_id,
        weak_kcs,
        kc_descriptions,
        kc_to_questions_map,
        question_text_map,
        question_choices_df,
        student_records_df,
        test_question_ids=test_question_ids  # ä¼ é€’æµ‹è¯•é›†IDï¼Œé¿å…é‡å¤åˆ’åˆ†
    )
    if not user_prompt or not actual_kcs:
        return None

    # 3) è°ƒç”¨LLM
    try:
        raw_resp = await user_sys_call_with_model(
            user_prompt=user_prompt,
            system_prompt=system_prompt,
            model_name=MODEL_NAME
        )
    except Exception as e:
        print(f"ä¸ªæ€§åŒ–è¾…å¯¼æ™ºèƒ½ä½“è°ƒç”¨å¤±è´¥: {e}")
        raw_resp = f"LLM_CALL_FAILED: {e}"

    # 4) è®°å½•æ—¥å¿—
    try:
        with open(prompt_log_path, "a", encoding="utf-8") as f:
            f.write(f"--- TUTORING AGENT FOR STUDENT {student_id} ---\n")
            f.write("--- SYSTEM PROMPT ---\n" + system_prompt + "\n\n")
            f.write("--- USER PROMPT ---\n" + user_prompt + "\n\n")
            f.write("--- LLM RESPONSE ---\n" + str(raw_resp) + "\n" + "="*80 + "\n\n")
    except Exception as e:
        print(f"å†™å…¥è¾…å¯¼æ—¥å¿—å¤±è´¥: {e}")

    # 5) è§£æLLMå“åº”ï¼ŒæŒ‰çŸ¥è¯†ç‚¹åˆ†æ®µå­˜å‚¨ï¼ˆä½¿ç”¨å®é™…çš„çŸ¥è¯†ç‚¹åˆ—è¡¨ï¼‰
    parsed_tutoring = parse_tutoring_by_kc(raw_resp, actual_kcs)
    
    # è¿”å›ç»“æ„åŒ–å­—å…¸ï¼š{kc_name: tutoring_content}
    return parsed_tutoring


async def run_recommendation_agent(student_id, student_records_df, kc_to_questions_map, question_text_map, prompt_log_path):
    wrong_details, kc_candidates = prepare_recommendation_inputs(
        student_records_df,
        kc_to_questions_map,
        question_text_map
    )

    if not wrong_details or not kc_candidates:
        return None

    system_prompt, user_prompt = build_recommendation_agent_prompt(student_id, wrong_details, kc_candidates)

    try:
        raw_resp = await user_sys_call_with_model(
            user_prompt=user_prompt,
            system_prompt=system_prompt,
            model_name=MODEL_NAME
        )
    except Exception as e:
        print(f"æ¨èæ™ºèƒ½ä½“è°ƒç”¨å¤±è´¥: {e}")
        raw_resp = f"LLM_CALL_FAILED: {e}"

    try:
        with open(prompt_log_path, "a", encoding="utf-8") as f:
            f.write(f"--- PERSO-RECO AGENT FOR STUDENT {student_id} ---\n")
            f.write("--- SYSTEM PROMPT ---\n" + system_prompt + "\n\n")
            f.write("--- USER PROMPT ---\n" + user_prompt + "\n\n")
            f.write("--- LLM RESPONSE ---\n" + str(raw_resp) + "\n" + "="*80 + "\n\n")
    except Exception as e:
        print(f"å†™å…¥ä¸ªæ€§åŒ–æ¨èæ—¥å¿—å¤±è´¥: {e}")

    summary = parse_recommendation_response(raw_resp)
    return summary


async def run_mastery_assessment_pipeline(concurrency, student_ids=None, student_count=-1, mode="both", model_name=None):
    """è°ƒç”¨æŒæ¡åº¦è¯„ä¼°è„šæœ¬ï¼Œè¿”å›æ˜¯å¦æ‰§è¡ŒæˆåŠŸã€‚"""
    assess_script = os.path.join(os.path.dirname(__file__), 'assess_mastery.py')
    try:
        cmd = [
            sys.executable,
            assess_script,
            '--concurrency',
            str(concurrency),
            '--mode',
            mode
        ]
        # ä¼ é€’æ¨¡å‹åç§°ï¼Œç¡®ä¿è¯„ä¼°è„šæœ¬ä¸ä¸»å®éªŒä½¿ç”¨ç›¸åŒæ¨¡å‹
        if model_name:
            cmd.extend(['--model', str(model_name)])
        if student_ids is not None and student_ids:
            cmd.extend(['--student-ids', ','.join(map(str, student_ids))])
        else:
            cmd.extend(['--students', str(student_count)])

        proc = await asyncio.create_subprocess_exec(*cmd)
        await proc.wait()
        if proc.returncode != 0:
            print(f"æŒæ¡åº¦è¯„ä¼°è„šæœ¬è¿è¡Œå¤±è´¥ï¼Œè¿”å›ç  {proc.returncode}ã€‚")
            return False
        return True
    except FileNotFoundError:
        print("æœªæ‰¾åˆ° assess_mastery.pyï¼Œæ— æ³•è‡ªåŠ¨ç”ŸæˆæŒæ¡åº¦æ•°æ®ã€‚")
        return False
    except Exception as e:
        print(f"è¿è¡ŒæŒæ¡åº¦è¯„ä¼°è„šæœ¬æ—¶å‡ºç°å¼‚å¸¸: {e}")
        return False


# --- 3. Agent æ ¸å¿ƒæ¨¡å— ---

# Agent é…ç½®å‚æ•°
SIM_PARAMS = {
    'short_term_size': 5,
    'long_term_thresh': 3,
    'forget_lambda': 0.95
}

class Profile:
    def __init__(self, student_id, history_df, total_kc_count):
        self.student_id = student_id
        self.total_kc_count = total_kc_count
        if history_df.empty:
            self.success_rate = "medium"
            self.ability = "common"
            self.activity = "medium"
            self.diversity = "low"
            self.preference = "N/A"
        else:
            self._build_profile(history_df)

    def _build_profile(self, df):
        # æˆåŠŸç‡
        success_rate_val = df['score'].mean()
        self.success_rate = "high" if success_rate_val > 0.6 else ("medium" if success_rate_val > 0.3 else "low")
        
        # èƒ½åŠ›
        self.ability = "good" if success_rate_val > 0.5 else ("common" if success_rate_val > 0.4 else "poor")
        
        # æ´»è·ƒåº¦
        self.activity = "high" if len(df) > 200 else ("medium" if len(df) > 50 else "low")
        
        # çŸ¥è¯†å¤šæ ·æ€§
        kc_diversity_ratio = df['know_name'].nunique() / self.total_kc_count
        self.diversity = "high" if kc_diversity_ratio > 0.75 else ("medium" if kc_diversity_ratio > 0.4 else "low")
        
        # åå¥½
        self.preference = df['know_name'].mode().iloc[0] if not df.empty else "N/A"

    def build_prompt(self):
        # å°†æ´»è·ƒåº¦è½¬æ¢ä¸ºæ›´è‡ªç„¶çš„æè¿°
        activity_desc = {
            'high': 'You practice frequently and stay engaged with learning',
            'medium': 'You practice occasionally when needed',
            'low': 'You practice rarely and prefer familiar topics'
        }.get(self.activity, f'Your activity level is {self.activity}')
        
        # å°†çŸ¥è¯†å¤šæ ·æ€§è½¬æ¢ä¸ºæ›´è‡ªç„¶çš„æè¿°
        diversity_desc = {
            'high': 'You explore many different topics and concepts',
            'medium': 'You focus on select topics that interest you',
            'low': 'You stick to familiar topics you feel comfortable with'
        }.get(self.diversity, f'Your knowledge diversity is {self.diversity}')
        
        return (
            f"You ARE a student with these learning characteristics:\n\n"
            f"ğŸ“š Your Learning Profile:\n"
            f"  â€¢ Activity Level: {self.activity} - {activity_desc}\n"
            f"  â€¢ Knowledge Breadth: {self.diversity} - {diversity_desc}\n"
            f"  â€¢ Typical Success Rate: {self.success_rate}\n"
            f"  â€¢ Problem-Solving Ability: {self.ability}\n"
            f"  â€¢ Most Comfortable Topic: {self.preference}\n\n"
            f"ğŸ¯ How to Respond:\n"
            f"1. Think and answer as THIS student would - based on YOUR actual abilities and experiences\n"
            f"2. Be honest about your confidence level - don't overestimate or underestimate yourself\n"
            f"3. When predicting performance, reflect on YOUR past experiences with similar problems\n"
            f"4. If you're unsure or haven't mastered a concept, it's okay to predict 'No' - be realistic\n"
            f"5. Your responses should reflect your genuine thought process as this student\n"
        )

# --- 3.5 Agent è¡Œä¸ºå‡½æ•° (æ›¿ä»£ AgentAction ç±») ---

def get_question_choices(question_id, question_choices_df):
    """è·å–æŒ‡å®šé¢˜ç›®çš„ç­”æ¡ˆé€‰é¡¹"""
    if question_choices_df is None:
        return None
    choices = question_choices_df[question_choices_df['question_id'] == question_id]
    if choices.empty:
        return None
    # è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨
    return [
        {
            'choice_id': row['id'],
            'choice_text': row['choice_text'],
            'is_correct': row['is_correct']
        }
        for _, row in choices.iterrows()
    ]

def _build_agent_prompt(practice, all_kc_names, question_choices, mastery_summary=None, tutoring_dict=None):
    """
    æ„å»ºç”¨äº LLM çš„ç”¨æˆ·æç¤ºè¯ - ä»¥å­¦ç”Ÿç¬¬ä¸€äººç§°è§†è§’ã€‚
    
    å‚æ•°è¯´æ˜ï¼š
    - mastery_summary: æŒæ¡åº¦ä¿¡æ¯ï¼ˆé•¿æœŸè®°å¿†ï¼‰ï¼Œä»… Mastery Only æ¨¡å¼æœ‰
    - tutoring_dict: è¾…å¯¼å†…å®¹å­—å…¸ï¼ˆæŒ‰çŸ¥è¯†ç‚¹ç»„ç»‡ï¼‰ï¼Œä»… Tutoring Only æ¨¡å¼æœ‰
    - Baseline æ¨¡å¼ï¼šä¸¤è€…éƒ½æ²¡æœ‰ï¼Œåªæœ‰é¢˜ç›®æœ¬èº«
    """
    prompt = f"=== ğŸ“ The Question in Front of You ===\n"
    prompt += f"Question: {practice['exer_content']}\n"
    
    # å±•ç¤ºç­”æ¡ˆé€‰é¡¹
    if question_choices is not None and len(question_choices) > 0:
        prompt += f"\nAnswer Choices:\n"
        for idx, choice in enumerate(question_choices):
            choice_letter = chr(65 + idx)  # A, B, C, D...
            prompt += f"  {choice_letter}. {choice['choice_text']}\n"
        prompt += "\n"
    
    prompt += f"Topic: {practice['know_name']}\n\n"
    
    # é•¿æœŸè®°å¿†ï¼šæŒæ¡åº¦ä¿¡æ¯ï¼ˆä»… Mastery Only æ¨¡å¼ï¼‰
    if mastery_summary:
        prompt += "=== ğŸ§  Your Long-term Knowledge of This Topic ===\n"
        prompt += "Based on your accumulated learning experience:\n"
        # å°†å®¢è§‚æè¿°è½¬åŒ–ä¸ºç¬¬ä¸€äººç§°è®¤çŸ¥
        personalized_summary = mastery_summary.replace(
            "Target Concept:", "You're looking at:"
        ).replace(
            "Mastery Level:", "You feel you are at:"
        ).replace(
            "Confidence:", "Your confidence level:"
        ).replace(
            "Analysis:", "You've noticed:"
        ).replace(
            "Related Concepts:", "Related topics you've worked on:"
        )
        prompt += personalized_summary + "\n"
        prompt += "ğŸ’­ Keep this self-awareness in mind as you work through this question.\n\n"
    
    # çŸ­æœŸè®°å¿†ï¼šè¾…å¯¼å†…å®¹ï¼ˆä»… Tutoring Only æ¨¡å¼ï¼‰
    # é‡è¦æ”¹è¿›ï¼šåªä½¿ç”¨ä¸å½“å‰é¢˜ç›®çŸ¥è¯†ç‚¹ç›¸å…³çš„è¾…å¯¼å†…å®¹ï¼
    if tutoring_dict:
        current_kc = practice['know_name']
        relevant_tutoring = tutoring_dict.get(current_kc, None)
        
        # ğŸ”¥ ç±»å‹æ£€æŸ¥å’Œæ•°æ®æ¸…æ´—ï¼šç¡®ä¿ relevant_tutoring æ˜¯å­—ç¬¦ä¸²
        if relevant_tutoring is not None:
            # å¤„ç† NaN æˆ–å…¶ä»–éå­—ç¬¦ä¸²ç±»å‹
            if not isinstance(relevant_tutoring, str):
                try:
                    # å°è¯•è½¬æ¢ä¸ºå­—ç¬¦ä¸²
                    if pd.isna(relevant_tutoring):
                        relevant_tutoring = None  # NaN è§†ä¸ºæ— è¾…å¯¼å†…å®¹
                    else:
                        relevant_tutoring = str(relevant_tutoring)
                except Exception as e:
                    # è®°å½•ç±»å‹é”™è¯¯åˆ°æ—¥å¿—
                    import logging
                    logging.error(f"è¾…å¯¼å†…å®¹ç±»å‹è½¬æ¢å¤±è´¥ - å­¦ç”Ÿ: {practice.get('student_id', 'Unknown')}, çŸ¥è¯†ç‚¹: {current_kc}, ç±»å‹: {type(relevant_tutoring)}, å€¼: {relevant_tutoring}, é”™è¯¯: {e}")
                    relevant_tutoring = None
        
        if relevant_tutoring and len(str(relevant_tutoring).strip()) > 0:
            # åªæœ‰å½“å‰çŸ¥è¯†ç‚¹æœ‰æœ‰æ•ˆè¾…å¯¼å†…å®¹æ—¶æ‰æ˜¾ç¤º
            prompt += "=== ğŸ“š What You Just Reviewed (Short-term Memory) ===\n"
            prompt += "You recently reviewed this specific topic:\n"
            prompt += str(relevant_tutoring) + "\n\n"  # ç¡®ä¿æ˜¯å­—ç¬¦ä¸²
            
            # æ·»åŠ æ˜ç¡®çš„åº”ç”¨å¼•å¯¼
            prompt += "ğŸ’¡ **How to Use This Review:**\n"
            prompt += f"â€¢ This review is specifically about '{current_kc}' - exactly what this question tests!\n"
            prompt += "â€¢ Apply the key points and methods you just studied directly to this problem.\n"
            prompt += "â€¢ Check if this question is similar to the example problems you reviewed.\n"
            prompt += "â€¢ Recall the common mistakes and solution strategies you learned.\n\n"
        # å¦‚æœæ²¡æœ‰ç›¸å…³è¾…å¯¼å†…å®¹ï¼Œä¸æ˜¾ç¤ºè¾…å¯¼éƒ¨åˆ†ï¼ˆç±»ä¼¼baselineï¼‰

    # åŠ¨æ€ç”ŸæˆçŸ¥è¯†ç‚¹é€‰é¡¹
    correct_kc = practice['know_name']
    wrong_kcs = [kc for kc in all_kc_names if kc != correct_kc]
    kc_options = [correct_kc] + random.sample(wrong_kcs, min(2, len(wrong_kcs)))
    random.shuffle(kc_options)

    prompt += "=== ğŸ¤” Now, Think Through This Question as This Student ===\n\n"
    
    # Task 1: è‡ªæˆ‘é¢„æµ‹
    prompt += f"Task 1: Honestly predict - will you get this right?\n"
    prompt += f"        (Based on your knowledge and confidence about '{practice['know_name']}')\n"
    # æ£€æŸ¥æ˜¯å¦æœ‰è¾…å¯¼å†…å®¹ï¼ˆæ ¹æ®tutoring_dictæ˜¯å¦ä¸ºdictä¸”æœ‰å½“å‰KCï¼‰
    has_tutoring = tutoring_dict and isinstance(tutoring_dict, dict) and practice['know_name'] in tutoring_dict
    if has_tutoring:
        prompt += f"        Think to yourself:\n"
        prompt += f"          â€¢ Did I just review this topic? If so, I should feel more confident!\n"
        prompt += f"          â€¢ Do the example problems I studied help me understand this question?\n"
        prompt += f"          â€¢ Am I confident I can apply what I just learned?\n"
    else:
        prompt += f"        Think to yourself:\n"
        prompt += f"          â€¢ Do I understand this concept well?\n"
        prompt += f"          â€¢ Am I confident I can solve this correctly?\n"
    prompt += f"        Your honest prediction (Yes/No):\n\n"
    
    # Task 2: çŸ¥è¯†ç‚¹è¯†åˆ«ï¼ˆåŸTask2ä¿æŒï¼‰
    prompt += f"Task 2: What topic does this question test?\n"
    prompt += f"        (Based on what you see, which concept is this about?)\n"
    prompt += f"        Options: {', '.join(kc_options)}\n"
    prompt += f"        Your identification:\n\n"
    
    # Task 3: è§£é¢˜è¿‡ç¨‹
    prompt += f"Task 3: How would you approach and solve this?\n"
    if has_tutoring:
        prompt += f"        (Think about what you just reviewed - can you apply any of those concepts or methods here?)\n"
        prompt += f"        (If this is similar to the example problems, follow that solving approach)\n"
    else:
        prompt += f"        (Write your thought process and reasoning as you naturally would)\n"
    prompt += f"        Your work:\n\n"
    
    # Task 4: æœ€ç»ˆç­”æ¡ˆé€‰æ‹©ï¼ˆæ–°è®¾è®¡ï¼‰
    if question_choices is not None and len(question_choices) > 0:
        choice_letters = [chr(65 + i) for i in range(len(question_choices))]
        prompt += f"Task 4: What is your final answer choice?\n"
        prompt += f"        (Select the option you believe is correct)\n"
        prompt += f"        Available options: {', '.join(choice_letters)}\n"
        prompt += f"        Your choice:\n\n"
    else:
        # å¦‚æœæ²¡æœ‰é€‰é¡¹ï¼Œä¿æŒåŸæœ‰çš„Yes/Noé¢„æµ‹
        prompt += f"Task 4: Based on your work above, do you think your answer is correct?\n"
        prompt += f"        Your confidence (Yes/No):\n\n"

    prompt += "Output format:\n"
    prompt += "Task1: <Answer>\n"
    prompt += "Task2: <Answer>\n"
    prompt += "Task3: <Answer>\n"
    prompt += "Task4: <Answer>"
    
    return prompt

def _parse_llm_response(text):
    """ä» LLM çš„åŸå§‹æ–‡æœ¬è¾“å‡ºä¸­è§£æå‡ºå››ä¸ªä»»åŠ¡çš„ç»“æœã€‚"""
    ans = {f'task{i}': 'N/A' for i in range(1, 5)}
    if not isinstance(text, str):
        return ans
        
    for line in text.strip().split('\n'):
        if ':' in line:
            try:
                key, value = line.split(':', 1)
                key = key.strip().lower()
                if key in ans: 
                    ans[key] = value.strip()
            except ValueError:
                continue # å¿½ç•¥æ ¼å¼ä¸æ­£ç¡®çš„è¡Œ
    return ans

# run_agent_step å‡½æ•°å·²ç§»é™¤ï¼Œé€»è¾‘åˆå¹¶åˆ° run_simulation_for_student ä¸­


# --- 4. å®éªŒä¸»å¾ªç¯ ---
async def create_concurrent_llm_requests(requests, concurrency_limit=30, spread_duration=0):
    """
    ç»Ÿä¸€å¹¶å‘æ‰§è¡Œæ‰€æœ‰LLMè¯·æ±‚
    
    æ–°æ¶æ„ï¼š
    - ä½¿ç”¨ä¿¡å·é‡æ§åˆ¶å¹¶å‘æ•°ï¼ˆçœŸæ­£çš„APIå¹¶å‘æ§åˆ¶ï¼‰
    - æ”¯æŒå‰Šå³°å¡«è°·ï¼ˆå°†è¯·æ±‚å‡åŒ€åˆ†æ•£åˆ°æŒ‡å®šæ—¶é—´ï¼‰
    - å¸¦é‡è¯•æœºåˆ¶
    
    Args:
        requests: è¯·æ±‚åˆ—è¡¨ï¼ˆåŒ…å«æ‰€æœ‰å­¦ç”Ÿçš„æ‰€æœ‰é¢˜ç›®ï¼‰
        concurrency_limit: æœ€å¤§å¹¶å‘APIè¯·æ±‚æ•°
        spread_duration: å°†æ‰€æœ‰è¯·æ±‚åˆ†æ•£åˆ°æŒ‡å®šç§’æ•°å†…ï¼ˆ0è¡¨ç¤ºç¦ç”¨ï¼‰
    """
    print(f"   ğŸ“Š è¯·æ±‚ç»Ÿè®¡: {len(requests)} ä¸ª")
    print(f"   âš¡ å¹¶å‘æ§åˆ¶: æœ€å¤š {concurrency_limit} ä¸ªåŒæ—¶è¿›è¡Œ")
    
    if spread_duration > 0:
        delay_per_request = spread_duration / len(requests)
        print(f"   ğŸŒŠ å‰Šå³°å¡«è°·: æ¯ä¸ªè¯·æ±‚é—´éš” {delay_per_request:.2f} ç§’")
    else:
        delay_per_request = 0
        print(f"   ğŸš€ ç›´æ¥å¹¶å‘: æ— å»¶è¿Ÿå¯åŠ¨")
    
    # åˆ›å»ºä¿¡å·é‡æ§åˆ¶å¹¶å‘
    semaphore = asyncio.Semaphore(concurrency_limit)
    
    async def execute_single_request(req, index, start_delay):
        """æ‰§è¡Œå•ä¸ªè¯·æ±‚ï¼ˆå¸¦ä¿¡å·é‡æ§åˆ¶å’Œé‡è¯•ï¼‰"""
        # å‰Šå³°å¡«è°·å»¶è¿Ÿ
        if start_delay > 0:
            await asyncio.sleep(start_delay)
        
        # ä¿¡å·é‡æ§åˆ¶å¹¶å‘
        async with semaphore:
            student_id = req.get('student_id', 'Unknown')
            question_id = req.get('practice_data', {}).get('question_id', index)
            
            # é‡è¯•æœºåˆ¶
            retry_delays = [5, 10, 30]
            last_error = None
            
            for attempt in range(len(retry_delays) + 1):
                try:
                    if attempt == 0:
                        # é¦–æ¬¡å°è¯•
                        result = await user_sys_call_with_model(
                            user_prompt=req.get('user_prompt', ''),
                            system_prompt=req.get('system_prompt', ''),
                            model_name=req.get('model_name', MODEL_NAME)
                        )
                        # æˆåŠŸ
                        if (index + 1) % 100 == 0:  # æ¯100ä¸ªæ‰“å°ä¸€æ¬¡
                            print(f"   âœ… [{index+1}/{len(requests)}] è¿›åº¦æ›´æ–° - å­¦ç”Ÿ{student_id}")
                        return {"index": index, "result": result, "error": None}
                    else:
                        # é‡è¯•
                        retry_delay = retry_delays[attempt - 1]
                        print(f"   ğŸ”„ [{index+1}] é‡è¯• {attempt}/{len(retry_delays)} - ç­‰å¾…{retry_delay}ç§’")
                        await asyncio.sleep(retry_delay)
                        
                        result = await user_sys_call_with_model(
                            user_prompt=req.get('user_prompt', ''),
                            system_prompt=req.get('system_prompt', ''),
                            model_name=req.get('model_name', MODEL_NAME)
                        )
                        print(f"   âœ… [{index+1}] é‡è¯•æˆåŠŸ")
                        return {"index": index, "result": result, "error": None}
                        
                except Exception as e:
                    last_error = str(e) if str(e) else f"{type(e).__name__}"
                    if attempt == len(retry_delays):
                        # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥
                        error_detail = last_error[:100] if len(last_error) > 100 else last_error
                        print(f"   âŒ [{index+1}] æœ€ç»ˆå¤±è´¥: å­¦ç”Ÿ{student_id} é¢˜ç›®{question_id} - {error_detail}")
                        return {"index": index, "result": None, "error": last_error}
                    # ç»§ç»­é‡è¯•
                    continue
    
    # åˆ›å»ºæ‰€æœ‰ä»»åŠ¡
    tasks = []
    for i, req in enumerate(requests):
        start_delay = i * delay_per_request if spread_duration > 0 else 0
        tasks.append(execute_single_request(req, i, start_delay))
    
    # å¹¶å‘æ‰§è¡Œæ‰€æœ‰ä»»åŠ¡
    print(f"\nâ³ å¼€å§‹æ‰§è¡Œ {len(tasks)} ä¸ªè¯·æ±‚...\n")
    results = await asyncio.gather(*tasks, return_exceptions=True)
    
    # å¤„ç†å¼‚å¸¸ç»“æœ
    processed_results = []
    for i, result in enumerate(results):
        if isinstance(result, Exception):
            error_msg = str(result) if str(result) else f"{type(result).__name__}"
            processed_results.append({"index": i, "result": None, "error": error_msg})
        else:
            processed_results.append(result)
    
    # ç»Ÿè®¡ç»“æœ
    success_count = sum(1 for r in processed_results if r.get('error') is None)
    fail_count = len(processed_results) - success_count
    print(f"\nâœ… è¯·æ±‚å®Œæˆç»Ÿè®¡:")
    print(f"   æˆåŠŸ: {success_count}/{len(processed_results)}")
    print(f"   å¤±è´¥: {fail_count}/{len(processed_results)}")
    if fail_count > 0:
        print(f"   âš ï¸  å¤±è´¥ç‡: {fail_count/len(processed_results):.1%}")
    
    return processed_results

# æ³¨æ„ï¼šrun_simulation_for_student å‡½æ•°å·²åºŸå¼ƒï¼Œæ–°æ¶æ„ä½¿ç”¨ç»Ÿä¸€è¯·æ±‚æ± 
# ä¿ç•™æ­¤å‡½æ•°ä»…ä¸ºå‘åå…¼å®¹ï¼Œå®é™…å·²ä¸å†ä½¿ç”¨
async def run_simulation_for_student_DEPRECATED(student_id, student_records_df, semaphore, prompt_log_path, position, all_kc_names, overall_pbar, mastery_lookup=None, related_kc_map=None, kc_to_questions_map=None, question_text_map=None, recommendation_log_path=None, kc_descriptions=None, question_choices_df=None, use_mastery=True, use_tutoring=True, spread_duration=0):
    """
    å¯¹å•ä¸ªå­¦ç”Ÿè¿è¡Œå®Œæ•´æ¨¡æ‹Ÿå®éªŒ
    
    Args:
        use_mastery: æ˜¯å¦ä½¿ç”¨æŒæ¡åº¦å¢å¼ºï¼ˆæ§åˆ¶mastery_summaryï¼‰
        use_tutoring: æ˜¯å¦ä½¿ç”¨è¾…å¯¼è¾“å‡ºï¼ˆæ§åˆ¶tutoring_dictï¼‰
        spread_duration: å‰Šå³°å¡«è°·æ—¶é—´
    
    è¿”å›: student_resultsåˆ—è¡¨
    """
    async with semaphore:
        student_results = []
        print(f"\n{'='*60}")
        print(f"ğŸ“ å­¦ç”Ÿ {student_id} - å‡†å¤‡è¯·æ±‚")
        print(f"{'='*60}")
        
        train_df, test_df = train_test_split(student_records_df, test_size=0.1, random_state=42, shuffle=True)
        
        # æå–æµ‹è¯•é›†é¢˜ç›®IDï¼ˆæ¯ä¸ªå­¦ç”Ÿçš„æµ‹è¯•é›†ä¸åŒï¼‰
        test_question_ids = set(test_df['question_id'].tolist()) if not test_df.empty else set()
        
        print(f"ğŸ“Š å­¦ç”Ÿ {student_id} ç»Ÿè®¡:")
        print(f"   â€¢ è®­ç»ƒé›†é¢˜ç›®æ•°: {len(train_df)}")
        print(f"   â€¢ æµ‹è¯•é›†é¢˜ç›®æ•°: {len(test_df)}")
        
        # --- æ„å»ºå­¦ç”ŸProfileï¼ˆåŸºäºè®­ç»ƒé›†åšé¢˜è®°å½•ï¼‰ ---
        profile = Profile(student_id, train_df, len(all_kc_names))
        
        # --- å‡†å¤‡è¾…å¯¼å†…å®¹ï¼ˆå¦‚æœå¯ç”¨ï¼‰ ---
        tutoring_dict = None
        if use_tutoring:
            # ä½¿ç”¨ä¸ªæ€§åŒ–è¾…å¯¼æ™ºèƒ½ä½“ç”ŸæˆæŒ‰çŸ¥è¯†ç‚¹ç»„ç»‡çš„è¾…å¯¼å†…å®¹
            tutoring_dict = await run_tutoring_agent(
                student_id,
                train_df,
                kc_to_questions_map,
                question_text_map,
                kc_descriptions,
                question_choices_df,
                recommendation_log_path,
                mastery_lookup,
                test_question_ids=test_question_ids  # ä¼ é€’æµ‹è¯•é›†IDï¼Œé¿å…é‡å¤åˆ’åˆ†
            )
        
        # --- æµ‹è¯•é˜¶æ®µ (å¹¶å‘å¤„ç†) ---
        # 1. å‡†å¤‡æ‰€æœ‰å¹¶å‘è¯·æ±‚
        def build_requests(include_mastery=False, tutoring_content_dict=None):
            """
            æ„å»ºLLMè¯·æ±‚
            - include_mastery: æ˜¯å¦åŒ…å«æŒæ¡åº¦ä¿¡æ¯ï¼ˆé•¿æœŸè®°å¿†ï¼‰
            - tutoring_content_dict: è¾…å¯¼å†…å®¹å­—å…¸ï¼ˆæŒ‰çŸ¥è¯†ç‚¹ç»„ç»‡ï¼‰
            """
            requests = []
            for _, practice in test_df.iterrows():
                # é•¿æœŸè®°å¿†ï¼šæŒæ¡åº¦ä¿¡æ¯ï¼ˆä»… Mastery Only æ¨¡å¼ï¼‰
                mastery_summary = None
                if include_mastery and use_mastery and mastery_lookup and related_kc_map:
                    mastery_summary = build_mastery_summary(
                        student_id, 
                        practice['know_name'], 
                        related_kc_map, 
                        mastery_lookup, 
                        kc_descriptions
                    )

                # è·å–é¢˜ç›®é€‰é¡¹
                question_choices = get_question_choices(practice['question_id'], question_choices_df)
                
                # æ„å»ºPromptï¼ˆä¼ å…¥è¾…å¯¼å­—å…¸ï¼Œå†…éƒ¨ä¼šè‡ªåŠ¨åŒ¹é…ç›¸å…³çŸ¥è¯†ç‚¹ï¼‰
                system_prompt = profile.build_prompt()
                user_prompt = _build_agent_prompt(
                    practice,
                    all_kc_names,
                    question_choices,
                    mastery_summary=mastery_summary,
                    tutoring_dict=tutoring_content_dict
                )
                
                # è·å–å½“å‰é¢˜ç›®å®é™…ä½¿ç”¨çš„è¾…å¯¼å†…å®¹ï¼ˆç”¨äºæ—¥å¿—ï¼‰
                actual_tutoring_used = None
                if tutoring_content_dict:
                    actual_tutoring_used = tutoring_content_dict.get(practice['know_name'], None)
                
                request = {
                    "system_prompt": system_prompt,
                    "user_prompt": user_prompt,
                    "model_name": MODEL_NAME,
                    "practice_data": practice.to_dict(),
                    "mastery_summary": mastery_summary,
                    "tutoring_summary": actual_tutoring_used,  # è®°å½•å®é™…ä½¿ç”¨çš„è¾…å¯¼å†…å®¹
                    "question_choices": question_choices
                }
                requests.append(request)
            return requests
        
        def create_desc(label):
            return f"Student {student_id} ({label})" if label else f"Student {student_id}"
        
        async def execute_requests(llm_requests, experiment_label):
            llm_results = []
            if llm_requests:
                print(f"\nğŸš€ å¼€å§‹å‘é€ {len(llm_requests)} ä¸ªæµ‹è¯•é¢˜ç›®è¯·æ±‚ ({experiment_label})...")
                
                # ä½¿ç”¨æ–°çš„å¹¶å‘è°ƒç”¨å‡½æ•°ï¼ˆæ”¯æŒå‰Šå³°å¡«è°·ï¼‰
                # æ³¨æ„: ä¸åœ¨è¿™é‡Œé™åˆ¶å¹¶å‘ï¼Œç”±å…¨å±€ semaphore æ§åˆ¶
                llm_results = await create_concurrent_llm_requests(
                    llm_requests, 
                    concurrency_limit=999,  # ä¸é™åˆ¶ï¼Œè®©å…¨å±€ semaphore æ§åˆ¶
                    spread_duration=0  # ä¸åœ¨å•ä¸ªå­¦ç”Ÿçº§åˆ«å‰Šå³°
                )
                
                # ç»Ÿè®¡æˆåŠŸå’Œå¤±è´¥
                success_count = sum(1 for r in llm_results if r.get('error') is None)
                fail_count = len(llm_results) - success_count
                
                print(f"\nğŸ“ˆ å­¦ç”Ÿ {student_id} è¯·æ±‚å®Œæˆ ({experiment_label}):")
                print(f"   âœ… æˆåŠŸ: {success_count}/{len(llm_results)}")
                print(f"   âŒ å¤±è´¥: {fail_count}/{len(llm_results)}")
                
                # æ˜¾ç¤ºè¿›åº¦
                for _ in tqdm(range(len(llm_results)), desc=create_desc(experiment_label), position=position, leave=False):
                    pass

            # llm_results å·²ç»æ˜¯æŒ‰ç´¢å¼•æ’åºçš„

            for i, result in enumerate(llm_results):
                raw_resp = result.get('result')
                error = result.get('error')
                if error:
                    raw_resp = f"LLM_CALL_FAILED: {error}"
                    question_id = llm_requests[i]['practice_data'].get('question_id', 'Unknown')
                    print(f"   âš ï¸  é¢˜ç›® {question_id} è¯·æ±‚å¤±è´¥: {str(error)[:80]}")

                ans = _parse_llm_response(raw_resp)
                practice_data = llm_requests[i]['practice_data']
                question_choices = llm_requests[i].get('question_choices')
                
                # è·å–æ­£ç¡®ç­”æ¡ˆçš„choice_id
                correct_choice_id = None
                if question_choices:
                    for choice in question_choices:
                        if choice.get('is_correct'):
                            correct_choice_id = choice.get('choice_id')
                            break

                with open(prompt_log_path, "a", encoding="utf-8") as f:
                    f.write(f"--- PROMPT FOR STUDENT {student_id}, QUESTION {practice_data['question_id']} ({experiment_label or 'baseline'}) ---\n")
                    f.write("--- SYSTEM PROMPT ---\n" + llm_requests[i]['system_prompt'] + "\n\n")
                    f.write("--- USER PROMPT ---\n" + llm_requests[i]['user_prompt'] + "\n\n")
                    f.write("--- LLM RESPONSE ---\n" + str(raw_resp) + "\n" + "="*80 + "\n\n")

                student_results.append({
                    'student_id': student_id,
                    'question_id': practice_data['question_id'],
                    'true_know_name': practice_data['know_name'],
                    'true_score': practice_data['score'],
                    'true_answer_choice_id': correct_choice_id,
                    'true_answer_text': practice_data.get('answer_text', ''),
                    'predicted_task1_selfpredict': ans.get('task1'),
                    'predicted_task2_know_name': ans.get('task2'),
                    'predicted_task3_reasoning': ans.get('task3'),
                    'predicted_task4_answer_choice': ans.get('task4'),
                    'llm_raw_response': raw_resp,
                    'prompt_system': llm_requests[i].get('system_prompt', ''),
                    'prompt_user': llm_requests[i].get('user_prompt', ''),
                    'mastery_summary': llm_requests[i].get('mastery_summary'),
                    'tutoring_summary': llm_requests[i].get('tutoring_summary'),
                    'experiment_type': experiment_label or 'baseline',
                    'question_choices': str(question_choices) if question_choices else None
                })
                if overall_pbar:
                    overall_pbar.update(1)

        # æ ¹æ®å®éªŒæ¨¡å¼æ‰§è¡Œä¸åŒçš„è¯·æ±‚æ„å»º
        if mastery_lookup and related_kc_map and use_mastery:
            # Mastery Onlyæ¨¡å¼ï¼šæœ‰æŒæ¡åº¦ï¼ˆé•¿æœŸè®°å¿†ï¼‰ï¼Œæ— è¾…å¯¼ï¼ˆçŸ­æœŸè®°å¿†ï¼‰
            requests = build_requests(include_mastery=True, tutoring_content_dict=None)
            await execute_requests(requests, experiment_label='mastery_enhanced')
        else:
            # Baselineæ¨¡å¼ æˆ– Tutoring Onlyæ¨¡å¼
            # Baseline: æ— æŒæ¡åº¦ï¼Œæ— è¾…å¯¼
            # Tutoring Only: æ— æŒæ¡åº¦ï¼Œæœ‰è¾…å¯¼å­—å…¸ï¼ˆçŸ­æœŸè®°å¿†ï¼‰
            requests = build_requests(include_mastery=False, tutoring_content_dict=tutoring_dict)
            await execute_requests(requests, experiment_label='baseline' if not use_tutoring else 'tutoring_enhanced')
        
        print(f"{'='*60}")
        print(f"âœ… å­¦ç”Ÿ {student_id} å®éªŒå®Œæˆ")
        print(f"{'='*60}\n")
        
        return student_results

async def run_experiment(student_ids, all_student_records, concurrency_limit, prompt_log_path, all_kc_names, mastery_lookup=None, related_kc_map=None, kc_to_questions_map=None, question_text_map=None, recommendation_log_path=None, kc_descriptions=None, question_choices_df=None, use_mastery=True, use_tutoring=True, tutoring_lookup=None, spread_duration=0, on_student_complete=None):
    """
    å¹¶å‘åœ°å¯¹æŒ‡å®šå­¦ç”Ÿåˆ—è¡¨è¿è¡Œå®Œæ•´çš„ Agent æ¨¡æ‹Ÿå®éªŒã€‚
    
    æ–°æ¶æ„ï¼š
    1. æ”¶é›†æ‰€æœ‰å­¦ç”Ÿçš„æ‰€æœ‰é¢˜ç›®è¯·æ±‚åˆ°å…¨å±€è¯·æ±‚æ± 
    2. ä½¿ç”¨ concurrency_limit æ§åˆ¶å¹¶å‘ API è¯·æ±‚æ•°
    3. ä½¿ç”¨ spread_duration å°†æ‰€æœ‰è¯·æ±‚å‡åŒ€åˆ†æ•£ï¼ˆå‰Šå³°å¡«è°·ï¼‰
    
    Args:
        use_mastery: æ˜¯å¦ä½¿ç”¨æŒæ¡åº¦å¢å¼º
        use_tutoring: æ˜¯å¦ä½¿ç”¨è¾…å¯¼è¾“å‡º
        tutoring_lookup: é¢„åŠ è½½çš„è¾…å¯¼å†…å®¹å­—å…¸ {student_id: {kc_name: {...}}}
        spread_duration: è¯·æ±‚å‰Šå³°å¡«è°·æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œ0è¡¨ç¤ºç¦ç”¨
        on_student_complete: å›è°ƒå‡½æ•°ï¼Œåœ¨æ¯ä¸ªå­¦ç”Ÿå®Œæˆæ—¶è°ƒç”¨ï¼Œç­¾å: callback(student_id, student_results)
    """
    print("\n" + "="*80)
    print("ğŸ¤– é˜¶æ®µ 2/3: å¹¶å‘è¿è¡Œæ™ºèƒ½ä½“æ¨¡æ‹Ÿ (ç»Ÿä¸€è¯·æ±‚æ± æ¶æ„)".center(80))
    print("="*80)
    print(f"   ğŸ¯ ç›®æ ‡å­¦ç”Ÿæ•°: {len(student_ids)} å")
    print(f"   âš¡ APIå¹¶å‘åº¦: {concurrency_limit} ä¸ªå¹¶å‘è¯·æ±‚")
    print(f"   ğŸ¤– ä½¿ç”¨æ¨¡å‹: {MODEL_NAME}")
    print(f"   ğŸ§ª å®éªŒç±»å‹: {'Baseline + æŒæ¡åº¦å¢å¼º' if mastery_lookup else 'Baseline Only'}")
    if spread_duration > 0:
        print(f"   ğŸŒŠ å‰Šå³°å¡«è°·: å¼€å¯ - æ‰€æœ‰è¯·æ±‚åˆ†æ•£åˆ° {spread_duration} ç§’å†…")
    else:
        print(f"   ğŸŒŠ å‰Šå³°å¡«è°·: å…³é—­ - ç›´æ¥å¹¶å‘")
    print("-"*80)
    
    # å¤±è´¥æ—¥å¿—æ–‡ä»¶ï¼ˆæŒ‰æ¨¡å¼ä¸æ¨¡å‹åŒºåˆ†ï¼‰
    model_suffix = MODEL_NAME.replace('/', '_').replace('.', '_')
    exp_mode_label = 'mastery_only' if (use_mastery and not use_tutoring) else ('tutoring_only' if (use_tutoring and not use_mastery) else 'baseline')
    error_log_path = os.path.join(os.path.dirname(prompt_log_path), f"experiment_errors_{exp_mode_label}_{model_suffix}.txt")
    print(f"   ğŸ“ å¤±è´¥æ—¥å¿—: {error_log_path}")
    
    # ç¬¬ä¸€æ­¥ï¼šæ”¶é›†æ‰€æœ‰å­¦ç”Ÿçš„æ‰€æœ‰è¯·æ±‚åˆ°å…¨å±€è¯·æ±‚æ± 
    print("\nğŸ“¦ ç¬¬1æ­¥: æ”¶é›†æ‰€æœ‰å­¦ç”Ÿçš„è¯·æ±‚...")
    all_requests = []  # å…¨å±€è¯·æ±‚æ± 
    student_request_mapping = {}  # {student_id: [request_indices]}
    skipped_students = []  # ğŸ”¥ è®°å½•è¢«è·³è¿‡çš„å­¦ç”Ÿ
    
    # ğŸ”¥ ä½¿ç”¨ tqdm çš„ write æ–¹æ³•é¿å…å¹²æ‰°è¿›åº¦æ¡
    pbar = tqdm(student_ids, desc="å‡†å¤‡å­¦ç”Ÿè¯·æ±‚")
    for student_id in pbar:
        try:
            student_records_df = all_student_records[student_id]
            train_df, test_df = train_test_split(student_records_df, test_size=0.1, random_state=42, shuffle=True)
            
            # æ„å»º Profile
            profile = Profile(student_id, train_df, len(all_kc_names))
            
            # å‡†å¤‡è¾…å¯¼å†…å®¹ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            tutoring_dict = None
            if use_tutoring:
                # ğŸ”¥ åªä½¿ç”¨é¢„åŠ è½½çš„è¾…å¯¼å†…å®¹ï¼Œä¸æ”¯æŒå®æ—¶ç”Ÿæˆ
                if tutoring_lookup and student_id in tutoring_lookup:
                    # ä»é¢„åŠ è½½çš„æ•°æ®ä¸­æå–è¾…å¯¼å†…å®¹
                    tutoring_dict = {}
                    for kc_name, kc_data in tutoring_lookup[student_id].items():
                        content = kc_data.get('tutoring_content', '')
                        
                        # ğŸ”¥ æ•°æ®æ¸…æ´—ï¼šå¤„ç† NaN å’Œéå­—ç¬¦ä¸²ç±»å‹
                        if content is None or (isinstance(content, float) and pd.isna(content)):
                            content = ''  # NaN æˆ– None è½¬ä¸ºç©ºå­—ç¬¦ä¸²
                        elif not isinstance(content, str):
                            try:
                                content = str(content)  # å°è¯•è½¬æ¢ä¸ºå­—ç¬¦ä¸²
                            except:
                                content = ''  # è½¬æ¢å¤±è´¥åˆ™ä½¿ç”¨ç©ºå­—ç¬¦ä¸²
                        
                        tutoring_dict[kc_name] = content
                    
                    # ğŸ”¥ ä½¿ç”¨ tqdm.write é¿å…å¹²æ‰°è¿›åº¦æ¡
                    if len(tutoring_dict) > 0:
                        pbar.write(f"   âœ… å­¦ç”Ÿ {student_id}: å·²åŠ è½½ {len(tutoring_dict)} ä¸ªçŸ¥è¯†ç‚¹çš„è¾…å¯¼å†…å®¹")
                else:
                    # ğŸ”¥ å¦‚æœæ²¡æœ‰é¢„åŠ è½½æ•°æ®ï¼Œä½¿ç”¨ç©ºå­—å…¸ï¼ˆé€€åŒ–ä¸º baselineï¼‰
                    tutoring_dict = {}  # ç©ºå­—å…¸ï¼Œä¸ä¼šæ·»åŠ è¾…å¯¼å†…å®¹åˆ° Prompt
                    pbar.write(f"   âš ï¸  å­¦ç”Ÿ {student_id}: æœªæ‰¾åˆ°é¢„åŠ è½½çš„è¾…å¯¼å†…å®¹ï¼Œä½¿ç”¨ç©ºè¾…å¯¼ï¼ˆé€€åŒ–ä¸º baseline æ¨¡å¼ï¼‰")
                    skipped_students.append(student_id)  # ä»ç„¶è®°å½•ï¼Œä¾¿äºåç»­è¡¥å……æ•°æ®
            
            # ä¸ºè¯¥å­¦ç”Ÿçš„æ¯é“æµ‹è¯•é¢˜æ„å»ºè¯·æ±‚
            student_request_indices = []
            for _, practice in test_df.iterrows():
                # æ„å»ºæŒæ¡åº¦æ‘˜è¦ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                mastery_summary = None
                if use_mastery and mastery_lookup and related_kc_map:
                    mastery_summary = build_mastery_summary(
                        student_id, practice['know_name'],
                        related_kc_map, mastery_lookup, kc_descriptions
                    )
                
                # è·å–é¢˜ç›®é€‰é¡¹
                question_choices = get_question_choices(practice['question_id'], question_choices_df)
                
                # æ„å»º Prompt
                system_prompt = profile.build_prompt()
                user_prompt = _build_agent_prompt(
                    practice, all_kc_names, question_choices,
                    mastery_summary=mastery_summary,
                    tutoring_dict=tutoring_dict
                )
                
                # è·å–å®é™…ä½¿ç”¨çš„è¾…å¯¼å†…å®¹
                actual_tutoring_used = tutoring_dict.get(practice['know_name'], None) if tutoring_dict else None
                
                # æ·»åŠ åˆ°å…¨å±€è¯·æ±‚æ± 
                request_index = len(all_requests)
                all_requests.append({
                    "system_prompt": system_prompt,
                    "user_prompt": user_prompt,
                    "model_name": MODEL_NAME,
                    "student_id": student_id,
                    "practice_data": practice.to_dict(),
                    "mastery_summary": mastery_summary,
                    "tutoring_summary": actual_tutoring_used,
                    "question_choices": question_choices
                })
                student_request_indices.append(request_index)
            
            student_request_mapping[student_id] = student_request_indices
            
        except Exception as e:
            # ğŸ”¥ æ·»åŠ å¼‚å¸¸å¤„ç†ï¼Œé¿å…å•ä¸ªå­¦ç”Ÿå¤±è´¥å¯¼è‡´æ•´ä¸ªæµç¨‹å¡ä½
            pbar.write(f"   âŒ å­¦ç”Ÿ {student_id} å‡†å¤‡è¯·æ±‚å¤±è´¥: {e}")
            
            # ğŸ”¥ è®°å½•è¯¦ç»†é”™è¯¯ä¿¡æ¯åˆ°æ—¥å¿—æ–‡ä»¶
            import traceback
            error_details = traceback.format_exc()
            pbar.write(error_details)
            
            # å†™å…¥è¯¦ç»†è°ƒè¯•ä¿¡æ¯åˆ°é”™è¯¯æ—¥å¿—
            try:
                with open(error_log_path, 'a', encoding='utf-8') as f:
                    f.write(f"\n{'='*80}\n")
                    f.write(f"âŒ å­¦ç”Ÿ {student_id} å‡†å¤‡è¯·æ±‚å¤±è´¥\n")
                    f.write(f"æ—¶é—´: {pd.Timestamp.now()}\n")
                    f.write(f"é”™è¯¯: {e}\n")
                    f.write(f"\n--- å †æ ˆè·Ÿè¸ª ---\n")
                    f.write(error_details)
                    
                    # ğŸ”¥ è®°å½•è¾…å¯¼å†…å®¹ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
                    if use_tutoring and tutoring_dict:
                        f.write(f"\n--- è¾…å¯¼å†…å®¹ä¿¡æ¯ ---\n")
                        f.write(f"å·²åŠ è½½çŸ¥è¯†ç‚¹æ•°: {len(tutoring_dict)}\n")
                        f.write(f"çŸ¥è¯†ç‚¹åˆ—è¡¨: {list(tutoring_dict.keys())[:10]}...\n")
                        
                        # æ£€æŸ¥æ˜¯å¦æœ‰éå­—ç¬¦ä¸²ç±»å‹çš„è¾…å¯¼å†…å®¹
                        non_string_kcs = []
                        for kc, content in tutoring_dict.items():
                            if not isinstance(content, str):
                                non_string_kcs.append({
                                    'kc': kc,
                                    'type': type(content).__name__,
                                    'is_nan': pd.isna(content) if hasattr(pd, 'isna') else False,
                                    'value_preview': str(content)[:100]
                                })
                        
                        if non_string_kcs:
                            f.write(f"\nâš ï¸  å‘ç° {len(non_string_kcs)} ä¸ªéå­—ç¬¦ä¸²ç±»å‹çš„è¾…å¯¼å†…å®¹:\n")
                            for item in non_string_kcs[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
                                f.write(f"  - çŸ¥è¯†ç‚¹: {item['kc']}\n")
                                f.write(f"    ç±»å‹: {item['type']}\n")
                                f.write(f"    æ˜¯å¦NaN: {item['is_nan']}\n")
                                f.write(f"    å€¼é¢„è§ˆ: {item['value_preview']}\n")
                    
                    f.write(f"{'='*80}\n\n")
            except Exception as log_error:
                pbar.write(f"   âš ï¸  å†™å…¥é”™è¯¯æ—¥å¿—å¤±è´¥: {log_error}")
            
            continue
    
    pbar.close()
    
    total_requests = len(all_requests)
    successful_students = len(student_request_mapping)
    
    print(f"\nâœ… è¯·æ±‚æ”¶é›†å®Œæˆ")
    print(f"   ğŸ“Š æ€»å­¦ç”Ÿæ•°: {len(student_ids)} ä¸ª")
    print(f"   âœ… æˆåŠŸå‡†å¤‡: {successful_students} ä¸ªå­¦ç”Ÿ")
    print(f"   âš ï¸  ç¼ºè¾…å¯¼æ•°æ®: {len(skipped_students)} ä¸ªå­¦ç”Ÿï¼ˆå·²é€€åŒ–ä¸º baselineï¼‰")
    print(f"   ğŸ“ æ€»è¯·æ±‚æ•°: {total_requests} ä¸ª")
    
    if skipped_students:
        print(f"\nâš ï¸  ä»¥ä¸‹å­¦ç”Ÿç¼ºå°‘é¢„åŠ è½½è¾…å¯¼å†…å®¹ï¼ˆå·²è‡ªåŠ¨é€€åŒ–ä¸º baseline æ¨¡å¼ï¼‰:")
        if len(skipped_students) <= 20:
            print(f"   {skipped_students}")
        else:
            print(f"   {skipped_students[:20]} ... è¿˜æœ‰ {len(skipped_students) - 20} ä¸ª")
        print(f"\nğŸ’¡ æç¤º: è¿™äº›å­¦ç”Ÿä¼šç»§ç»­è¿è¡Œå®éªŒï¼ˆç­‰åŒäº baselineï¼‰ï¼Œå¦‚éœ€è¾…å¯¼å†…å®¹è¯·è¿è¡Œ:")
        print(f"   python generate_tutoring_content.py --student-ids {','.join(map(str, skipped_students[:5]))}... --use-mastery")
    
    if total_requests == 0:
        print(f"\nâŒ æ²¡æœ‰å¯æ‰§è¡Œçš„è¯·æ±‚ï¼Œå®éªŒç»ˆæ­¢")
        return pd.DataFrame()
    
    print("")
    
    # ç¬¬äºŒæ­¥ï¼šç»Ÿä¸€å¹¶å‘æ‰§è¡Œæ‰€æœ‰è¯·æ±‚
    print(f"ğŸš€ ç¬¬2æ­¥: å¹¶å‘æ‰§è¡Œæ‰€æœ‰è¯·æ±‚ï¼ˆå¹¶å‘åº¦: {concurrency_limit}ï¼‰...")
    llm_results = await create_concurrent_llm_requests(
        all_requests,
        concurrency_limit=concurrency_limit,
        spread_duration=spread_duration
    )
    
    # ç¬¬ä¸‰æ­¥ï¼šæŒ‰å­¦ç”Ÿåˆ†ç»„ç»“æœå¹¶è§¦å‘å›è°ƒ
    print(f"\nğŸ“Š ç¬¬3æ­¥: å¤„ç†ç»“æœå¹¶ä¿å­˜...")
    all_results = []
    for student_id in tqdm(student_ids, desc="å¤„ç†å­¦ç”Ÿç»“æœ"):
        # ğŸ”¥ è·³è¿‡å› å¼‚å¸¸æœªèƒ½æˆåŠŸå‡†å¤‡è¯·æ±‚çš„å­¦ç”Ÿï¼ˆæ³¨æ„ï¼šç¼ºå°‘è¾…å¯¼å†…å®¹çš„å­¦ç”Ÿå·²æ­£å¸¸å¤„ç†ï¼‰
        if student_id not in student_request_mapping:
            continue
        
        student_results = []
        request_indices = student_request_mapping[student_id]
        
        for req_idx in request_indices:
            result = llm_results[req_idx]
            request = all_requests[req_idx]
            
            raw_resp = result.get('result')
            error = result.get('error')
            if error:
                raw_resp = f"LLM_CALL_FAILED: {error}"
                # å†™å…¥å¤±è´¥æ—¥å¿—
                try:
                    with open(error_log_path, "a", encoding="utf-8") as ef:
                        practice_data = request.get('practice_data', {})
                        ef.write(f"--- FAILED REQUEST ---\n")
                        ef.write(f"Student ID: {practice_data.get('student_id', 'Unknown')}\n")
                        ef.write(f"Question ID: {practice_data.get('question_id', 'Unknown')}\n")
                        ef.write(f"KC: {practice_data.get('know_name', '')}\n")
                        ef.write(f"Error: {str(error)[:300]}\n")
                        ef.write("--- SYSTEM PROMPT ---\n")
                        ef.write(request.get('system_prompt', '') + "\n\n")
                        ef.write("--- USER PROMPT (truncated) ---\n")
                        user_p = request.get('user_prompt', '')
                        ef.write((user_p[:2000] + ('...' if len(user_p) > 2000 else '')) + "\n")
                        ef.write("="*80 + "\n\n")
                except Exception as _:
                    pass
            
            # è§£æå“åº”
            ans = _parse_llm_response(raw_resp)
            practice_data = request['practice_data']
            question_choices = request.get('question_choices')
            
            # è·å–æ­£ç¡®ç­”æ¡ˆ
            correct_choice_id = None
            if question_choices:
                for choice in question_choices:
                    if choice.get('is_correct'):
                        correct_choice_id = choice.get('choice_id')
                        break
            
            # è®°å½•æ—¥å¿—
            with open(prompt_log_path, "a", encoding="utf-8") as f:
                exp_label = 'mastery_enhanced' if use_mastery else ('tutoring_enhanced' if use_tutoring else 'baseline')
                f.write(f"--- PROMPT FOR STUDENT {student_id}, QUESTION {practice_data['question_id']} ({exp_label}) ---\n")
                f.write("--- SYSTEM PROMPT ---\n" + request['system_prompt'] + "\n\n")
                f.write("--- USER PROMPT ---\n" + request['user_prompt'] + "\n\n")
                f.write("--- LLM RESPONSE ---\n" + str(raw_resp) + "\n" + "="*80 + "\n\n")
            
            # ä¿å­˜ç»“æœ
            student_results.append({
                'student_id': student_id,
                'question_id': practice_data['question_id'],
                'true_know_name': practice_data['know_name'],
                'true_score': practice_data['score'],
                'true_answer_choice_id': correct_choice_id,
                'true_answer_text': practice_data.get('answer_text', ''),
                'predicted_task1_selfpredict': ans.get('task1'),
                'predicted_task2_know_name': ans.get('task2'),
                'predicted_task3_reasoning': ans.get('task3'),
                'predicted_task4_answer_choice': ans.get('task4'),
                'llm_raw_response': raw_resp,
                'prompt_system': request.get('system_prompt', ''),
                'prompt_user': request.get('user_prompt', ''),
                'mastery_summary': request.get('mastery_summary'),
                'tutoring_summary': request.get('tutoring_summary'),
                'experiment_type': 'mastery_enhanced' if use_mastery else ('tutoring_enhanced' if use_tutoring else 'baseline'),
                'question_choices': str(question_choices) if question_choices else None
            })
        
        all_results.extend(student_results)
        
        # è§¦å‘å›è°ƒï¼ˆå¢é‡ä¿å­˜ï¼‰
        if on_student_complete and student_results:
            on_student_complete(student_id, student_results)
    
    print(f"âœ… æ‰€æœ‰ç»“æœå¤„ç†å®Œæˆ\n")
    return pd.DataFrame(all_results)


# --- 5. ç»“æœè¯„ä¼° ---
def generate_three_mode_comparison_report(df, output_dir):
    """
    ç”Ÿæˆå››æ¨¡å¼ï¼ˆBaseline, Mastery Only, Tutoring Only, Bothï¼‰ç»¼åˆå¯¹æ¯”æŠ¥å‘Š
    åˆ†åˆ«è®¡ç®— Task1 å’Œ Task4 çš„æŒ‡æ ‡
    """
    if df.empty:
        print("âŒ æ•°æ®ä¸ºç©ºï¼Œæ— æ³•ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š")
        return
    
    # æ•°æ®é¢„å¤„ç†å‡½æ•°
    def normalize_yes_no(val):
        """å°† Yes/No è½¬æ¢ä¸º 1/0"""
        if isinstance(val, str):
            val_lower = val.strip().lower().replace('.', '')
            if val_lower == 'yes': return 1
            if val_lower == 'no': return 0
        return None
    
    def to_prob(val, high_confidence=0.95, low_confidence=0.05):
        """å°† 1/0 è½¬æ¢ä¸ºæ¦‚ç‡å€¼ç”¨äºè®¡ç®—äº¤å‰ç†µ"""
        if val == 1:
            return high_confidence
        elif val == 0:
            return low_confidence
        return 0.5
    
    def parse_answer_choice(val):
        """è§£æTask4çš„ç­”æ¡ˆé€‰æ‹© (A/B/C/D)"""
        if not isinstance(val, str):
            return None
        val = val.strip().upper()
        if val and val[0] in 'ABCDEFGH':
            return val[0]
        return None
    
    def check_answer_correctness(row):
        """æ£€æŸ¥Task4ç­”æ¡ˆæ˜¯å¦æ­£ç¡®"""
        predicted_choice = row.get('predicted_answer_choice')
        if predicted_choice is None:
            return None
        
        question_choices_str = row.get('question_choices')
        if not question_choices_str or question_choices_str == 'None':
            return None
        
        try:
            import ast
            choices = ast.literal_eval(question_choices_str)
            if not isinstance(choices, list) or len(choices) == 0:
                return None
            
            choice_index = ord(predicted_choice) - ord('A')
            if 0 <= choice_index < len(choices):
                predicted_choice_id = choices[choice_index].get('choice_id')
                correct_choice_id = row.get('true_answer_choice_id')
                return 1 if predicted_choice_id == correct_choice_id else 0
        except:
            pass
        
        return None
    
    # é¢„å¤„ç†æ•°æ®
    df = df.copy()
    df['task1_pred_normalized'] = df['predicted_task1_selfpredict'].apply(normalize_yes_no)
    df['predicted_answer_choice'] = df['predicted_task4_answer_choice'].apply(parse_answer_choice)
    df['task4_correct'] = df.apply(check_answer_correctness, axis=1)
    
    report_lines = []
    report_lines.append("=" * 80)
    report_lines.append("ğŸ“Š å››æ¨¡å¼ç»¼åˆå¯¹æ¯”æŠ¥å‘Š (Task1 & Task4 åˆ†ç¦»è¯„ä¼°)".center(80))
    report_lines.append("=" * 80)
    report_lines.append("")
    
    # åŸºæœ¬ä¿¡æ¯
    report_lines.append("ğŸ“‹ åŸºæœ¬ä¿¡æ¯")
    report_lines.append(f"   â€¢ ä½¿ç”¨æ¨¡å‹: {MODEL_NAME}")
    report_lines.append(f"   â€¢ æ€»æµ‹è¯•æ ·æœ¬æ•°: {len(df)}")
    report_lines.append(f"   â€¢ æµ‹è¯•å­¦ç”Ÿæ•°: {df['student_id'].nunique() if 'student_id' in df.columns else 'N/A'}")
    report_lines.append("")
    
    if 'experiment_mode' not in df.columns:
        print("âŒ ç¼ºå°‘ experiment_mode åˆ—ï¼Œæ— æ³•ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š")
        return
    
    modes = df['experiment_mode'].unique()
    mode_results = {}
    
    # ä¸ºæ¯ä¸ªæ¨¡å¼è®¡ç®— Task1 å’Œ Task4 æŒ‡æ ‡
    for mode in sorted(modes):
        mode_df = df[df['experiment_mode'] == mode].copy()
        
        # ========== Task1 æŒ‡æ ‡è®¡ç®— (è‡ªæˆ‘é¢„æµ‹ Yes/No) ==========
        task1_df = mode_df[mode_df['task1_pred_normalized'].notna()].copy()
        
        if len(task1_df) > 0:
            # Task1 ACC
            task1_acc = (task1_df['task1_pred_normalized'] == task1_df['true_score']).mean()
            
            # Task1 F1
            task1_f1 = f1_score(task1_df['true_score'], task1_df['task1_pred_normalized'], average='weighted')
            
            # Task1 Cross Entropy
            task1_df['task1_prob'] = task1_df['task1_pred_normalized'].apply(to_prob)
            task1_ce = log_loss(task1_df['true_score'], task1_df['task1_prob'])
            
            # Task1 æ··æ·†çŸ©é˜µ
            task1_tp = ((task1_df['true_score'] == 1) & (task1_df['task1_pred_normalized'] == 1)).sum()
            task1_fp = ((task1_df['true_score'] == 0) & (task1_df['task1_pred_normalized'] == 1)).sum()
            task1_tn = ((task1_df['true_score'] == 0) & (task1_df['task1_pred_normalized'] == 0)).sum()
            task1_fn = ((task1_df['true_score'] == 1) & (task1_df['task1_pred_normalized'] == 0)).sum()
        else:
            task1_acc = task1_f1 = task1_ce = 0
            task1_tp = task1_fp = task1_tn = task1_fn = 0
        
        # ========== Task4 æŒ‡æ ‡è®¡ç®— (ç­”æ¡ˆé€‰æ‹© A/B/C/D) ==========
        task4_df = mode_df[mode_df['task4_correct'].notna()].copy()
        
        if len(task4_df) > 0:
            # Task4 ACC
            task4_acc = task4_df['task4_correct'].mean()
            
            # Task4 F1
            task4_f1 = f1_score(task4_df['true_score'], task4_df['task4_correct'], average='weighted')
            
            # Task4 æ··æ·†çŸ©é˜µ
            task4_tp = ((task4_df['true_score'] == 1) & (task4_df['task4_correct'] == 1)).sum()
            task4_fp = ((task4_df['true_score'] == 0) & (task4_df['task4_correct'] == 1)).sum()
            task4_tn = ((task4_df['true_score'] == 0) & (task4_df['task4_correct'] == 0)).sum()
            task4_fn = ((task4_df['true_score'] == 1) & (task4_df['task4_correct'] == 0)).sum()
        else:
            task4_acc = task4_f1 = 0
            task4_tp = task4_fp = task4_tn = task4_fn = 0
        
        # ========== Task2 æŒ‡æ ‡è®¡ç®— (çŸ¥è¯†ç‚¹è¯†åˆ«) ==========
        task2_df = mode_df[mode_df['predicted_task2_know_name'].notna()].copy()
        if len(task2_df) > 0:
            task2_acc = (task2_df['predicted_task2_know_name'] == task2_df['true_know_name']).mean()
        else:
            task2_acc = 0
        
        # ä¿å­˜ç»“æœ
        mode_results[mode] = {
            # Task1 æŒ‡æ ‡
            'task1_acc': task1_acc,
            'task1_f1': task1_f1,
            'task1_ce': task1_ce,
            'task1_total': len(task1_df),
            'task1_tp': task1_tp,
            'task1_fp': task1_fp,
            'task1_tn': task1_tn,
            'task1_fn': task1_fn,
            'task1_df': task1_df,  # ä¿å­˜ DataFrame ç”¨äºç”Ÿæˆåˆ†ç±»æŠ¥å‘Š
            
            # Task4 æŒ‡æ ‡
            'task4_acc': task4_acc,
            'task4_f1': task4_f1,
            'task4_total': len(task4_df),
            'task4_tp': task4_tp,
            'task4_fp': task4_fp,
            'task4_tn': task4_tn,
            'task4_fn': task4_fn,
            'task4_df': task4_df,  # ä¿å­˜ DataFrame ç”¨äºç”Ÿæˆåˆ†ç±»æŠ¥å‘Š
            
            # Task2 æŒ‡æ ‡
            'task2_acc': task2_acc,
        }
    
    # ========== è¾“å‡ºè¯¦ç»†æŠ¥å‘Š ==========
    report_lines.append("ğŸ¯ ä¸‰æ¨¡å¼æŒ‡æ ‡å¯¹æ¯”")
    report_lines.append("-" * 80)
    report_lines.append("")
    
    # è·å– baseline çš„æŒ‡æ ‡ç”¨äºå¯¹æ¯”
    baseline_task1_acc = mode_results.get('baseline', {}).get('task1_acc', 0)
    baseline_task4_acc = mode_results.get('baseline', {}).get('task4_acc', 0)
    
    for mode in ['baseline', 'mastery_only', 'tutoring_only', 'both']:
        if mode not in mode_results:
            continue
        
        result = mode_results[mode]
        
        # å›¾æ ‡å’Œæ ‡ç­¾
        if mode == 'baseline':
            icon = "ğŸ”µ"
            label = "BASELINE (æ— æŒæ¡åº¦ + æ— è¾…å¯¼)"
        elif mode == 'mastery_only':
            icon = "ğŸŸ¢"
            label = "MASTERY ONLY (æœ‰æŒæ¡åº¦ + æ— è¾…å¯¼)"
        elif mode == 'tutoring_only':
            icon = "ğŸŸ¡"
            label = "TUTORING ONLY (æ— æŒæ¡åº¦ + æœ‰è¾…å¯¼)"
        else:
            icon = "ğŸŸ£"
            label = "BOTH (æœ‰æŒæ¡åº¦ + æœ‰è¾…å¯¼)"
        
        report_lines.append(f"{icon} {label}")
        report_lines.append("=" * 80)
        report_lines.append("")
        
        # ========== Task1 æŒ‡æ ‡ (è‡ªæˆ‘é¢„æµ‹) ==========
        report_lines.append(f"   ğŸ“ Task1: è‡ªæˆ‘é¢„æµ‹ (Self-Prediction) - å­¦ç”Ÿé¢„æµ‹èƒ½å¦ç­”å¯¹ (Yes/No)")
        report_lines.append(f"      â€¢ å‡†ç¡®ç‡ (ACC):        {result['task1_acc']:.2%}")
        
        # ä¸baselineå¯¹æ¯”
        if mode != 'baseline' and baseline_task1_acc > 0:
            diff = result['task1_acc'] - baseline_task1_acc
            arrow = "â¬†ï¸" if diff > 0 else "â¬‡ï¸" if diff < 0 else "â¡ï¸"
            report_lines.append(f"        ç›¸æ¯”Baseline:       {diff:+.2%} {arrow}")
        
        report_lines.append(f"      â€¢ F1-Score:            {result['task1_f1']:.4f}")
        report_lines.append(f"      â€¢ äº¤å‰ç†µ (Cross Entropy): {result['task1_ce']:.4f}")
        report_lines.append(f"      â€¢ æœ‰æ•ˆæ ·æœ¬æ•°:          {result['task1_total']}")
        report_lines.append("")
        
        # ========== Task4 æŒ‡æ ‡ (ç­”æ¡ˆé€‰æ‹©) ==========
        report_lines.append(f"   âœï¸  Task4: ç­”æ¡ˆé€‰æ‹© (Answer Choice) - æœ€ç»ˆé€‰æ‹©çš„ç­”æ¡ˆ (A/B/C/D)")
        report_lines.append(f"      â€¢ å‡†ç¡®ç‡ (ACC):        {result['task4_acc']:.2%}")
        
        # ä¸baselineå¯¹æ¯”
        if mode != 'baseline' and baseline_task4_acc > 0:
            diff = result['task4_acc'] - baseline_task4_acc
            arrow = "â¬†ï¸" if diff > 0 else "â¬‡ï¸" if diff < 0 else "â¡ï¸"
            report_lines.append(f"        ç›¸æ¯”Baseline:       {diff:+.2%} {arrow}")
        
        report_lines.append(f"      â€¢ F1-Score:            {result['task4_f1']:.4f}")
        report_lines.append(f"      â€¢ æœ‰æ•ˆæ ·æœ¬æ•°:          {result['task4_total']}")
        report_lines.append("")
        
        # ========== Task2 æŒ‡æ ‡ (çŸ¥è¯†ç‚¹è¯†åˆ«) ==========
        report_lines.append(f"   ğŸ¯ Task2: çŸ¥è¯†ç‚¹è¯†åˆ« (KC Recognition)")
        report_lines.append(f"      â€¢ å‡†ç¡®ç‡ (ACC):        {result['task2_acc']:.2%}")
        report_lines.append("")
        report_lines.append("")
    
    # ========== Task1 è¯¦ç»†åˆ†ç±»æŠ¥å‘Š ==========
    report_lines.append("ğŸ“Š Task1 è¯¦ç»†åˆ†ç±»æŠ¥å‘Š (è‡ªæˆ‘é¢„æµ‹)")
    report_lines.append("=" * 80)
    report_lines.append("")
    
    for mode in ['baseline', 'mastery_only', 'tutoring_only', 'both']:
        if mode not in mode_results:
            continue
        
        result = mode_results[mode]
        task1_df = result.get('task1_df')
        
        if mode == 'baseline':
            label = "BASELINE"
            icon = "ğŸ”µ"
        elif mode == 'mastery_only':
            label = "MASTERY ONLY"
            icon = "ğŸŸ¢"
        elif mode == 'tutoring_only':
            label = "TUTORING ONLY"
            icon = "ğŸŸ¡"
        else:
            label = "BOTH"
            icon = "ğŸŸ£"
        
        report_lines.append(f"{icon} {label}")
        report_lines.append("-" * 80)
        
        if task1_df is not None and len(task1_df) > 0:
            report_lines.append(classification_report(
                task1_df['true_score'], 
                task1_df['task1_pred_normalized'], 
                target_names=['é¢„æµ‹é”™è¯¯', 'é¢„æµ‹æ­£ç¡®']
            ))
        else:
            report_lines.append("   âš ï¸  æ— å¯ç”¨çš„ Task1 æ•°æ®")
        
        report_lines.append("")
    
    # ========== Task4 è¯¦ç»†åˆ†ç±»æŠ¥å‘Š ==========
    report_lines.append("ğŸ“Š Task4 è¯¦ç»†åˆ†ç±»æŠ¥å‘Š (ç­”æ¡ˆé€‰æ‹©)")
    report_lines.append("=" * 80)
    report_lines.append("")
    
    for mode in ['baseline', 'mastery_only', 'tutoring_only', 'both']:
        if mode not in mode_results:
            continue
        
        result = mode_results[mode]
        task4_df = result.get('task4_df')
        
        if mode == 'baseline':
            label = "BASELINE"
            icon = "ğŸ”µ"
        elif mode == 'mastery_only':
            label = "MASTERY ONLY"
            icon = "ğŸŸ¢"
        elif mode == 'tutoring_only':
            label = "TUTORING ONLY"
            icon = "ğŸŸ¡"
        else:
            label = "BOTH"
            icon = "ğŸŸ£"
        
        report_lines.append(f"{icon} {label}")
        report_lines.append("-" * 80)
        
        if task4_df is not None and len(task4_df) > 0:
            report_lines.append(classification_report(
                task4_df['true_score'], 
                task4_df['task4_correct'], 
                target_names=['é¢„æµ‹é”™è¯¯', 'é¢„æµ‹æ­£ç¡®']
            ))
        else:
            report_lines.append("   âš ï¸  æ— å¯ç”¨çš„ Task4 æ•°æ®")
        
        report_lines.append("")
    
    # ========== è¯¦ç»†æ··æ·†çŸ©é˜µ ==========
    report_lines.append("ğŸ“Š è¯¦ç»†æ··æ·†çŸ©é˜µ (Confusion Matrix)")
    report_lines.append("=" * 80)
    report_lines.append("")
    
    for mode in ['baseline', 'mastery_only', 'tutoring_only', 'both']:
        if mode not in mode_results:
            continue
        
        result = mode_results[mode]
        
        if mode == 'baseline':
            label = "BASELINE"
            icon = "ğŸ”µ"
        elif mode == 'mastery_only':
            label = "MASTERY ONLY"
            icon = "ğŸŸ¢"
        elif mode == 'tutoring_only':
            label = "TUTORING ONLY"
            icon = "ğŸŸ¡"
        else:
            label = "BOTH"
            icon = "ğŸŸ£"
        
        report_lines.append(f"{icon} {label}")
        report_lines.append("-" * 80)
        
        # Task1 æ··æ·†çŸ©é˜µ
        report_lines.append(f"   Task1 (è‡ªæˆ‘é¢„æµ‹):")
        report_lines.append(f"                é¢„æµ‹æ­£ç¡®(Yes)  é¢„æµ‹é”™è¯¯(No)")
        report_lines.append(f"   å®é™…æ­£ç¡®:       {result['task1_tp']:6d}         {result['task1_fn']:6d}")
        report_lines.append(f"   å®é™…é”™è¯¯:       {result['task1_fp']:6d}         {result['task1_tn']:6d}")
        report_lines.append("")
        
        # Task4 æ··æ·†çŸ©é˜µ
        report_lines.append(f"   Task4 (ç­”æ¡ˆé€‰æ‹©):")
        report_lines.append(f"                é€‰å¯¹ç­”æ¡ˆ       é€‰é”™ç­”æ¡ˆ")
        report_lines.append(f"   å®é™…æ­£ç¡®:       {result['task4_tp']:6d}         {result['task4_fn']:6d}")
        report_lines.append(f"   å®é™…é”™è¯¯:       {result['task4_fp']:6d}         {result['task4_tn']:6d}")
        report_lines.append("")
        report_lines.append("")
    
    # ========== ç»“è®ºä¸åˆ†æ ==========
    report_lines.append("=" * 80)
    report_lines.append("ğŸ’¡ ç»“è®ºä¸åˆ†æ".center(80))
    report_lines.append("=" * 80)
    report_lines.append("")
    
    # æ‰¾å‡ºæœ€ä½³æ¨¡å¼
    if mode_results:
        # åˆ†åˆ«æ‰¾å‡º Task1 å’Œ Task4 çš„æœ€ä½³æ¨¡å¼
        best_task1_mode = max(mode_results.items(), key=lambda x: x[1]['task1_acc'])
        best_task4_mode = max(mode_results.items(), key=lambda x: x[1]['task4_acc'])
        
        mode_name_map = {
            'baseline': 'Baselineï¼ˆæ— å¢å¼ºï¼‰',
            'mastery_only': 'Mastery Onlyï¼ˆä»…æŒæ¡åº¦ï¼‰',
            'tutoring_only': 'Tutoring Onlyï¼ˆä»…è¾…å¯¼ï¼‰',
            'both': 'Bothï¼ˆæŒæ¡åº¦+è¾…å¯¼ï¼‰'
        }
        
        report_lines.append(f"ğŸ† æœ€ä½³æ¨¡å¼ç»Ÿè®¡:")
        report_lines.append(f"   â€¢ Task1 (è‡ªæˆ‘é¢„æµ‹): {mode_name_map.get(best_task1_mode[0], best_task1_mode[0])} - ACC: {best_task1_mode[1]['task1_acc']:.2%}")
        report_lines.append(f"   â€¢ Task4 (ç­”æ¡ˆé€‰æ‹©): {mode_name_map.get(best_task4_mode[0], best_task4_mode[0])} - ACC: {best_task4_mode[1]['task4_acc']:.2%}")
        report_lines.append("")
        
        # è¯¦ç»†åˆ†æ
        if 'mastery_only' in mode_results and 'tutoring_only' in mode_results:
            baseline_res = mode_results.get('baseline', {})
            mastery_res = mode_results['mastery_only']
            tutoring_res = mode_results['tutoring_only']
            
            report_lines.append("ğŸ“ˆ å…³é”®å‘ç°:")
            report_lines.append("")
            
            # Task1 åˆ†æ
            report_lines.append("   ã€Task1: è‡ªæˆ‘é¢„æµ‹èƒ½åŠ›ã€‘")
            if mastery_res['task1_acc'] > baseline_res.get('task1_acc', 0):
                diff = mastery_res['task1_acc'] - baseline_res.get('task1_acc', 0)
                report_lines.append(f"   âœ… Masteryæ¨¡å¼ç›¸æ¯”Baselineæå‡äº† {diff:.2%}")
                report_lines.append(f"      â†’ æŒæ¡åº¦å¢å¼ºæ˜¾è‘—æå‡äº†å­¦ç”Ÿçš„è‡ªæˆ‘è®¤çŸ¥å‡†ç¡®æ€§")
            else:
                diff = mastery_res['task1_acc'] - baseline_res.get('task1_acc', 0)
                report_lines.append(f"   âš ï¸  Masteryæ¨¡å¼ç›¸æ¯”Baselineå˜åŒ– {diff:+.2%}")
            
            if tutoring_res['task1_acc'] > baseline_res.get('task1_acc', 0):
                diff = tutoring_res['task1_acc'] - baseline_res.get('task1_acc', 0)
                report_lines.append(f"   âœ… Tutoringæ¨¡å¼ç›¸æ¯”Baselineæå‡äº† {diff:.2%}")
                report_lines.append(f"      â†’ è¾…å¯¼è¾“å‡ºå¸®åŠ©å­¦ç”Ÿæ›´å‡†ç¡®åœ°è¯„ä¼°è‡ªå·±çš„èƒ½åŠ›")
            else:
                diff = tutoring_res['task1_acc'] - baseline_res.get('task1_acc', 0)
                report_lines.append(f"   âš ï¸  Tutoringæ¨¡å¼ç›¸æ¯”Baselineå˜åŒ– {diff:+.2%}")
            
            report_lines.append("")
            
            # Task4 åˆ†æ
            report_lines.append("   ã€Task4: å®é™…åšé¢˜èƒ½åŠ›ã€‘")
            if mastery_res['task4_acc'] > baseline_res.get('task4_acc', 0):
                diff = mastery_res['task4_acc'] - baseline_res.get('task4_acc', 0)
                report_lines.append(f"   âœ… Masteryæ¨¡å¼ç›¸æ¯”Baselineæå‡äº† {diff:.2%}")
                report_lines.append(f"      â†’ æŒæ¡åº¦è¯„ä¼°æœ‰åŠ©äºæå‡å®é™…åšé¢˜æ­£ç¡®ç‡")
            else:
                diff = mastery_res['task4_acc'] - baseline_res.get('task4_acc', 0)
                report_lines.append(f"   âš ï¸  Masteryæ¨¡å¼ç›¸æ¯”Baselineå˜åŒ– {diff:+.2%}")
            
            if tutoring_res['task4_acc'] > baseline_res.get('task4_acc', 0):
                diff = tutoring_res['task4_acc'] - baseline_res.get('task4_acc', 0)
                report_lines.append(f"   âœ… Tutoringæ¨¡å¼ç›¸æ¯”Baselineæå‡äº† {diff:.2%}")
                report_lines.append(f"      â†’ è¾…å¯¼è¾“å‡ºç›´æ¥æå‡äº†åšé¢˜æ­£ç¡®ç‡")
            else:
                diff = tutoring_res['task4_acc'] - baseline_res.get('task4_acc', 0)
                report_lines.append(f"   âš ï¸  Tutoringæ¨¡å¼ç›¸æ¯”Baselineå˜åŒ– {diff:+.2%}")
            
            report_lines.append("")
            
            # æ¨¡å¼å¯¹æ¯”
            if mastery_res['task4_acc'] > tutoring_res['task4_acc']:
                diff = mastery_res['task4_acc'] - tutoring_res['task4_acc']
                report_lines.append(f"   ğŸ“Š Masteryæ¨¡å¼æ¯”Tutoringæ¨¡å¼åœ¨åšé¢˜å‡†ç¡®ç‡ä¸Šé«˜ {diff:.2%}")
                report_lines.append(f"      â†’ æŒæ¡åº¦è¯„ä¼°å¯¹æå‡åšé¢˜æ­£ç¡®ç‡çš„æ•ˆæœæ›´æ˜æ˜¾")
            elif tutoring_res['task4_acc'] > mastery_res['task4_acc']:
                diff = tutoring_res['task4_acc'] - mastery_res['task4_acc']
                report_lines.append(f"   ğŸ“Š Tutoringæ¨¡å¼æ¯”Masteryæ¨¡å¼åœ¨åšé¢˜å‡†ç¡®ç‡ä¸Šé«˜ {diff:.2%}")
                report_lines.append(f"      â†’ è¾…å¯¼è¾“å‡ºå¯¹æå‡åšé¢˜æ­£ç¡®ç‡çš„æ•ˆæœæ›´æ˜æ˜¾")
    
    report_lines.append("")
    report_lines.append("=" * 80)
    
    # ========== æ·»åŠ æ±‡æ€»å¯¹æ¯”è¡¨ ==========
    report_lines.append("")
    report_lines.append("ğŸ“‹ æŒ‡æ ‡æ±‡æ€»å¯¹æ¯”è¡¨")
    report_lines.append("=" * 80)
    report_lines.append("")
    
    # è¡¨å¤´
    report_lines.append("æ¨¡å¼              | Task1 ACC | Task1 F1  | Task1 CE  | Task4 ACC | Task4 F1  | Task2 ACC")
    report_lines.append("-" * 95)
    
    # æ¯ä¸ªæ¨¡å¼çš„æ•°æ®è¡Œ
    for mode in ['baseline', 'mastery_only', 'tutoring_only', 'both']:
        if mode not in mode_results:
            continue
        
        result = mode_results[mode]
        
        if mode == 'baseline':
            mode_label = "Baseline      "
        elif mode == 'mastery_only':
            mode_label = "Mastery Only  "
        elif mode == 'tutoring_only':
            mode_label = "Tutoring Only "
        else:
            mode_label = "Both          "
        
        line = (f"{mode_label} | "
                f"{result['task1_acc']:8.2%} | "
                f"{result['task1_f1']:8.4f} | "
                f"{result['task1_ce']:8.4f} | "
                f"{result['task4_acc']:8.2%} | "
                f"{result['task4_f1']:8.4f} | "
                f"{result['task2_acc']:8.2%}")
        report_lines.append(line)
    
    report_lines.append("")
    report_lines.append("è¯´æ˜:")
    report_lines.append("  â€¢ Task1: è‡ªæˆ‘é¢„æµ‹å‡†ç¡®æ€§ (å­¦ç”Ÿé¢„æµ‹èƒ½å¦ç­”å¯¹)")
    report_lines.append("  â€¢ Task4: å®é™…ç­”é¢˜å‡†ç¡®æ€§ (æœ€ç»ˆç­”æ¡ˆæ˜¯å¦æ­£ç¡®)")
    report_lines.append("  â€¢ Task2: çŸ¥è¯†ç‚¹è¯†åˆ«å‡†ç¡®æ€§")
    report_lines.append("  â€¢ ACC: å‡†ç¡®ç‡, F1: F1åˆ†æ•°, CE: äº¤å‰ç†µ (è¶Šä½è¶Šå¥½)")
    report_lines.append("")
    report_lines.append("=" * 80)
    
    # ä¿å­˜æŠ¥å‘Š
    report_path = os.path.join(output_dir, 'three_mode_comparison_report.txt')
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write('\n'.join(report_lines))
    
    # æ‰“å°åˆ°æ§åˆ¶å°
    for line in report_lines:
        print(line)
    
    print("")
    print(f"âœ… ä¸‰æ¨¡å¼ç»¼åˆå¯¹æ¯”æŠ¥å‘Šå·²ä¿å­˜è‡³: {report_path}")
    print("")

def save_in_out_cases(combined_df, output_dir, all_student_records, kcs_df, kc_relationships_df, 
                      kc_to_questions_map, question_text_map, kc_descriptions, question_choices_df,
                      mastery_lookup, tutoring_lookup, related_kc_map, all_kc_names):
    """
    æ‰¾åˆ°3ä¸ªåŒæ—¶æ‹¥æœ‰æŒæ¡åº¦å’Œè¾…å¯¼å†…å®¹çš„å­¦ç”Ÿï¼Œä¿å­˜ä»–ä»¬çš„å®Œæ•´è¾“å…¥è¾“å‡ºæ¡ˆä¾‹ã€‚
    
    æ¡ˆä¾‹åŒ…æ‹¬ï¼š
    1. æŒæ¡åº¦è¯„ä¼°çš„è¾“å…¥è¾“å‡º
    2. è¾…å¯¼å†…å®¹ç”Ÿæˆçš„è¾“å…¥è¾“å‡º
    3. è¯„æµ‹æ™ºèƒ½ä½“çš„è¾“å…¥è¾“å‡º
    """
    print("\n" + "="*80)
    print("ğŸ“ æ”¶é›†è¾“å…¥è¾“å‡ºæ¡ˆä¾‹".center(80))
    print("="*80)
    
    # ç­›é€‰åŒæ—¶æœ‰æŒæ¡åº¦å’Œè¾…å¯¼å†…å®¹çš„è®°å½•
    filtered_df = combined_df[
        (combined_df['mastery_summary'].notna()) & 
        (combined_df['mastery_summary'] != '') &
        (combined_df['tutoring_summary'].notna()) &
        (combined_df['tutoring_summary'] != '')
    ].copy()
    
    if filtered_df.empty:
        print("âš ï¸  æœªæ‰¾åˆ°åŒæ—¶æ‹¥æœ‰æŒæ¡åº¦å’Œè¾…å¯¼å†…å®¹çš„å­¦ç”Ÿè®°å½•")
        return
    
    # æŒ‰å­¦ç”Ÿåˆ†ç»„ï¼Œé€‰æ‹©å‰3ä¸ª
    student_ids = filtered_df['student_id'].unique()[:3]
    
    # è½¬æ¢ numpy ç±»å‹ä¸º Python åŸç”Ÿç±»å‹
    student_ids = [int(sid) for sid in student_ids]
    
    print(f"âœ… æ‰¾åˆ° {len(student_ids)} ä¸ªç¬¦åˆæ¡ä»¶çš„å­¦ç”Ÿ: {student_ids}")
    
    cases = []
    
    for student_id in student_ids:
        print(f"\nğŸ“‹ å¤„ç†å­¦ç”Ÿ {student_id}...")
        
        student_df = filtered_df[filtered_df['student_id'] == student_id].iloc[0]
        student_records_df = all_student_records[student_id]
        
        # è·å–è®­ç»ƒé›†å’Œæµ‹è¯•é›†
        from sklearn.model_selection import train_test_split
        train_df, test_df = train_test_split(student_records_df, test_size=0.1, random_state=42, shuffle=True)
        
        case = {
            'student_id': int(student_id),
            'mastery_assessment': {},
            'tutoring_generation': {},
            'agent_evaluation': {}
        }
        
        # ===== 1. æŒæ¡åº¦è¯„ä¼°æ¡ˆä¾‹ =====
        # éœ€è¦ä» mastery_lookup é‡å»ºè¾“å…¥
        if mastery_lookup and student_id in mastery_lookup:
            kc_name = student_df['true_know_name']
            
            # é‡å»ºæŒæ¡åº¦è¯„ä¼°çš„è¾“å…¥ï¼ˆä» assess_mastery.py ä¸­æå–é€»è¾‘ï¼‰
            # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œç›´æ¥ä» mastery_lookup è·å–ç»“æœ
            mastery_info = mastery_lookup[student_id].get(kc_name, {})
            
            case['mastery_assessment'] = {
                'input': {
                    'student_id': student_id,
                    'kc_name': kc_name,
                    'note': 'æŒæ¡åº¦è¯„ä¼°çš„å®Œæ•´è¾“å…¥éœ€è¦æŸ¥çœ‹ mastery_assessment_results.csv æˆ–æ—¥å¿—æ–‡ä»¶'
                },
                'output': {
                    'mastery_level': mastery_info.get('mastery_level', 'N/A'),
                    'rationale': mastery_info.get('rationale', ''),
                    'suggestions': mastery_info.get('suggestions', '')
                },
                'full_summary': student_df['mastery_summary']
            }
        
        # ===== 2. è¾…å¯¼å†…å®¹ç”Ÿæˆæ¡ˆä¾‹ =====
        if tutoring_lookup and student_id in tutoring_lookup:
            kc_name = student_df['true_know_name']
            tutoring_info = tutoring_lookup[student_id].get(kc_name, {})
            
            case['tutoring_generation'] = {
                'input': {
                    'system_prompt': tutoring_info.get('prompt_system', ''),
                    'user_prompt': tutoring_info.get('prompt_user', ''),
                },
                'output': {
                    'tutoring_content': tutoring_info.get('tutoring_content', ''),
                    'llm_raw_response': tutoring_info.get('llm_raw_response', ''),
                    'example_question_ids': tutoring_info.get('example_question_ids', [])
                },
                'full_summary': student_df['tutoring_summary']
            }
        
        # ===== 3. è¯„æµ‹æ™ºèƒ½ä½“æ¡ˆä¾‹ =====
        # é‡å»º Agent Prompt
        profile = Profile(student_id, train_df, len(all_kc_names))
        
        practice = {
            'question_id': student_df['question_id'],
            'exer_content': student_records_df[student_records_df['question_id'] == student_df['question_id']].iloc[0]['exer_content'],
            'know_name': student_df['true_know_name']
        }
        
        question_choices = get_question_choices(student_df['question_id'], question_choices_df)
        
        mastery_summary = build_mastery_summary(
            student_id, 
            student_df['true_know_name'],
            related_kc_map,
            mastery_lookup,
            kc_descriptions
        )
        
        # æ„å»ºè¾…å¯¼å­—å…¸
        tutoring_dict = {}
        if tutoring_lookup and student_id in tutoring_lookup:
            for kc, info in tutoring_lookup[student_id].items():
                tutoring_dict[kc] = info.get('tutoring_content', '')
        
        system_prompt = profile.build_prompt()
        user_prompt = _build_agent_prompt(
            practice,
            all_kc_names,
            question_choices,
            mastery_summary=mastery_summary,
            tutoring_dict=tutoring_dict
        )
        
        case['agent_evaluation'] = {
            'input': {
                'system_prompt': system_prompt,
                'user_prompt': user_prompt,
                'question_id': student_df['question_id'],
                'kc_name': student_df['true_know_name']
            },
            'output': {
                'llm_raw_response': student_df['llm_raw_response'],
                'task1_selfpredict': student_df['predicted_task1_selfpredict'],
                'task2_know_name': student_df['predicted_task2_know_name'],
                'task3_reasoning': student_df['predicted_task3_reasoning'],
                'task4_answer_choice': student_df['predicted_task4_answer_choice']
            },
            'ground_truth': {
                'true_score': student_df['true_score'],
                'true_know_name': student_df['true_know_name'],
                'true_answer_choice_id': student_df.get('true_answer_choice_id', None)
            }
        }
        
        cases.append(case)
    
    # ä¿å­˜æ¡ˆä¾‹åˆ° JSON æ–‡ä»¶
    import json
    import numpy as np
    
    # å®šä¹‰è‡ªå®šä¹‰ JSON ç¼–ç å™¨ï¼Œå¤„ç† numpy ç±»å‹
    class NumpyEncoder(json.JSONEncoder):
        def default(self, obj):
            if isinstance(obj, np.integer):
                return int(obj)
            elif isinstance(obj, np.floating):
                return float(obj)
            elif isinstance(obj, np.ndarray):
                return obj.tolist()
            elif isinstance(obj, np.bool_):
                return bool(obj)
            elif pd.isna(obj):
                return None
            return super(NumpyEncoder, self).default(obj)
    
    case_json_path = os.path.join(output_dir, 'in_out_cases.json')
    with open(case_json_path, 'w', encoding='utf-8') as f:
        json.dump(cases, f, ensure_ascii=False, indent=2, cls=NumpyEncoder)
    
    print(f"\nâœ… æ¡ˆä¾‹å·²ä¿å­˜è‡³: {case_json_path}")
    
    # åŒæ—¶æ‰“å°åˆ°æ§åˆ¶å°
    print("\n" + "="*80)
    print("ğŸ“„ è¾“å…¥è¾“å‡ºæ¡ˆä¾‹è¯¦æƒ…".center(80))
    print("="*80)
    
    for i, case in enumerate(cases, 1):
        print(f"\n{'='*80}")
        print(f"æ¡ˆä¾‹ {i}: å­¦ç”Ÿ {case['student_id']}")
        print(f"{'='*80}")
        
        # æŒæ¡åº¦è¯„ä¼°
        print(f"\nã€1. æŒæ¡åº¦è¯„ä¼°ã€‘")
        print(f"è¾“å…¥: çŸ¥è¯†ç‚¹ = {case['mastery_assessment']['input']['kc_name']}")
        print(f"è¾“å‡º: æŒæ¡ç­‰çº§ = {case['mastery_assessment']['output']['mastery_level']}")
        print(f"ç†ç”±: {case['mastery_assessment']['output']['rationale'][:200]}...")
        
        # è¾…å¯¼å†…å®¹ç”Ÿæˆ
        print(f"\nã€2. è¾…å¯¼å†…å®¹ç”Ÿæˆã€‘")
        print(f"ç³»ç»Ÿæç¤ºè¯é•¿åº¦: {len(case['tutoring_generation']['input']['system_prompt'])} å­—ç¬¦")
        print(f"ç”¨æˆ·æç¤ºè¯é•¿åº¦: {len(case['tutoring_generation']['input']['user_prompt'])} å­—ç¬¦")
        print(f"è¾“å‡ºå†…å®¹é•¿åº¦: {len(case['tutoring_generation']['output']['tutoring_content'])} å­—ç¬¦")
        print(f"ç¤ºä¾‹é¢˜ç›®ID: {case['tutoring_generation']['output']['example_question_ids']}")
        
        # è¯„æµ‹æ™ºèƒ½ä½“
        print(f"\nã€3. è¯„æµ‹æ™ºèƒ½ä½“ã€‘")
        print(f"é¢˜ç›®ID: {case['agent_evaluation']['input']['question_id']}")
        print(f"çŸ¥è¯†ç‚¹: {case['agent_evaluation']['input']['kc_name']}")
        print(f"ç³»ç»Ÿæç¤ºè¯é•¿åº¦: {len(case['agent_evaluation']['input']['system_prompt'])} å­—ç¬¦")
        print(f"ç”¨æˆ·æç¤ºè¯é•¿åº¦: {len(case['agent_evaluation']['input']['user_prompt'])} å­—ç¬¦")
        print(f"é¢„æµ‹: Task1={case['agent_evaluation']['output']['task1_selfpredict']}, Task4={case['agent_evaluation']['output']['task4_answer_choice']}")
        print(f"çœŸå®: æˆç»©={case['agent_evaluation']['ground_truth']['true_score']}")
    
    print("\n" + "="*80)
    return case_json_path


def evaluate_results(df):
    """
    è®¡ç®—å¹¶å±•ç¤ºå„é¡¹è¯„ä¼°æŒ‡æ ‡, åŒ…æ‹¬ ACC, F1-score, ROUGE-3ã€‚
    """
    print("\n" + "="*80)
    print("ğŸ“Š é˜¶æ®µ 3/3: ç»“æœè¯„ä¼°ä¸åˆ†æ".center(80))
    print("="*80)
    if df.empty:
        print("ç»“æœDataFrameä¸ºç©ºï¼Œæ— æ³•è¯„ä¼°ã€‚")
        return

    eval_df = df.copy()

    # --- æ•°æ®æ¸…æ´—å’Œè§„èŒƒåŒ– ---
    def normalize_yes_no(val):
        if isinstance(val, str):
            val_lower = val.strip().lower().replace('.', '')
            if val_lower == 'yes': return 1
            if val_lower == 'no': return 0
        return np.nan
    
    def normalize_to_prob(val, high_confidence=0.95, low_confidence=0.05):
        """å°† Yes/No è½¬æ¢ä¸ºæ¦‚ç‡å€¼"""
        if isinstance(val, str):
            val_lower = val.strip().lower().replace('.', '')
            if val_lower == 'yes': return high_confidence
            if val_lower == 'no': return low_confidence
        return 0.5 # å¯¹äºæ— æ³•è§£æçš„å€¼ï¼Œè¿”å›ä¸­æ€§æ¦‚ç‡

    # æ–°çš„è¯„ä¼°é€»è¾‘ï¼šåŸºäºç­”æ¡ˆé€‰é¡¹è¿›è¡Œæ¯”å¯¹
    def parse_answer_choice(val):
        """è§£æTask4çš„ç­”æ¡ˆé€‰é¡¹ (A/B/C/D) è½¬æ¢ä¸ºchoice_id"""
        if not isinstance(val, str):
            return None
        val = val.strip().upper()
        # æå–ç¬¬ä¸€ä¸ªå­—æ¯ä½œä¸ºé€‰é¡¹
        if val and val[0] in 'ABCDEFGH':
            return val[0]
        return None
    
    eval_df['predicted_answer_choice'] = eval_df['predicted_task4_answer_choice'].apply(parse_answer_choice)
    
    # Task1çš„è‡ªæˆ‘é¢„æµ‹ (Yes/No)
    eval_df['pred_t1_selfpredict'] = eval_df['predicted_task1_selfpredict'].apply(normalize_yes_no)
    
    # --- æ ¸å¿ƒè¯„ä¼°é€»è¾‘ ---
    # åŸºäºç­”æ¡ˆé€‰é¡¹è®¡ç®—å‡†ç¡®ç‡
    def check_answer_correctness(row):
        """æ£€æŸ¥ç­”æ¡ˆæ˜¯å¦æ­£ç¡®ï¼šå°†é¢„æµ‹çš„é€‰é¡¹å­—æ¯è½¬æ¢ä¸ºchoice_idå¹¶æ¯”å¯¹"""
        predicted_choice = row.get('predicted_answer_choice')
        if predicted_choice is None:
            return None
        
        # ä»question_choiceså­—ç¬¦ä¸²ä¸­è§£æé€‰é¡¹
        question_choices_str = row.get('question_choices')
        if not question_choices_str or question_choices_str == 'None':
            # å¦‚æœæ²¡æœ‰é€‰é¡¹ï¼Œå›é€€åˆ°ä½¿ç”¨true_score
            return row.get('true_score')
        
        try:
            import ast
            choices = ast.literal_eval(question_choices_str)
            if not isinstance(choices, list) or len(choices) == 0:
                return row.get('true_score')
            
            # æ‰¾åˆ°é¢„æµ‹é€‰é¡¹å¯¹åº”çš„choice_id
            choice_index = ord(predicted_choice) - ord('A')
            if 0 <= choice_index < len(choices):
                predicted_choice_id = choices[choice_index].get('choice_id')
                correct_choice_id = row.get('true_answer_choice_id')
                return 1 if predicted_choice_id == correct_choice_id else 0
        except:
            pass
        
        return None
    
    eval_df['effective_prediction'] = eval_df.apply(check_answer_correctness, axis=1)
    
    # å¯¹äºæ²¡æœ‰æœ‰æ•ˆé¢„æµ‹çš„è¡Œï¼Œä½¿ç”¨true_scoreä½œä¸ºåå¤‡
    eval_df['effective_prediction'] = eval_df['effective_prediction'].fillna(eval_df['true_score'])
    
    # è‡ªæˆ‘é¢„æµ‹å‡†ç¡®ç‡ (Task1)
    meta_df = eval_df[['true_score', 'pred_t1_selfpredict', 'experiment_type']].dropna()
    meta_results = {}
    if not meta_df.empty:
        meta_results['overall'] = (meta_df['pred_t1_selfpredict'] == meta_df['true_score']).mean()
        for exp_type in meta_df['experiment_type'].unique():
            subset = meta_df[meta_df['experiment_type'] == exp_type]
            if subset.empty:
                continue
            meta_results[exp_type] = (subset['pred_t1_selfpredict'] == subset['true_score']).mean()

    # 3. ä»»åŠ¡2 (çŸ¥è¯†ç‚¹è¯†åˆ«) å‡†ç¡®ç‡
    acc_t2 = (eval_df['predicted_task2_know_name'] == eval_df['true_know_name']).mean()

    # 4. è‡ªæˆ‘é¢„æµ‹ä¸€è‡´æ€§ (Task1è‡ªæˆ‘é¢„æµ‹ vs å®é™…ç»“æœ)
    # ä¸€è‡´æ€§å®šä¹‰ï¼šé¢„æµ‹èƒ½åšå¯¹ä¸”å®é™…åšå¯¹ï¼Œæˆ–é¢„æµ‹ä¸èƒ½åšå¯¹ä¸”å®é™…æ²¡åšå¯¹
    consistency_df = eval_df[['true_score', 'pred_t1_selfpredict', 'experiment_type']].dropna()

    # --- åˆ†åˆ«ä¸ºä¸åŒæŒ‡æ ‡å‡†å¤‡æ•°æ® ---
    # ç”¨äº ACC, F1, åˆ†ç±»æŠ¥å‘Š
    report_df = eval_df[['true_score', 'effective_prediction', 'experiment_type']].dropna()
    
    # ç®€åŒ–ï¼šä¸å†è®¡ç®—äº¤å‰ç†µï¼ˆå› ä¸ºç°åœ¨æ˜¯é€‰æ‹©é¢˜ï¼Œä¸å†æœ‰æ¦‚ç‡é¢„æµ‹ï¼‰
    
    # åœ¨è°ƒç”¨è¯„ä¼°å‡½æ•°å‰æ£€æŸ¥ report_df æ˜¯å¦ä¸ºç©º
    if report_df.empty:
        print("\nè­¦å‘Š: æ²¡æœ‰æœ‰æ•ˆçš„é¢„æµ‹ç»“æœå¯ä¾›è¯„ä¼°ã€‚")
        print("è¿™å¯èƒ½æ˜¯ç”±äºæ‰€æœ‰é¢„æµ‹éƒ½è¢«è¿‡æ»¤æ‰ï¼ˆä¾‹å¦‚ï¼Œå‡ä¸º 'No' æˆ–æ— æ³•è§£æï¼‰ã€‚")
        # åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå¯ä»¥åˆ›å»ºä¸€ä¸ªç©ºçš„è¯„ä¼°æŠ¥å‘Šæˆ–ç›´æ¥è¿”å›
        report_lines = [
            "--- æ™ºèƒ½ä½“è¡¨ç°è¯„ä¼°æŠ¥å‘Š ---",
            f"æ€»æµ‹è¯•æ ·æœ¬æ•°: {len(eval_df)}",
            "æ²¡æœ‰æœ‰æ•ˆçš„é¢„æµ‹ç»“æœå¯ä¾›ç”Ÿæˆè¯¦ç»†æŠ¥å‘Šã€‚"
        ]
        # ä¿å­˜ä¸€ä¸ªç®€åŒ–çš„æŠ¥å‘Š
        output_dir = os.path.join(os.path.dirname(__file__), '../results')
        os.makedirs(output_dir, exist_ok=True)
        report_path = os.path.join(output_dir, 'assessment_report.txt')
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write('\n'.join(report_lines))
        print(f"\nç®€åŒ–çš„è¯„ä¼°æŠ¥å‘Šå·²ä¿å­˜è‡³: {report_path}")
        return # æå‰é€€å‡ºå‡½æ•°


    acc_results = {}
    f1_results = {}
    cross_entropy_results = {}
    kc_recognition_results = {}
    consistency_acc_results = {}
    consistency_f1_results = {}
    consistency_ce_results = {}

    # å¦‚æœæœ‰ experiment_mode åˆ—ï¼Œåˆ™æŒ‰ experiment_mode åˆ†ç»„
    if 'experiment_mode' in report_df.columns:
        for mode in report_df['experiment_mode'].unique():
            mode_subset = report_df[report_df['experiment_mode'] == mode]
            if mode_subset.empty:
                continue
            for exp_type in mode_subset['experiment_type'].unique():
                subset = mode_subset[mode_subset['experiment_type'] == exp_type]
                if subset.empty:
                    continue
                key = f"{mode}_{exp_type}" if mode != 'baseline' else mode
                acc_results[key] = (subset['effective_prediction'] == subset['true_score']).mean()
                f1_results[key] = f1_score(subset['true_score'], subset['effective_prediction'], average='weighted')
        
        # é€‰æ‹©é¢˜è¯„ä¼°ç®€åŒ–ï¼šä¸è®¡ç®—äº¤å‰ç†µï¼ˆæ— æ¦‚ç‡è¾“å…¥ï¼‰ï¼Œç§»é™¤ log_loss_df ç›¸å…³é€»è¾‘
        
        # è®¡ç®—çŸ¥è¯†ç‚¹è¯†åˆ«å‡†ç¡®ç‡
        kc_df = eval_df[['predicted_task2_know_name', 'true_know_name', 'experiment_mode', 'experiment_type']].dropna()
        if not kc_df.empty:
            for mode in kc_df['experiment_mode'].unique():
                mode_subset = kc_df[kc_df['experiment_mode'] == mode]
                if mode_subset.empty:
                    continue
                for exp_type in mode_subset['experiment_type'].unique():
                    subset = mode_subset[mode_subset['experiment_type'] == exp_type]
                    if subset.empty:
                        continue
                    key = f"{mode}_{exp_type}" if mode != 'baseline' else mode
                    kc_recognition_results[key] = (subset['predicted_task2_know_name'] == subset['true_know_name']).mean()
        
        # è®¡ç®—è‡ªæˆ‘é¢„æµ‹ä¸€è‡´æ€§æŒ‡æ ‡
        if not consistency_df.empty and 'experiment_mode' in consistency_df.columns:
            for mode in consistency_df['experiment_mode'].unique():
                mode_subset = consistency_df[consistency_df['experiment_mode'] == mode]
                if mode_subset.empty:
                    continue
                for exp_type in mode_subset['experiment_type'].unique():
                    subset = mode_subset[mode_subset['experiment_type'] == exp_type]
                    if subset.empty:
                        continue
                    key = f"{mode}_{exp_type}" if mode != 'baseline' else mode
                    # ACC: é¢„æµ‹ä¸å®é™…çš„ä¸€è‡´æ€§ï¼ˆä½¿ç”¨Task1çš„è‡ªæˆ‘é¢„æµ‹ï¼‰
                    consistency_acc_results[key] = (subset['pred_t1_selfpredict'] == subset['true_score']).mean()
                    # F1-Score
                    consistency_f1_results[key] = f1_score(subset['true_score'], subset['pred_t1_selfpredict'], average='weighted')
                    # Cross Entropy
                    subset_with_prob = subset.copy()
                    subset_with_prob['consistency_prob'] = subset_with_prob['pred_t1_selfpredict'].apply(normalize_to_prob)
                    consistency_ce_results[key] = log_loss(subset_with_prob['true_score'], subset_with_prob['consistency_prob'])
    else:
        # åŸæœ‰é€»è¾‘ï¼ˆå‘åå…¼å®¹ï¼‰
        for exp_type in report_df['experiment_type'].unique():
            subset = report_df[report_df['experiment_type'] == exp_type]
            if subset.empty:
                continue
            acc_results[exp_type] = (subset['effective_prediction'] == subset['true_score']).mean()
            f1_results[exp_type] = f1_score(subset['true_score'], subset['effective_prediction'], average='weighted')

        # é€‰æ‹©é¢˜è¯„ä¼°ç®€åŒ–ï¼šä¸è®¡ç®—äº¤å‰ç†µï¼ˆæ— æ¦‚ç‡è¾“å…¥ï¼‰ï¼Œç§»é™¤ log_loss_df ç›¸å…³é€»è¾‘
        
        # çŸ¥è¯†ç‚¹è¯†åˆ«å‡†ç¡®ç‡
        kc_df = eval_df[['predicted_task2_know_name', 'true_know_name', 'experiment_type']].dropna()
        if not kc_df.empty:
            for exp_type in kc_df['experiment_type'].unique():
                subset = kc_df[kc_df['experiment_type'] == exp_type]
                if subset.empty:
                    continue
                kc_recognition_results[exp_type] = (subset['predicted_task2_know_name'] == subset['true_know_name']).mean()
        
        # è‡ªæˆ‘é¢„æµ‹ä¸€è‡´æ€§ï¼ˆä½¿ç”¨Task1çš„è‡ªæˆ‘é¢„æµ‹ï¼‰
        if not consistency_df.empty:
            for exp_type in consistency_df['experiment_type'].unique():
                subset = consistency_df[consistency_df['experiment_type'] == exp_type]
                if subset.empty:
                    continue
                consistency_acc_results[exp_type] = (subset['pred_t1_selfpredict'] == subset['true_score']).mean()
                consistency_f1_results[exp_type] = f1_score(subset['true_score'], subset['pred_t1_selfpredict'], average='weighted')
                subset_with_prob = subset.copy()
                subset_with_prob['consistency_prob'] = subset_with_prob['pred_t1_selfpredict'].apply(normalize_to_prob)
                consistency_ce_results[exp_type] = log_loss(subset_with_prob['true_score'], subset_with_prob['consistency_prob'])
    
    # 4. ROUGE-3 åˆ†æ•° (ä»»åŠ¡3 - ç­”æ¡ˆæ–‡æœ¬ç›¸ä¼¼åº¦)
    # æ³¨æ„: Transaction.csv ä¸­ 'answer_text' ä¸ºç©º, æ— æ³•ç›´æ¥å¯¹æ¯”ã€‚
    # è¿™é‡Œæˆ‘ä»¬åšä¸€ä¸ªç®€åŒ–ç¤ºèŒƒï¼šå°†æ¨¡å‹è¾“å‡ºçš„Task3ä¸ä¸€ä¸ªå‡è®¾çš„"æ ‡å‡†ç­”æ¡ˆ"å¯¹æ¯”ã€‚
    # åœ¨çœŸå®åœºæ™¯ä¸­ï¼Œæ‚¨éœ€è¦æœ‰å¯å¯¹æ¯”çš„å‚è€ƒç­”æ¡ˆæ–‡æœ¬ã€‚
    scorer = rouge_scorer.RougeScorer(['rouge3'], use_stemmer=True)
    rouge_scores = []
    for index, row in eval_df.iterrows():
        # ç®€åŒ–ï¼šæ­¤å¤„æˆ‘ä»¬æ²¡æœ‰çœŸå®çš„å­¦ç”Ÿç­”æ¡ˆæ–‡æœ¬ï¼Œæ‰€ä»¥æ— æ³•è®¡ç®—ROUGEã€‚
        # ä»…ä¸ºæ¼”ç¤ºé€»è¾‘ï¼Œæˆ‘ä»¬å°†æ¨¡å‹è¾“å‡ºä¸è‡ªèº«å¯¹æ¯”ï¼ŒçœŸå®åœºæ™¯éœ€è¦æ›¿æ¢ä¸ºå‚è€ƒç­”æ¡ˆã€‚
        reference_answer = str(row['true_answer_text']) # åº”è¯¥æ˜¯çœŸå®çš„å­¦ç”Ÿç­”æ¡ˆ
        model_answer = str(row['predicted_task3_reasoning'])
        if reference_answer and model_answer:
            scores = scorer.score(reference_answer, model_answer)
            rouge_scores.append(scores['rouge3'].fmeasure)
    
    avg_rouge3 = np.mean(rouge_scores) if rouge_scores else 0

    # --- æ‰“å°æŠ¥å‘Š ---
    report_lines = []
    report_lines.append("\n" + "="*80)
    report_lines.append("ğŸ“ˆ æ™ºèƒ½ä½“è¡¨ç°è¯„ä¼°æŠ¥å‘Š".center(80))
    report_lines.append("="*80)
    report_lines.append(f"\nğŸ“‹ åŸºæœ¬ä¿¡æ¯")
    report_lines.append(f"   â€¢ ä½¿ç”¨æ¨¡å‹: {MODEL_NAME}")
    report_lines.append(f"   â€¢ æ€»æµ‹è¯•æ ·æœ¬æ•°: {len(eval_df)}")
    report_lines.append(f"\nğŸ¯ ä»»åŠ¡å‡†ç¡®ç‡")
    report_lines.append(f"   â€¢ ä»»åŠ¡2 (çŸ¥è¯†ç‚¹è¯†åˆ«): {acc_t2:.2%}")
    if meta_results:
        overall_val = meta_results.get('overall')
        if overall_val is not None:
            report_lines.append(f"   â€¢ è‡ªæˆ‘é¢„æµ‹å‡†ç¡®ç‡ (æ•´ä½“): {overall_val:.2%}")
        if len(meta_results) > 1:
            report_lines.append(f"\nğŸ”¬ å®éªŒå¯¹æ¯” - è‡ªæˆ‘é¢„æµ‹å‡†ç¡®ç‡ (Task4)")
            for exp_label, val in meta_results.items():
                if exp_label == 'overall':
                    continue
                icon = "ğŸ”µ" if exp_label == "baseline" else "ğŸŸ¢"
                report_lines.append(f"   {icon} {exp_label:20s}: {val:.2%}")
    
    report_lines.append(f"\nğŸ“ æœ€ç»ˆåšé¢˜ç»“æœè¯„ä¼° (ç»“åˆ Task1 + Task4)")
    report_lines.append("-"*80)
    for exp_label, acc_val in acc_results.items():
        icon = "ğŸ”µ" if exp_label == "baseline" else "ğŸŸ¢"
        report_lines.append(f"\n{icon} {exp_label.upper()}")
        report_lines.append(f"   â€¢ å‡†ç¡®ç‡ (ACC):        {acc_val:.2%}")
        f1_val = f1_results.get(exp_label)
        if f1_val is not None:
            report_lines.append(f"   â€¢ F1-Score (åŠ æƒ):     {f1_val:.4f}")
        cross_val = cross_entropy_results.get(exp_label)
        if cross_val is not None:
            report_lines.append(f"   â€¢ äº¤å‰ç†µ (Cross Entropy): {cross_val:.4f}")
    
    # çŸ¥è¯†ç‚¹è¯†åˆ«å‡†ç¡®ç‡ï¼ˆæŒ‰å®éªŒç±»å‹ï¼‰
    if kc_recognition_results:
        report_lines.append(f"\nğŸ¯ çŸ¥è¯†ç‚¹è¯†åˆ«å‡†ç¡®ç‡ (Task2)")
        report_lines.append("-"*80)
        for exp_label, kc_acc_val in kc_recognition_results.items():
            icon = "ğŸ”µ" if exp_label == "baseline" else "ğŸŸ¢"
            report_lines.append(f"   {icon} {exp_label:20s}: {kc_acc_val:.2%}")
    
    # è‡ªæˆ‘é¢„æµ‹ä¸€è‡´æ€§ï¼ˆæŒ‰å®éªŒç±»å‹ï¼‰
    if consistency_acc_results:
        report_lines.append(f"\nğŸ”® è‡ªæˆ‘é¢„æµ‹ä¸€è‡´æ€§ (Task4 vs å®é™…ç»“æœ)")
        report_lines.append("-"*80)
        for exp_label in consistency_acc_results.keys():
            icon = "ğŸ”µ" if exp_label == "baseline" else "ğŸŸ¢"
            report_lines.append(f"\n{icon} {exp_label.upper()}")
            cons_acc = consistency_acc_results.get(exp_label)
            if cons_acc is not None:
                report_lines.append(f"   â€¢ å‡†ç¡®ç‡ (ACC):        {cons_acc:.2%}")
            cons_f1 = consistency_f1_results.get(exp_label)
            if cons_f1 is not None:
                report_lines.append(f"   â€¢ F1-Score (åŠ æƒ):     {cons_f1:.4f}")
            cons_ce = consistency_ce_results.get(exp_label)
            if cons_ce is not None:
                report_lines.append(f"   â€¢ äº¤å‰ç†µ (Cross Entropy): {cons_ce:.4f}")
    
    if avg_rouge3 > 0:
        report_lines.append(f"\nğŸ“ ç­”æ¡ˆæ–‡æœ¬ç›¸ä¼¼åº¦")
        report_lines.append(f"   â€¢ ROUGE-3 F-Score: {avg_rouge3:.4f}")
    
    # --- Task1 è¯¦ç»†åˆ†ç±»æŠ¥å‘Š (è‡ªæˆ‘é¢„æµ‹) ---
    report_lines.append("\n" + "="*80)
    report_lines.append("ğŸ“Š Task1 è¯¦ç»†åˆ†ç±»æŠ¥å‘Š (è‡ªæˆ‘é¢„æµ‹)".center(80))
    report_lines.append("="*80)
    if not consistency_df.empty:
        report_lines.append(classification_report(
            consistency_df['true_score'], 
            consistency_df['pred_t1_selfpredict'], 
            target_names=['é¢„æµ‹é”™è¯¯', 'é¢„æµ‹æ­£ç¡®']
        ))
    else:
        report_lines.append("   âš ï¸  æ— å¯ç”¨çš„ Task1 æ•°æ®")
    
    # --- Task4 è¯¦ç»†åˆ†ç±»æŠ¥å‘Š (ç­”æ¡ˆé€‰æ‹©) ---
    report_lines.append("\n" + "="*80)
    report_lines.append("ğŸ“Š Task4 è¯¦ç»†åˆ†ç±»æŠ¥å‘Š (ç­”æ¡ˆé€‰æ‹©)".center(80))
    report_lines.append("="*80)
    report_lines.append(classification_report(report_df['true_score'], report_df['effective_prediction'], target_names=['é¢„æµ‹é”™è¯¯', 'é¢„æµ‹æ­£ç¡®']))
    
    # å°†æŠ¥å‘Šæ‰“å°åˆ°ç»ˆç«¯
    for line in report_lines:
        print(line)

    output_dir = os.path.join(os.path.dirname(__file__), '../results')
    os.makedirs(output_dir, exist_ok=True)

    # ä¿å­˜è¯„ä¼°æŠ¥å‘Šåˆ°æ–‡ä»¶
    report_path = os.path.join(output_dir, 'assessment_report.txt')
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write('\n'.join(report_lines))

    # --- Task1 æ··æ·†çŸ©é˜µ (è‡ªæˆ‘é¢„æµ‹) ---
    if not consistency_df.empty:
        fig, axes = plt.subplots(1, 2, figsize=(14, 5))
        
        # Task1 æ··æ·†çŸ©é˜µ
        cm_t1 = confusion_matrix(consistency_df['true_score'], consistency_df['pred_t1_selfpredict'])
        disp_t1 = ConfusionMatrixDisplay(confusion_matrix=cm_t1, display_labels=['é”™è¯¯', 'æ­£ç¡®'])
        disp_t1.plot(cmap='Blues', ax=axes[0])
        axes[0].set_title('Task1: è‡ªæˆ‘é¢„æµ‹æ··æ·†çŸ©é˜µ')
        
        # Task4 æ··æ·†çŸ©é˜µ
        cm_t4 = confusion_matrix(report_df['true_score'], report_df['effective_prediction'])
        disp_t4 = ConfusionMatrixDisplay(confusion_matrix=cm_t4, display_labels=['é”™è¯¯', 'æ­£ç¡®'])
        disp_t4.plot(cmap='Greens', ax=axes[1])
        axes[1].set_title('Task4: ç­”æ¡ˆé€‰æ‹©æ··æ·†çŸ©é˜µ')
        
        plt.tight_layout()
        fig_path = os.path.join(output_dir, 'confusion_matrix.png')
        plt.savefig(fig_path, dpi=150)
        plt.close()
    else:
        # å¦‚æœæ²¡æœ‰ Task1 æ•°æ®ï¼Œåªä¿å­˜ Task4 æ··æ·†çŸ©é˜µ
        cm = confusion_matrix(report_df['true_score'], report_df['effective_prediction'])
        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['é”™è¯¯', 'æ­£ç¡®'])
        disp.plot(cmap='Blues')
        plt.title('Task4: ç­”æ¡ˆé€‰æ‹©æ··æ·†çŸ©é˜µ')
        fig_path = os.path.join(output_dir, 'confusion_matrix.png')
        plt.savefig(fig_path)
        plt.close()
    
    print("\n" + "="*80)
    print("ğŸ’¾ è¾“å‡ºæ–‡ä»¶".center(80))
    print("="*80)
    print(f"   ğŸ“„ è¯„ä¼°æŠ¥å‘Š: {report_path}")
    print(f"   ğŸ“Š æ··æ·†çŸ©é˜µ: {fig_path}")


# --- ä¸»å‡½æ•° ---
async def main():
    parser = argparse.ArgumentParser(description="è¿è¡Œ Agent4Edu å­¦ç”Ÿè¡¨ç°è¯„ä¼°å®éªŒã€‚")
    parser.add_argument("--students", type=int, default=10, help="è¦è¿è¡Œæ¨¡æ‹Ÿçš„å­¦ç”Ÿæ•°é‡ã€‚è®¾ç½®ä¸º-1åˆ™è¿è¡Œæ‰€æœ‰å­¦ç”Ÿã€‚é»˜è®¤10ä¸ªå­¦ç”Ÿã€‚")
    parser.add_argument("--concurrency", type=int, default=30, help="LLM è¯·æ±‚å¹¶å‘æ•°é‡ï¼ˆæ¨¡å‹è¯·æ±‚ç»´åº¦ï¼‰ã€‚é»˜è®¤30ã€‚")
    parser.add_argument("--rerun", action="store_true", help="å¼ºåˆ¶é‡æ–°ç”ŸæˆæŒæ¡åº¦è¯„ä¼°æ•°æ®ã€‚")
    parser.add_argument("--spread-duration", type=int, default=120, help="å°†æ‰€æœ‰è¯·æ±‚å‡åŒ€åˆ†æ•£åˆ°æŒ‡å®šç§’æ•°å†…ï¼Œå®ç°å‰Šå³°å¡«è°·ã€‚é»˜è®¤120ç§’ã€‚è®¾ç½®ä¸º0åˆ™ç¦ç”¨ã€‚")
    parser.add_argument("--experiment-mode", type=str, default="all", 
                       choices=["baseline", "mastery_only", "tutoring_only", "both", "all"],
                       help="å®éªŒæ¨¡å¼: baseline(æ— å¢å¼º), mastery_only(ä»…æŒæ¡åº¦), tutoring_only(ä»…è¾…å¯¼), both(æŒæ¡åº¦+è¾…å¯¼), all(å…¨éƒ¨è¿è¡Œ). é»˜è®¤all.")
    parser.add_argument("--model-name", type=str, default=None, 
                       help="æŒ‡å®šä½¿ç”¨çš„LLMæ¨¡å‹åç§°ï¼ˆå¦‚ gpt-3.5-turbo, qwen-plus, doubao-pro-32kç­‰ï¼‰ã€‚è‹¥ä¸æŒ‡å®šåˆ™ä½¿ç”¨ä»£ç ä¸­çš„é»˜è®¤å€¼ã€‚")
    parser.add_argument("--save-interval", type=int, default=20,
                       help="æ¯å®Œæˆå¤šå°‘ä¸ªå­¦ç”Ÿä¿å­˜ä¸€æ¬¡ç»“æœã€‚é»˜è®¤20ã€‚")
    parser.add_argument("--no-resume", action="store_true",
                       help="ç¦ç”¨æ–­ç‚¹ç»­è·‘ï¼šä¸åŠ è½½å·²æœ‰ç»“æœï¼Œä»å¤´å¼€å§‹è¿è¡Œæ‰€æœ‰å­¦ç”Ÿã€‚")
    args = parser.parse_args()
    
    # å¦‚æœç”¨æˆ·æŒ‡å®šäº†æ¨¡å‹åç§°ï¼Œè¦†ç›–é»˜è®¤å€¼
    global MODEL_NAME
    if args.model_name:
        MODEL_NAME = args.model_name
        print(f"\nğŸ¤– ä½¿ç”¨æŒ‡å®šæ¨¡å‹: {MODEL_NAME}")
    else:
        print(f"\nğŸ¤– ä½¿ç”¨é»˜è®¤æ¨¡å‹: {MODEL_NAME}")

    if not user_sys_call_with_model:
        print("LLMå·¥å…·æ¨¡å—æœªèƒ½åŠ è½½ï¼Œè¯·æ£€æŸ¥é¡¹ç›®è·¯å¾„ã€‚è„šæœ¬é€€å‡ºã€‚")
        sys.exit(1)

    # 1. æ•°æ®åŠ è½½ä¸é¢„å¤„ç†
    (
        all_student_records,
        kcs_df,
        kc_relationships_df,
        question_to_kc_map,
        questions_df,
        kc_to_questions_map,
        question_text_map,
        kc_descriptions,
        question_choices_df
    ) = load_and_preprocess_data(PROJECT_ROOT)

    # å‡†å¤‡KCGå’Œæ‰€æœ‰çŸ¥è¯†ç‚¹åˆ—è¡¨
    know_name_map = kcs_df.set_index('id')['name'].to_dict()
    all_kc_names = list(kcs_df['name'].unique())
    kcg_df = kc_relationships_df.copy()
    kcg_df['from_kc_name'] = kcg_df['from_knowledgecomponent_id'].map(know_name_map)
    kcg_df['to_kc_name'] = kcg_df['to_knowledgecomponent_id'].map(know_name_map)
    KCG = set(zip(kcg_df['from_kc_name'], kcg_df['to_kc_name']))

    # é€‰å–å­¦ç”Ÿï¼ˆä½¿ç”¨éšæœºé‡‡æ ·ä»¥è·å¾—æ›´å¥½çš„ä»£è¡¨æ€§ï¼‰
    student_ids = sorted(all_student_records.keys())
    if args.students != -1:
        # ä½¿ç”¨éšæœºç§å­ç¡®ä¿å¯é‡ç°æ€§ï¼ŒåŒæ—¶æä¾›å¤šæ ·æ€§
        random.seed(2024)  # ä½¿ç”¨2024ä½œä¸ºéšæœºç§å­
        random.shuffle(student_ids)  # æ‰“ä¹±é¡ºåº
        student_ids = student_ids[:min(args.students, len(student_ids))]
        student_ids = sorted(student_ids)  # é‡æ–°æ’åºä»¥ä¾¿äºæ—¥å¿—æŸ¥çœ‹

    # å‡†å¤‡æ—¥å¿—æ–‡ä»¶
    output_dir = os.path.join(os.path.dirname(__file__), '../results')
    os.makedirs(output_dir, exist_ok=True)
    prompt_log_path = os.path.join(output_dir, 'prompt_logs.txt')
    if os.path.exists(prompt_log_path):
        os.remove(prompt_log_path) # æ¯æ¬¡è¿è¡Œæ—¶æ¸…ç©ºæ—§æ—¥å¿—
    print(f"\nğŸ“ æ—¥å¿—æ–‡ä»¶: {prompt_log_path}")

    related_kc_map = build_related_kc_map(all_kc_names, KCG)
    
    # 2. è¿è¡Œä¸‰ç»„å¯¹ç…§å®éªŒ
    all_experiments_results = []
    
    # ç¡®å®šè¦è¿è¡Œçš„å®éªŒæ¨¡å¼
    experiment_modes = []
    if args.experiment_mode == "all":
        experiment_modes = ["baseline", "mastery_only", "tutoring_only", "both"]
    else:
        experiment_modes = [args.experiment_mode]
    
    print("\n" + "="*80)
    print(f"ğŸ§ª å°†è¿è¡Œ {len(experiment_modes)} ç»„å®éªŒï¼ˆåŒä¸€æ‰¹å­¦ç”Ÿï¼‰".center(80))
    print("="*80)
    print(f"   å­¦ç”ŸIDåˆ—è¡¨: {student_ids[:10]}{'...' if len(student_ids) > 10 else ''}")
    print(f"   å®éªŒæ¨¡å¼: {', '.join(experiment_modes)}")
    print("="*80)
    
    # å¢é‡ä¿å­˜ç›¸å…³å˜é‡
    completed_students_count = {}  # {exp_mode: count}
    accumulated_results = {}  # {exp_mode: DataFrame}
    
    # ğŸ”¹ åªæœ‰åœ¨éœ€è¦ mastery_only æˆ– both æ¨¡å¼æ—¶æ‰åŠ è½½/ç”ŸæˆæŒæ¡åº¦è¯„ä¼°æ•°æ®
    # ğŸ”¥ ä¼˜åŒ–ï¼štutoring_only æ¨¡å¼ä¹Ÿå°è¯•åŠ è½½æŒæ¡åº¦æ•°æ®ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    mastery_lookup = None
    if "mastery_only" in experiment_modes or "tutoring_only" in experiment_modes or "both" in experiment_modes:
        # æŒæ¡åº¦è¯„ä¼°æ¨¡å¼
        mastery_mode = "minimal"  # ä¿æŒæ–‡ä»¶åå…¼å®¹æ€§
        needs_rerun = args.rerun
        target_count = len(student_ids) if args.students != -1 else -1
        
        # ç”Ÿæˆå¸¦æ¨¡å‹åç§°çš„æ–‡ä»¶åç¼€ï¼ˆä¸ assess_mastery.py ä¿æŒä¸€è‡´ï¼‰
        safe_model_name = MODEL_NAME.replace('/', '_').replace('.', '_')
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡æ–°è¿è¡ŒæŒæ¡åº¦è¯„ä¼°
        mode_path = os.path.join(
            PROJECT_ROOT,
            f'results/mastery_assessment_results_{mastery_mode}_{safe_model_name}.csv'
        )
        
        # æ£€æŸ¥æŒæ¡åº¦æ•°æ®æ˜¯å¦åŒ…å«å½“å‰é‡‡æ ·çš„å­¦ç”Ÿ
        mastery_students_mismatch = False
        if os.path.exists(mode_path) and not needs_rerun:
            try:
                mastery_df = pd.read_csv(mode_path)
                mastery_student_ids = set(mastery_df['student_id'].unique())
                current_student_ids = set(student_ids)
                if not current_student_ids.issubset(mastery_student_ids):
                    missing_students = current_student_ids - mastery_student_ids
                    print(f"\nâš ï¸  æŒæ¡åº¦æ•°æ®ä¸åŒ…å«å½“å‰é‡‡æ ·çš„æ‰€æœ‰å­¦ç”Ÿ")
                    print(f"   å½“å‰å®éªŒå­¦ç”Ÿ: {len(current_student_ids)} ä¸ª")
                    print(f"   æŒæ¡åº¦æ•°æ®å­¦ç”Ÿ: {len(mastery_student_ids)} ä¸ª")
                    print(f"   ç¼ºå¤±å­¦ç”Ÿæ•°: {len(missing_students)} ä¸ª")
                    mastery_students_mismatch = True
            except Exception as e:
                print(f"âš ï¸  æ£€æŸ¥æŒæ¡åº¦æ•°æ®æ—¶å‡ºé”™: {e}")
                mastery_students_mismatch = True
        
        if needs_rerun or not os.path.exists(mode_path) or mastery_students_mismatch:
            # ğŸ”¥ åªæœ‰åœ¨ mastery_only æˆ– both æ¨¡å¼ä¸‹æ‰ç”ŸæˆæŒæ¡åº¦æ•°æ®
            if "mastery_only" in experiment_modes or "both" in experiment_modes:
                print("\n" + "="*80)
                print("ğŸ” æŒæ¡åº¦è¯„ä¼°æ•°æ®æ£€æŸ¥".center(80))
                print("="*80)
                
                if needs_rerun:
                    print(f"   åŸå› : æ”¶åˆ° --rerun å‚æ•°ï¼Œå¼ºåˆ¶é‡æ–°ç”Ÿæˆ")
                elif mastery_students_mismatch:
                    print(f"   åŸå› : å­¦ç”Ÿé‡‡æ ·ä¸åŒ¹é…ï¼Œéœ€è¦è¡¥å……ç”Ÿæˆ")
                else:
                    print(f"   åŸå› : æœªæ‰¾åˆ°æŒæ¡åº¦è¯„ä¼°æ•°æ®")
                print(f"   å­¦ç”Ÿæ•°: {len(student_ids)} ä¸ª")
                print(f"\nğŸ”„ æ­£åœ¨è‡ªåŠ¨è¿è¡ŒæŒæ¡åº¦è¯„ä¼°è„šæœ¬...")
                print("-"*80)
                
                success = await run_mastery_assessment_pipeline(
                    args.concurrency, 
                    student_ids=student_ids, 
                    student_count=target_count,
                    mode=mastery_mode,
                    model_name=MODEL_NAME
                )
                if not success:
                    print("\nâš ï¸  æŒæ¡åº¦è¯„ä¼°è„šæœ¬æœªæˆåŠŸï¼Œmastery_only å’Œ both å®éªŒå°†è¢«è·³è¿‡ã€‚")
                    experiment_modes = [m for m in experiment_modes if m not in ["mastery_only", "both"]]
                else:
                    print("\nâœ… æŒæ¡åº¦è¯„ä¼°æ•°æ®å·²ç”Ÿæˆå®Œæˆï¼")
                    print("="*80)
            else:
                # tutoring_only æ¨¡å¼ï¼šå¦‚æœæ²¡æœ‰æŒæ¡åº¦æ•°æ®ï¼Œåªæ˜¯æç¤ºï¼Œä¸å¼ºåˆ¶ç”Ÿæˆ
                print("\nğŸ’¡ æç¤º: æœªæ‰¾åˆ°æŒæ¡åº¦è¯„ä¼°æ•°æ®")
                print(f"   ğŸ“ æœŸæœ›è·¯å¾„: {mode_path}")
                print(f"   â„¹ï¸  è¾…å¯¼å†…å®¹ç”Ÿæˆå°†ä½¿ç”¨é”™é¢˜ç»Ÿè®¡è¯†åˆ«è–„å¼±çŸ¥è¯†ç‚¹")
        else:
            print("\nâœ… æ£€æµ‹åˆ°å·²æœ‰æŒæ¡åº¦è¯„ä¼°æ•°æ®ï¼Œç›´æ¥ä½¿ç”¨")
            print(f"   ğŸ“ æ–‡ä»¶è·¯å¾„: {mode_path}")
        
        # åŠ è½½æŒæ¡åº¦è¯„ä¼°æ•°æ®ï¼ˆmastery_onlyã€tutoring_onlyã€both éƒ½å°è¯•åŠ è½½ï¼‰
        if os.path.exists(mode_path):
            mastery_lookup = load_mastery_assessment_results(mode_path, set(student_ids))
            if not mastery_lookup:
                if "mastery_only" in experiment_modes:
                    print("âš ï¸  æ— æ³•åŠ è½½æŒæ¡åº¦æ•°æ®ï¼Œmastery_only å®éªŒå°†è¢«è·³è¿‡ã€‚")
                    experiment_modes = [m for m in experiment_modes if m != "mastery_only"]
                if "both" in experiment_modes:
                    print("âš ï¸  Both æ¨¡å¼æ— æ³•åŠ è½½æŒæ¡åº¦æ•°æ®ï¼Œå°†é™çº§ä¸º Baseline æ¨¡å¼")
                    # Both æ¨¡å¼é™çº§ä¸º baselineï¼ˆä¸ç§»é™¤ bothï¼Œç¨ååœ¨é…ç½®ä¸­å¤„ç†ï¼‰
                if "tutoring_only" in experiment_modes:
                    print("âš ï¸  æ— æ³•åŠ è½½æŒæ¡åº¦æ•°æ®ï¼Œè¾…å¯¼å†…å®¹å°†ä½¿ç”¨é”™é¢˜ç»Ÿè®¡ã€‚")
            else:
                if ("tutoring_only" in experiment_modes or "both" in experiment_modes) and "mastery_only" not in experiment_modes:
                    print(f"   âœ… å·²åŠ è½½æŒæ¡åº¦æ•°æ®ç”¨äºè¾…å¯¼å†…å®¹ç”Ÿæˆ: {len(mastery_lookup)} ä¸ªå­¦ç”Ÿ")
    
    # ğŸ”¹ åªæœ‰åœ¨éœ€è¦ tutoring_only æˆ– both æ¨¡å¼æ—¶æ‰åŠ è½½/ç”Ÿæˆè¾…å¯¼å†…å®¹æ•°æ®
    tutoring_lookup = None
    if "tutoring_only" in experiment_modes or "both" in experiment_modes:
        # ğŸ”¥ æ£€æŸ¥æŒæ¡åº¦æ•°æ®å¯ç”¨æ€§
        if not mastery_lookup:
            if "tutoring_only" in experiment_modes:
                print("\n" + "="*80)
                print("âš ï¸  Tutoring Only æ¨¡å¼è·³è¿‡".center(80))
                print("="*80)
                print(f"   åŸå› : ç¼ºå°‘æŒæ¡åº¦è¯„ä¼°æ•°æ®")
                print(f"   è¯´æ˜: è¾…å¯¼å†…å®¹ç”Ÿæˆéœ€è¦åŸºäºæŒæ¡åº¦æ•°æ®è¯†åˆ«è–„å¼±çŸ¥è¯†ç‚¹")
                print(f"   å»ºè®®: å…ˆè¿è¡Œ mastery_only æ¨¡å¼ç”ŸæˆæŒæ¡åº¦æ•°æ®ï¼Œæˆ–ä½¿ç”¨ --rerun å‚æ•°")
                print("="*80)
                experiment_modes = [m for m in experiment_modes if m != "tutoring_only"]
            # Both æ¨¡å¼å·²åœ¨ä¸Šé¢å¤„ç†é™çº§é€»è¾‘ï¼ˆé™çº§ä¸º baselineï¼‰
        else:
            needs_rerun = args.rerun
            target_count = len(student_ids) if args.students != -1 else -1
            
            # ç”Ÿæˆå¸¦æ¨¡å‹åç§°çš„æ–‡ä»¶åç¼€
            safe_model_name = MODEL_NAME.replace('/', '_').replace('.', '_')
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡æ–°è¿è¡Œè¾…å¯¼å†…å®¹ç”Ÿæˆ
            tutoring_path = os.path.join(
                PROJECT_ROOT,
                f'results/tutoring_content_results_{safe_model_name}.csv'
            )
            
            # ğŸ”¥ ä¼˜åŒ–ï¼šæŒ‰"å­¦ç”Ÿ + çŸ¥è¯†ç‚¹"ç»´åº¦æ£€æŸ¥è¾…å¯¼å†…å®¹æ•°æ®çš„å®Œæ•´æ€§
            tutoring_students_mismatch = False
            missing_pairs = set()  # ç¼ºå¤±çš„ (student_id, kc_name) å¯¹
            
            if os.path.exists(tutoring_path) and not needs_rerun:
                try:
                    tutoring_df = pd.read_csv(tutoring_path)
                    
                    # ğŸ”¥ æ–°é€»è¾‘ï¼šè®¡ç®—æœŸæœ›çš„è¾…å¯¼å¯¹
                    print(f"\nğŸ” æ­£åœ¨è®¡ç®—æœŸæœ›çš„è¾…å¯¼å†…å®¹...")
                    expected_result = calculate_expected_tutoring_pairs(
                        student_ids, 
                        all_student_records, 
                        mastery_lookup
                    )
                    expected_pairs = expected_result['expected_pairs']
                    student_weak_kcs_map = expected_result['student_weak_kcs']
                    
                    # æ„å»ºç°æœ‰çš„è¾…å¯¼å¯¹
                    existing_pairs = set()
                    for _, row in tutoring_df.iterrows():
                        sid = row['student_id']
                        kc = row['kc_name']
                        # åªç»Ÿè®¡å½“å‰å®éªŒæ¶‰åŠçš„å­¦ç”Ÿ
                        if sid in student_ids:
                            existing_pairs.add((sid, kc))
                    
                    # è®¡ç®—ç¼ºå¤±çš„è¾…å¯¼å¯¹
                    missing_pairs = expected_pairs - existing_pairs
                    
                    # è¯¦ç»†æŠ¥å‘Š
                    print(f"\nğŸ“Š è¾…å¯¼å†…å®¹å®Œæ•´æ€§æ£€æŸ¥æŠ¥å‘Š")
                    print(f"{'='*80}")
                    print(f"   â€¢ æœŸæœ›è¾…å¯¼å¯¹æ•°é‡: {len(expected_pairs)}")
                    print(f"   â€¢ å·²æœ‰è¾…å¯¼å¯¹æ•°é‡: {len(existing_pairs)}")
                    print(f"   â€¢ ç¼ºå¤±è¾…å¯¼å¯¹æ•°é‡: {len(missing_pairs)}")
                    
                    if missing_pairs:
                        tutoring_students_mismatch = True
                        
                        # æŒ‰å­¦ç”Ÿç»Ÿè®¡ç¼ºå¤±æƒ…å†µ
                        missing_by_student = defaultdict(list)
                        for sid, kc in missing_pairs:
                            missing_by_student[sid].append(kc)
                        
                        print(f"\nâš ï¸  æ£€æµ‹åˆ° {len(missing_by_student)} ä¸ªå­¦ç”Ÿçš„è¾…å¯¼å†…å®¹ä¸å®Œæ•´")
                        print(f"   ç¼ºå¤±è¯¦æƒ…ï¼ˆå‰10ä¸ªå­¦ç”Ÿï¼‰:")
                        for i, (sid, kcs) in enumerate(sorted(missing_by_student.items())[:10]):
                            expected_kcs = student_weak_kcs_map.get(sid, [])
                            print(f"   â€¢ å­¦ç”Ÿ {sid}: ç¼ºå¤± {len(kcs)}/{len(expected_kcs)} ä¸ªçŸ¥è¯†ç‚¹")
                            if len(kcs) <= 5:
                                print(f"     ç¼ºå¤±çŸ¥è¯†ç‚¹: {', '.join(kcs)}")
                            else:
                                print(f"     ç¼ºå¤±çŸ¥è¯†ç‚¹: {', '.join(kcs[:5])} ... (å…±{len(kcs)}ä¸ª)")
                        
                        if len(missing_by_student) > 10:
                            print(f"   ... è¿˜æœ‰ {len(missing_by_student) - 10} ä¸ªå­¦ç”Ÿæœªæ˜¾ç¤º")
                    else:
                        print(f"\nâœ… è¾…å¯¼å†…å®¹æ•°æ®å®Œæ•´æ€§æ£€æŸ¥é€šè¿‡ï¼")
                        print(f"   æ‰€æœ‰å­¦ç”Ÿçš„æ‰€æœ‰è–„å¼±çŸ¥è¯†ç‚¹éƒ½å·²ç”Ÿæˆè¾…å¯¼å†…å®¹")
                            
                except Exception as e:
                    print(f"âš ï¸  æ£€æŸ¥è¾…å¯¼å†…å®¹æ•°æ®æ—¶å‡ºé”™: {e}")
                    import traceback
                    traceback.print_exc()
                    tutoring_students_mismatch = True
            
            if needs_rerun or not os.path.exists(tutoring_path) or tutoring_students_mismatch:
                print("\n" + "="*80)
                print("ğŸ” è¾…å¯¼å†…å®¹æ•°æ®æ£€æŸ¥".center(80))
                print("="*80)
                
                if needs_rerun:
                    print(f"   åŸå› : æ”¶åˆ° --rerun å‚æ•°ï¼Œå¼ºåˆ¶é‡æ–°ç”Ÿæˆ")
                elif tutoring_students_mismatch:
                    print(f"   åŸå› : å­¦ç”Ÿé‡‡æ ·ä¸åŒ¹é…ï¼Œéœ€è¦è¡¥å……ç”Ÿæˆ")
                else:
                    print(f"   åŸå› : æœªæ‰¾åˆ°è¾…å¯¼å†…å®¹æ•°æ®")
                
                print(f"   å­¦ç”Ÿæ•°: {len(student_ids)} ä¸ª")
                print(f"\nğŸ”„ æ­£åœ¨è‡ªåŠ¨è¿è¡Œè¾…å¯¼å†…å®¹ç”Ÿæˆè„šæœ¬...")
                print("-"*80)
                
                # è¿è¡Œè¾…å¯¼å†…å®¹ç”Ÿæˆè„šæœ¬ï¼ˆä½¿ç”¨å¼‚æ­¥æ–¹å¼ï¼Œå®æ—¶è¾“å‡ºæ—¥å¿—ï¼‰
                script_path = os.path.join(os.path.dirname(__file__), 'generate_tutoring_content.py')
                student_ids_str = ','.join(map(str, student_ids))
                
                cmd = [
                    sys.executable,
                    script_path,
                    '--student-ids', student_ids_str,
                    '--concurrency', str(args.concurrency),
                    '--model', MODEL_NAME,
                    '--spread-duration', '60'
                ]
                
                # å¦‚æœæœ‰æŒæ¡åº¦æ•°æ®ï¼Œä½¿ç”¨å®ƒæ¥è¯†åˆ«è–„å¼±çŸ¥è¯†ç‚¹
                if mastery_lookup:
                    cmd.append('--use-mastery')
                
                try:
                    # ğŸ”¥ ä½¿ç”¨å¼‚æ­¥å­è¿›ç¨‹ï¼Œå®æ—¶è¾“å‡ºæ—¥å¿—ï¼ˆä¸ mastery_only ä¿æŒä¸€è‡´ï¼‰
                    proc = await asyncio.create_subprocess_exec(*cmd)
                    await proc.wait()
                    if proc.returncode != 0:
                        print(f"\nâš ï¸  è¾…å¯¼å†…å®¹ç”Ÿæˆè„šæœ¬æ‰§è¡Œå¤±è´¥ï¼Œè¿”å›ç  {proc.returncode}")
                        print("   tutoring_only å’Œ both å®éªŒå°†è¢«è·³è¿‡ã€‚")
                        experiment_modes = [m for m in experiment_modes if m not in ["tutoring_only", "both"]]
                    else:
                        print("\nâœ… è¾…å¯¼å†…å®¹æ•°æ®å·²ç”Ÿæˆå®Œæˆï¼")
                        print("="*80)
                except FileNotFoundError:
                    print(f"\nâš ï¸  æœªæ‰¾åˆ°è¾…å¯¼å†…å®¹ç”Ÿæˆè„šæœ¬: {script_path}")
                    print("   tutoring_only å’Œ both å®éªŒå°†è¢«è·³è¿‡ã€‚")
                    experiment_modes = [m for m in experiment_modes if m not in ["tutoring_only", "both"]]
                except Exception as e:
                    print(f"\nâš ï¸  è¾…å¯¼å†…å®¹ç”Ÿæˆè„šæœ¬æ‰§è¡Œå¤±è´¥: {e}")
                    print("   tutoring_only å’Œ both å®éªŒå°†è¢«è·³è¿‡ã€‚")
                    experiment_modes = [m for m in experiment_modes if m not in ["tutoring_only", "both"]]
            else:
                print("\nâœ… æ£€æµ‹åˆ°å·²æœ‰è¾…å¯¼å†…å®¹æ•°æ®ï¼Œç›´æ¥ä½¿ç”¨")
                print(f"   ğŸ“ æ–‡ä»¶è·¯å¾„: {tutoring_path}")
            
            # åŠ è½½è¾…å¯¼å†…å®¹æ•°æ®
            if "tutoring_only" in experiment_modes or "both" in experiment_modes:  # å¦‚æœè¿˜åœ¨å®éªŒåˆ—è¡¨ä¸­ï¼ˆæ²¡æœ‰å› ç”Ÿæˆå¤±è´¥è¢«ç§»é™¤ï¼‰
                tutoring_lookup = load_tutoring_content_results(tutoring_path, set(student_ids))
                if not tutoring_lookup:
                    if "tutoring_only" in experiment_modes:
                        print("âš ï¸  æ— æ³•åŠ è½½è¾…å¯¼å†…å®¹æ•°æ®ï¼Œtutoring_only å®éªŒå°†è¢«è·³è¿‡ã€‚")
                        experiment_modes = [m for m in experiment_modes if m != "tutoring_only"]
                    if "both" in experiment_modes:
                        print("âš ï¸  Both æ¨¡å¼æ— æ³•åŠ è½½è¾…å¯¼å†…å®¹ï¼Œå°†é™çº§ä¸º Mastery Only æ¨¡å¼")
                        # Both æ¨¡å¼é™çº§ä¸º mastery_onlyï¼ˆä¸ç§»é™¤ bothï¼Œç¨ååœ¨é…ç½®ä¸­å¤„ç†ï¼‰
    
    # è¿è¡Œå„ç»„å®éªŒ
    for exp_mode in experiment_modes:
        # æ–­ç‚¹ç»­è·‘ï¼ˆé»˜è®¤å¼€å¯ï¼‰ï¼šåŠ è½½å·²æœ‰ç»“æœå¹¶è¿‡æ»¤å­¦ç”Ÿ
        # æ–‡ä»¶ååŒ…å«æ¨¡å‹åç§°ï¼Œé¿å…ä¸åŒæ¨¡å‹ç»“æœäº’ç›¸è¦†ç›–
        # æ³¨æ„ï¼šsafe_model_name å·²åœ¨ä¸Šé¢å®šä¹‰ï¼Œè¿™é‡Œå¤ç”¨å³å¯
        if 'safe_model_name' not in locals():
            safe_model_name = MODEL_NAME.replace('/', '_').replace('.', '_')  # å¤„ç†ç‰¹æ®Šå­—ç¬¦
        results_pkl_path = os.path.join(output_dir, f'experiment_results_{exp_mode}_{safe_model_name}.pkl')
        completed_student_ids = set()
        existing_results_df = None
        
        # é»˜è®¤å¯ç”¨æ–­ç‚¹ç»­è·‘ï¼Œé™¤éç”¨æˆ·æŒ‡å®š --no-resume
        if not args.no_resume and os.path.exists(results_pkl_path):
            try:
                existing_results_df = pd.read_pickle(results_pkl_path)
                completed_student_ids = set(existing_results_df['student_id'].unique())
                print(f"\nğŸ“‚ æ£€æµ‹åˆ°å·²æœ‰ç»“æœï¼Œå¯ç”¨æ–­ç‚¹ç»­è·‘")
                print(f"   ğŸ“ æ–‡ä»¶è·¯å¾„: {results_pkl_path}")
                print(f"   âœ… å·²å®Œæˆå­¦ç”Ÿæ•°: {len(completed_student_ids)}")
                print(f"   ğŸ“Š å·²æœ‰æ•°æ®è¡Œæ•°: {len(existing_results_df)}")
            except Exception as e:
                print(f"\nâš ï¸  åŠ è½½å·²æœ‰ç»“æœå¤±è´¥ï¼Œå°†ä»å¤´å¼€å§‹: {e}")
                existing_results_df = None
        
        # è¿‡æ»¤å‡ºå¾…è¿è¡Œçš„å­¦ç”Ÿ
        remaining_student_ids = [sid for sid in student_ids if sid not in completed_student_ids]
        
        if not remaining_student_ids:
            print(f"\nâœ… {exp_mode.upper()} å®éªŒå·²å…¨éƒ¨å®Œæˆï¼Œè·³è¿‡")
            if existing_results_df is not None:
                existing_results_df['experiment_mode'] = actual_mode  # ğŸ”¥ ä½¿ç”¨å®é™…æ¨¡å¼æ ‡ç­¾
                all_experiments_results.append(existing_results_df)
            continue
        
        # é…ç½®å®éªŒå‚æ•°ï¼ˆåŒ…å« Both æ¨¡å¼é™çº§é€»è¾‘ï¼‰- å¿…é¡»å…ˆæ‰§è¡Œä»¥ç¡®å®š actual_mode
        if exp_mode == "baseline":
            use_mastery = False
            use_tutoring = False
            label = "BASELINE (æ— æŒæ¡åº¦ + æ— è¾…å¯¼)"
            icon = "ğŸ”µ"
            actual_mode = "baseline"
        elif exp_mode == "mastery_only":
            use_mastery = True
            use_tutoring = False
            label = "MASTERY ONLY (æœ‰æŒæ¡åº¦ + æ— è¾…å¯¼)"
            icon = "ğŸŸ¢"
            actual_mode = "mastery_only"
        elif exp_mode == "tutoring_only":
            use_mastery = False
            use_tutoring = True
            label = "TUTORING ONLY (æ— æŒæ¡åº¦ + æœ‰è¾…å¯¼)"
            icon = "ğŸŸ¡"
            actual_mode = "tutoring_only"
        elif exp_mode == "both":
            # Both æ¨¡å¼é™çº§é€»è¾‘
            has_mastery = mastery_lookup is not None and len(mastery_lookup) > 0
            has_tutoring = tutoring_lookup is not None and len(tutoring_lookup) > 0
            
            if not has_mastery and not has_tutoring:
                # é™çº§ä¸º Baseline
                use_mastery = False
                use_tutoring = False
                label = "BOTH (é™çº§ä¸º Baseline - æ— æŒæ¡åº¦ + æ— è¾…å¯¼)"
                icon = "ğŸ”µâ¬…ï¸ğŸŸ£"
                actual_mode = "baseline"  # å®é™…ç­‰åŒäº baseline
            elif has_mastery and not has_tutoring:
                # é™çº§ä¸º Mastery Only
                use_mastery = True
                use_tutoring = False
                label = "BOTH (é™çº§ä¸º Mastery Only - æœ‰æŒæ¡åº¦ + æ— è¾…å¯¼)"
                icon = "ğŸŸ¢â¬…ï¸ğŸŸ£"
                actual_mode = "mastery_only"  # å®é™…ç­‰åŒäº mastery_only
            elif not has_mastery and has_tutoring:
                # ç†è®ºä¸Šä¸åº”è¯¥å‡ºç°ï¼ˆè¾…å¯¼ä¾èµ–æŒæ¡åº¦ï¼‰ï¼Œä½†ä»å¤„ç†
                use_mastery = False
                use_tutoring = True
                label = "BOTH (é™çº§ä¸º Tutoring Only - æ— æŒæ¡åº¦ + æœ‰è¾…å¯¼)"
                icon = "ğŸŸ¡â¬…ï¸ğŸŸ£"
                actual_mode = "tutoring_only"  # å®é™…ç­‰åŒäº tutoring_only
            else:
                # æ­£å¸¸ Both æ¨¡å¼
                use_mastery = True
                use_tutoring = True
                label = "BOTH (æœ‰æŒæ¡åº¦ + æœ‰è¾…å¯¼)"
                icon = "ğŸŸ£"
                actual_mode = "both"  # å®Œæ•´çš„ both æ¨¡å¼
        else:
            actual_mode = exp_mode  # å…¶ä»–æ¨¡å¼ä¿æŒä¸å˜
        
        print(f"\nğŸ“‹ {exp_mode.upper()} å®éªŒè¿›åº¦:")
        print(f"   â€¢ å·²å®Œæˆ: {len(completed_student_ids)} ä¸ªå­¦ç”Ÿ")
        print(f"   â€¢ å¾…è¿è¡Œ: {len(remaining_student_ids)} ä¸ªå­¦ç”Ÿ")
        print(f"   â€¢ æ€»è®¡: {len(student_ids)} ä¸ªå­¦ç”Ÿ")
        
        print("\n" + "="*80)
        print(f"{icon} è¿è¡Œ {exp_mode.upper()} å®éªŒ".center(80))
        print("="*80)
        print(f"   ğŸ“‹ é…ç½®: {label}")
        print(f"   ğŸ¤– ä½¿ç”¨æ¨¡å‹: {MODEL_NAME}")
        print(f"   ğŸ¯ æŒæ¡åº¦å¢å¼º: {'âœ… å¼€å¯' if use_mastery else 'âŒ å…³é—­'}")
        print(f"   ğŸ“š è¾…å¯¼è¾“å‡º: {'âœ… å¼€å¯' if use_tutoring else 'âŒ å…³é—­'}")
        if exp_mode == "both" and actual_mode != "both":
            print(f"   âš ï¸  å®é™…è¿è¡Œæ¨¡å¼: {actual_mode.upper()} (é™çº§)")
        print("-"*80)
        
        # å‡†å¤‡æ—¥å¿—æ–‡ä»¶ï¼ˆæ–­ç‚¹ç»­è·‘æ¨¡å¼ä¸‹è¿½åŠ ï¼Œå¦åˆ™æ¸…ç©ºï¼‰
        prompt_log_path = os.path.join(output_dir, f'prompt_logs_{exp_mode}.txt')
        if args.no_resume and os.path.exists(prompt_log_path):
            os.remove(prompt_log_path)
            print(f"   ğŸ—‘ï¸  å·²æ¸…ç©ºæ—¥å¿—æ–‡ä»¶ï¼ˆ--no-resumeæ¨¡å¼ï¼‰")
        
        recommendation_log_path = os.path.join(output_dir, f'recommendation_logs_{exp_mode}.txt') if use_tutoring else None
        if recommendation_log_path and args.no_resume and os.path.exists(recommendation_log_path):
            os.remove(recommendation_log_path)
        
        # åˆå§‹åŒ–å¢é‡ä¿å­˜
        completed_students_count[exp_mode] = len(completed_student_ids)
        if existing_results_df is not None:
            accumulated_results[exp_mode] = existing_results_df
        else:
            accumulated_results[exp_mode] = pd.DataFrame()
        
        # å®šä¹‰å¢é‡ä¿å­˜å›è°ƒ
        def save_incremental_results(student_id, student_results):
            nonlocal accumulated_results, completed_students_count
            
            # æ·»åŠ æ–°ç»“æœ
            new_df = pd.DataFrame(student_results)
            new_df['experiment_mode'] = actual_mode  # ğŸ”¥ ä½¿ç”¨å®é™…æ¨¡å¼æ ‡ç­¾
            
            if accumulated_results[exp_mode].empty:
                accumulated_results[exp_mode] = new_df
            else:
                accumulated_results[exp_mode] = pd.concat([accumulated_results[exp_mode], new_df], ignore_index=True)
            
            completed_students_count[exp_mode] += 1
            
            # æ¯ save_interval ä¸ªå­¦ç”Ÿä¿å­˜ä¸€æ¬¡
            if completed_students_count[exp_mode] % args.save_interval == 0:
                try:
                    accumulated_results[exp_mode].to_pickle(results_pkl_path)
                    non_empty_rows = len(accumulated_results[exp_mode])
                    print(f"\nğŸ’¾ å¢é‡ä¿å­˜ ({exp_mode}): å·²å®Œæˆ {completed_students_count[exp_mode]} ä¸ªå­¦ç”Ÿ")
                    print(f"   ğŸ“Š å½“å‰æ•°æ®è¡Œæ•°: {non_empty_rows}")
                    print(f"   ğŸ“ ä¿å­˜è·¯å¾„: {results_pkl_path}")
                except Exception as e:
                    print(f"\nâš ï¸  å¢é‡ä¿å­˜å¤±è´¥: {e}")
        
        # è¿è¡Œå®éªŒ
        results = await run_experiment(
            remaining_student_ids,  # ä½¿ç”¨è¿‡æ»¤åçš„å­¦ç”Ÿåˆ—è¡¨
            all_student_records,
            args.concurrency,
            prompt_log_path,
            all_kc_names,
            mastery_lookup if use_mastery else None,
            related_kc_map if use_mastery else None,
            kc_to_questions_map,
            question_text_map,
            recommendation_log_path,
            kc_descriptions,
            question_choices_df,
            use_mastery,
            use_tutoring,
            tutoring_lookup if use_tutoring else None,  # ğŸ”¥ ä¼ é€’é¢„åŠ è½½çš„è¾…å¯¼å†…å®¹
            args.spread_duration,
            on_student_complete=save_incremental_results
        )
        
        # åˆå¹¶æ–°æ—§ç»“æœ
        results['experiment_mode'] = actual_mode  # ğŸ”¥ ä½¿ç”¨å®é™…æ¨¡å¼æ ‡ç­¾
        if existing_results_df is not None:
            final_results = pd.concat([existing_results_df, results], ignore_index=True)
        else:
            final_results = results
        
        # æœ€ç»ˆä¿å­˜
        try:
            final_results.to_pickle(results_pkl_path)
            non_empty_rows = len(final_results)
            print(f"\nğŸ’¾ æœ€ç»ˆä¿å­˜ ({exp_mode}):")
            print(f"   âœ… æ€»å®Œæˆå­¦ç”Ÿæ•°: {len(final_results['student_id'].unique())}")
            print(f"   ğŸ“Š æ€»æ•°æ®è¡Œæ•°: {non_empty_rows}")
            print(f"   ğŸ“ ä¿å­˜è·¯å¾„: {results_pkl_path}")
        except Exception as e:
            print(f"\nâš ï¸  æœ€ç»ˆä¿å­˜å¤±è´¥: {e}")
        
        all_experiments_results.append(final_results)
    
    # åˆå¹¶æ‰€æœ‰å®éªŒç»“æœ
    combined_results_df = pd.concat(all_experiments_results, ignore_index=True)
    
    # ä»…ä¿å­˜ CSV æ ¼å¼çš„ç»¼åˆå¯¹æ¯”ç»“æœï¼ˆæ–¹ä¾¿æŸ¥çœ‹ï¼‰
    # æ³¨æ„ï¼šä¸ä¿å­˜ pickle æ ¼å¼ï¼Œå› ä¸ºå„æ¨¡å¼çš„ pickle å·²å•ç‹¬ä¿å­˜
    results_csv_path = os.path.join(output_dir, 'experiment_results_comparison.csv')
    combined_results_df.to_csv(results_csv_path, index=False)
    print(f"\nğŸ’¾ ç»¼åˆå®éªŒç»“æœå·²ä¿å­˜è‡³:")
    print(f"   ğŸ“„ CSVæ ¼å¼: {results_csv_path}")
    print(f"   ğŸ“ˆ æ€»æ•°æ®è¡Œæ•°: {len(combined_results_df)}")
    print(f"   ğŸ’¡ æç¤º: å„æ¨¡å¼çš„è¯¦ç»†ç»“æœå·²ä¿å­˜ä¸ºç‹¬ç«‹çš„ .pkl æ–‡ä»¶ï¼ˆå¸¦æ¨¡å‹åç§°ï¼‰")

    # 3. è¯„ä¼°ç»“æœï¼ˆå¯¹æ¯”æ‰€æœ‰æ¨¡å¼ï¼‰
    print("\n" + "="*80)
    print("ğŸ“Š ç»¼åˆè¯„ä¼°æŠ¥å‘Šï¼ˆå¯¹æ¯”æ‰€æœ‰æ¨¡å¼ï¼‰".center(80))
    print("="*80)
    evaluate_results(combined_results_df)

    # 4. ç”Ÿæˆä¸‰æ¨¡å¼ç»¼åˆå¯¹æ¯”æŠ¥å‘Š
    print("\n" + "="*80)
    print("ğŸ“Š ç”Ÿæˆä¸‰æ¨¡å¼ç»¼åˆå¯¹æ¯”æŠ¥å‘Š".center(80))
    print("="*80)
    generate_three_mode_comparison_report(combined_results_df, output_dir)

    # 5. ä¿å­˜è¾“å…¥è¾“å‡ºæ¡ˆä¾‹ï¼ˆæ‰¾3ä¸ªåŒæ—¶æœ‰æŒæ¡åº¦å’Œè¾…å¯¼å†…å®¹çš„å­¦ç”Ÿï¼‰
    if mastery_lookup and tutoring_lookup:
        try:
            save_in_out_cases(
                combined_results_df,
                output_dir,
                all_student_records,
                kcs_df,
                kc_relationships_df,
                kc_to_questions_map,
                question_text_map,
                kc_descriptions,
                question_choices_df,
                mastery_lookup,
                tutoring_lookup,
                related_kc_map,
                all_kc_names
            )
        except Exception as e:
            print(f"\nâš ï¸  ä¿å­˜è¾“å…¥è¾“å‡ºæ¡ˆä¾‹å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
    else:
        print("\nâš ï¸  è·³è¿‡æ¡ˆä¾‹ä¿å­˜ï¼šç¼ºå°‘æŒæ¡åº¦æˆ–è¾…å¯¼å†…å®¹æ•°æ®")

    # è¾“å‡ºä¸Šä¸‹æ–‡å¿«ç…§ï¼Œä¾¿äºå¯¹æ¯”æŒæ¡åº¦ä¸è¾…å¯¼æ‘˜è¦ï¼ˆä»…ä¿å­˜ CSVï¼‰
    summary_columns = ['student_id', 'question_id', 'true_know_name', 'experiment_type', 'experiment_mode', 'mastery_summary', 'tutoring_summary']
    available_columns = [col for col in summary_columns if col in combined_results_df.columns]
    context_snapshot_df = combined_results_df[available_columns].drop_duplicates()
    
    # ä»…ä¿å­˜ CSV æ ¼å¼ï¼ˆæ–¹ä¾¿æŸ¥çœ‹ï¼‰
    context_snapshot_csv_path = os.path.join(output_dir, 'context_snapshot_comparison.csv')
    context_snapshot_df.to_csv(context_snapshot_csv_path, index=False)
    print(f"   ğŸ“¸ ä¸Šä¸‹æ–‡å¿«ç…§: {context_snapshot_csv_path}")
    print("="*80)
    print("\nğŸ‰ å®éªŒå®Œæˆï¼".center(80))
    print("="*80 + "\n")

if __name__ == "__main__":
    asyncio.run(main())
