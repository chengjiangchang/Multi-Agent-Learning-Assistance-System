import pandas as pd
import numpy as np
import json
import random
import os
import sys
import asyncio
import argparse
from tqdm import tqdm
from sklearn.model_selection import train_test_split

# --- 1. è®¾ç½®é¡¹ç›®è·¯å¾„ ---
def setup_project_path():
    """
    è·å–é¡¹ç›®æ ¹ç›®å½•ï¼ˆå»æ‰äº†æ—§çš„ yl_data_process æŸ¥æ‰¾é€»è¾‘ï¼‰
    """
    try:
        current_path = os.path.dirname(os.path.abspath(__file__))
    except NameError:
        current_path = os.getcwd()
    
    # é¡¹ç›®æ ¹ç›®å½•æ˜¯ Code æ–‡ä»¶å¤¹çš„çˆ¶ç›®å½•
    project_root = os.path.dirname(current_path)
    print(f"é¡¹ç›®æ ¹ç›®å½•: {project_root}")
    return project_root

# è®¾ç½®è·¯å¾„
PROJECT_ROOT = setup_project_path()

# å¯¼å…¥ LLM å·¥å…·å‡½æ•°
from llms.qwen import user_sys_call as user_sys_call_with_model

# --- Agent Model Config ---
MODEL_NAME = "qwen-plus"  # é»˜è®¤ä½¿ç”¨ Qwen-Plus æ¨¡å‹


# --- 2. æ•°æ®åŠ è½½ä¸é¢„å¤„ç† ---
def load_and_preprocess_data(project_root):
    """
    åŠ è½½æ‰€æœ‰CSVæ•°æ®å¹¶å°†å…¶é¢„å¤„ç†ä¸ºæŒ‰å­¦ç”ŸIDåˆ†ç»„çš„æ—¥å¿—ã€‚
    """
    print("\n" + "="*80)
    print("ğŸ”„ é˜¶æ®µ 1/3: æ•°æ®åŠ è½½ä¸é¢„å¤„ç†".center(80))
    print("="*80)
    data_path = os.path.join(project_root, 'data/')
    
    try:
        questions_df = pd.read_csv(os.path.join(data_path, "Questions.csv"))
        question_choices_df = pd.read_csv(os.path.join(data_path, "Question_Choices.csv"))
        question_kc_relationships_df = pd.read_csv(os.path.join(data_path, "Question_KC_Relationships.csv"))
        transactions_df = pd.read_csv(os.path.join(data_path, "Transaction.csv"))
        kcs_df = pd.read_csv(os.path.join(data_path, "KCs.csv"))
        kc_relationships_df = pd.read_csv(os.path.join(data_path, "KC_Relationships.csv"))
        print("æ‰€æœ‰æ•°æ®æ–‡ä»¶åŠ è½½æˆåŠŸï¼")
    except FileNotFoundError as e:
        print(f"åŠ è½½æ–‡ä»¶æ—¶å‡ºé”™: {e}")
        sys.exit(1)

    # åˆ›å»ºæ˜ å°„
    kc_id_to_name = kcs_df.set_index('id')['name'].to_dict()
    question_kc_relationships_df['know_name'] = question_kc_relationships_df['knowledgecomponent_id'].map(kc_id_to_name)

    # åˆå¹¶æ•°æ®
    merged = pd.merge(transactions_df, questions_df[['id', 'question_text']], left_on='question_id', right_on='id', how='left')
    merged = merged.drop(columns=['id_y']).rename(columns={'id_x': 'id'})
    merged = pd.merge(merged, question_kc_relationships_df[['question_id', 'know_name']], on='question_id', how='left')
    merged['score'] = merged['answer_state'].astype(int)
    merged = merged.dropna(subset=['know_name'])
    merged = merged.rename(columns={'question_text': 'exer_content'})

    # æŒ‰å­¦ç”Ÿåˆ†ç»„
    all_student_records = {}
    for student_id in merged['student_id'].unique():
        student_df = merged[merged['student_id'] == student_id].sort_values('start_time').reset_index(drop=True)
        all_student_records[student_id] = student_df

    # æ„å»ºçŸ¥è¯†ç‚¹åˆ°é¢˜ç›®çš„æ˜ å°„
    kc_to_questions_map = {}
    for _, row in question_kc_relationships_df.iterrows():
        kc_name = row['know_name']
        question_id = row['question_id']
        if pd.notna(kc_name):
            if kc_name not in kc_to_questions_map:
                kc_to_questions_map[kc_name] = []
            if question_id not in kc_to_questions_map[kc_name]:
                kc_to_questions_map[kc_name].append(question_id)

    # é¢˜ç›®æ–‡æœ¬æ˜ å°„
    question_text_map = questions_df.set_index('id')['question_text'].to_dict()
    
    # çŸ¥è¯†ç‚¹æè¿°
    kc_descriptions = kcs_df.set_index('name')['description'].fillna('').to_dict()

    print(f"âœ… æ•°æ®åŠ è½½å®Œæˆ")
    print(f"   â€¢ å­¦ç”Ÿæ•°: {len(all_student_records)}")
    print(f"   â€¢ çŸ¥è¯†ç‚¹æ•°: {len(kc_to_questions_map)}")
    print(f"   â€¢ é¢˜ç›®æ•°: {len(question_text_map)}")

    return all_student_records, kcs_df, kc_to_questions_map, question_text_map, kc_descriptions, question_choices_df


# --- 3. è¾…å¯¼å†…å®¹ç”Ÿæˆæ ¸å¿ƒå‡½æ•° ---

def _truncate_text(text, max_length=400):
    """
    æ™ºèƒ½æˆªæ–­æ–‡æœ¬ï¼šè¶…è¿‡max_lengthæ—¶ä¿ç•™å‰åéƒ¨åˆ†
    
    Args:
        text: åŸå§‹æ–‡æœ¬
        max_length: æœ€å¤§é•¿åº¦é˜ˆå€¼ï¼Œé»˜è®¤400å­—ç¬¦
    
    Returns:
        æˆªæ–­åçš„æ–‡æœ¬ï¼Œæ ¼å¼ï¼š[å‰200å­—ç¬¦] ... [å200å­—ç¬¦]
    """
    if not isinstance(text, str):
        return ''
    
    text = text.strip()
    
    if len(text) <= max_length:
        return text
    
    # è¶…è¿‡é˜ˆå€¼ï¼Œæˆªå–å‰åå„ä¸€åŠ
    half_length = max_length // 2
    front_part = text[:half_length].strip()
    back_part = text[-half_length:].strip()
    
    return f"{front_part} ... {back_part}"


def _select_three_questions_for_kc(kc_name, kc_to_questions_map, test_question_ids, question_text_map, question_choices_df, max_num=2):
    """
    ä¸ºæŒ‡å®šçŸ¥è¯†ç‚¹æŒ‘é€‰æœ€å¤š2é“é¢˜ï¼Œå¹¶é™„å¸¦é€‰é¡¹ä¸æ­£ç¡®ç­”æ¡ˆæ–‡æœ¬ã€‚
    
    é¢˜åº“é€‰æ‹©é€»è¾‘ï¼š
    - ä»è¯¥çŸ¥è¯†ç‚¹çš„æ‰€æœ‰é¢˜ç›®ä¸­æ’é™¤æµ‹è¯•é›†çš„é¢˜ç›®
    - å¯ä»¥åŒ…å«è®­ç»ƒé›†åšè¿‡çš„é¢˜ï¼ˆç”¨äºå¤ä¹ è®²è§£ï¼‰
    - ç¡®ä¿ä¸æ³„éœ²æµ‹è¯•é›†ç­”æ¡ˆ
    
    Args:
        test_question_ids: æµ‹è¯•é›†é¢˜ç›®IDé›†åˆï¼ˆéœ€è¦æ’é™¤çš„ï¼‰
    """
    question_ids = kc_to_questions_map.get(kc_name, []) or []
    if not question_ids:
        return []

    # æ’é™¤æµ‹è¯•é›†é¢˜ç›®ï¼ˆå¯ä»¥åŒ…å«è®­ç»ƒé›†é¢˜ç›®ï¼‰
    available = [qid for qid in question_ids if qid not in test_question_ids]
    pool = available

    picked = []
    candidates = list(pool)
    random.shuffle(candidates)
    for qid in candidates:
        if len(picked) >= max_num:
            break
        # é¢˜å¹²
        q_text = _truncate_text(question_text_map.get(qid, '') or '')
        # é€‰é¡¹ä¸æ­£ç¡®ç­”æ¡ˆ
        choices = question_choices_df[question_choices_df['question_id'] == qid]
        if choices.empty:
            picked.append({
                'question_id': qid,
                'question_text': q_text,
                'choices': [],
                'correct_letter': None,
                'correct_text': None
            })
            continue
        
        letters = [chr(65 + i) for i in range(len(choices))]
        correct_letter = None
        correct_text = None
        rendered_choices = []
        for idx, (_, ch) in enumerate(choices.iterrows()):
            letter = letters[idx]
            rendered_choices.append({'letter': letter, 'text': ch.get('choice_text', '')})
            if ch.get('is_correct'):
                correct_letter = letter
                correct_text = ch.get('choice_text', '')

        picked.append({
            'question_id': qid,
            'question_text': q_text,
            'choices': rendered_choices,
            'correct_letter': correct_letter,
            'correct_text': correct_text
        })

    return picked


def build_tutoring_prompt_single_kc(student_id, kc_name, kc_description, example_questions):
    """
    æ„å»ºå•ä¸ªçŸ¥è¯†ç‚¹çš„è¾…å¯¼æç¤ºè¯ï¼ˆä¼˜åŒ–ç‰ˆï¼šæ¯æ¬¡åªå¤„ç†1ä¸ªçŸ¥è¯†ç‚¹ + 2é“ä¾‹é¢˜ï¼‰
    
    Args:
        student_id: å­¦ç”ŸID
        kc_name: çŸ¥è¯†ç‚¹åç§°
        kc_description: çŸ¥è¯†ç‚¹æè¿°
        example_questions: 2é“ä¾‹é¢˜åˆ—è¡¨ï¼ˆåŒ…å«é¢˜å¹²ã€é€‰é¡¹ã€ç­”æ¡ˆï¼‰
    
    Returns:
        tuple: (system_prompt, user_prompt)
    """
    system_prompt = (
        "You are an experienced tutoring teacher. Your task:\n"
        "1. Explain the concept clearly - what it is and why it matters\n"
        "2. For each example, show step-by-step solution AND explicitly connect it to the concept\n"
        "\n"
        "Output format:\n"
        "Concept Explanation:\n"
        "[Clear explanation of the key ideas and their importance]\n"
        "\n"
        "Example 1 (Question ID: X):\n"
        "Solution: [Step-by-step process]\n"
        "Connection: [How this example demonstrates the concept]\n"
        "\n"
        "Example 2 (Question ID: Y):\n"
        "Solution: [Step-by-step process]\n"
        "Connection: [How this example demonstrates the concept]\n"
        "\n"
        "Example 3 (Question ID: Z):\n"
        "Solution: [Step-by-step process]\n"
        "Connection: [How this example demonstrates the concept]"
    )

    lines = []
    lines.append(f"Student ID: {student_id}")
    lines.append(f"Concept: {kc_name}")
    
    if kc_description:
        lines.append(f"\nConcept Description: {kc_description}")
    
    lines.append(f"\nPractice Examples:")
    for idx, ex in enumerate(example_questions, start=1):
        lines.append(f"\nExample {idx} (Question ID: {ex['question_id']}):")
        lines.append(f"{ex['question_text']}")
        if ex['choices']:
            for ch in ex['choices']:
                lines.append(f"{ch['letter']}. {ch['text']}")
        if ex['correct_letter']:
            lines.append(f"Correct Answer: {ex['correct_letter']}")

    return system_prompt, "\n".join(lines)


def parse_tutoring_by_kc(llm_response, weak_kc_list):
    """
    è§£æLLMçš„è¾…å¯¼å“åº”ï¼ŒæŒ‰çŸ¥è¯†ç‚¹åˆ†æ®µå­˜å‚¨ã€‚
    
    æ”¹è¿›ç‰ˆï¼šå¢åŠ å®¹é”™æœºåˆ¶å’Œå¤‡ç”¨ç­–ç•¥ï¼Œè§£å†³çŸ¥è¯†ç‚¹åç§°ä¸åŒ¹é…é—®é¢˜
    
    Args:
        llm_response: LLMè¿”å›çš„å®Œæ•´è¾…å¯¼å†…å®¹
        weak_kc_list: è–„å¼±çŸ¥è¯†ç‚¹åˆ—è¡¨
    
    Returns:
        dict: {kc_name: tutoring_content_for_that_kc}
    """
    if not isinstance(llm_response, str) or not weak_kc_list:
        return {}
    
    parsed = {}
    import re
    
    # ç­–ç•¥1ï¼šç²¾ç¡®åŒ¹é…ï¼ˆæ”¯æŒ Markdown åŠ ç²—æ ‡è®° **ï¼‰
    for kc_name in weak_kc_list:
        # å…è®¸çŸ¥è¯†ç‚¹åç§°å‰åæœ‰ ** æ ‡è®°ï¼ˆMarkdownåŠ ç²—ï¼‰
        # åŒ¹é…æ¨¡å¼ï¼šConcept: **XXX** æˆ– Concept: XXX
        pattern = rf'Concept:\s*\*?\*?\s*{re.escape(kc_name)}\s*\*?\*?(.*?)(?=Concept:\s*\*?\*?|\Z)'
        match = re.search(pattern, llm_response, re.DOTALL | re.IGNORECASE)
        
        if match:
            content = match.group(1).strip()
            if content:
                parsed[kc_name] = f"Concept: {kc_name}\n{content}"
    
    # ç­–ç•¥2ï¼šå¦‚æœç²¾ç¡®åŒ¹é…å¤±è´¥ï¼Œå°è¯•æ¨¡ç³ŠåŒ¹é…
    if not parsed:
        for kc_name in weak_kc_list:
            # æ¨¡ç³ŠåŒ¹é…ï¼šæŸ¥æ‰¾åŒ…å«éƒ¨åˆ†çŸ¥è¯†ç‚¹åç§°çš„æ®µè½
            sections = llm_response.split('Concept:')
            for section in sections[1:]:  # è·³è¿‡ç¬¬ä¸€ä¸ªç©ºæ®µ
                section = section.strip()
                if not section:
                    continue
                # æå–ç¬¬ä¸€è¡Œä½œä¸ºLLMè¿”å›çš„çŸ¥è¯†ç‚¹åç§°
                first_line = section.split('\n')[0].strip().strip('*').strip()
                # æ£€æŸ¥æ˜¯å¦åŒ…å«æœŸæœ›çš„çŸ¥è¯†ç‚¹å…³é”®è¯
                if kc_name.lower() in first_line.lower() or first_line.lower() in kc_name.lower():
                    content_lines = section.split('\n', 1)
                    content = content_lines[1] if len(content_lines) > 1 else ''
                    parsed[kc_name] = f"Concept: {kc_name}\n{content}"
                    break
    
    # ç­–ç•¥3ï¼šå¦‚æœæ‰€æœ‰çŸ¥è¯†ç‚¹éƒ½è§£æå¤±è´¥ï¼Œé‡‡ç”¨é¡ºåºåˆ†é…ï¼ˆæœ€åå¤‡ç”¨æ–¹æ¡ˆï¼‰
    if not parsed and weak_kc_list:
        sections = re.split(r'Concept:', llm_response, flags=re.IGNORECASE)
        sections = [s.strip() for s in sections[1:] if s.strip()]  # è·³è¿‡ç¬¬ä¸€ä¸ªç©ºæ®µ
        
        # æŒ‰é¡ºåºå°†æ®µè½åˆ†é…ç»™çŸ¥è¯†ç‚¹
        for i, kc_name in enumerate(weak_kc_list):
            if i < len(sections):
                # æå–æ®µè½çš„ç¬¬ä¸€è¡Œä½œä¸ºLLMè¿”å›çš„çŸ¥è¯†ç‚¹åç§°
                first_line = sections[i].split('\n')[0].strip().strip('*').strip()
                # ä½¿ç”¨åŸå§‹çŸ¥è¯†ç‚¹åç§°ä½œä¸ºkeyï¼Œä½†ä¿ç•™LLMè¿”å›çš„å®Œæ•´å†…å®¹
                content_lines = sections[i].split('\n', 1)
                content = content_lines[1] if len(content_lines) > 1 else sections[i]
                parsed[kc_name] = f"Concept: {kc_name}\n{content}"
                
                # è®°å½•è­¦å‘Šï¼šLLMè¿”å›çš„åç§°ä¸æœŸæœ›ä¸ç¬¦
                if first_line.lower() != kc_name.lower():
                    print(f"   âš ï¸  çŸ¥è¯†ç‚¹åç§°ä¸åŒ¹é… - æœŸæœ›: '{kc_name}', LLMè¿”å›: '{first_line}' (å·²ä½¿ç”¨é¡ºåºåˆ†é…)")
    
    return parsed


def identify_weak_kcs(student_records_df, mastery_lookup=None, student_id=None):
    """
    è¯†åˆ«å­¦ç”Ÿçš„è–„å¼±çŸ¥è¯†ç‚¹
    
    ä¼˜å…ˆçº§ï¼š
    1. å¦‚æœæœ‰æŒæ¡åº¦æ•°æ®ï¼Œä½¿ç”¨ Novice/Developing çš„çŸ¥è¯†ç‚¹
    2. å¦åˆ™ä½¿ç”¨é”™é¢˜ç»Ÿè®¡
    """
    weak_kcs = []
    
    # æ–¹å¼1: åŸºäºæŒæ¡åº¦è¯„ä¼°
    if mastery_lookup and student_id and student_id in mastery_lookup:
        for kc_name, info in mastery_lookup[student_id].items():
            level = (info or {}).get('mastery_level', '')
            if isinstance(level, str) and level.strip() in ['Novice', 'Developing']:
                weak_kcs.append(kc_name)
    
    # æ–¹å¼2: åŸºäºé”™é¢˜ç»Ÿè®¡
    if not weak_kcs:
        wrong_df = student_records_df[student_records_df['score'] == 0]
        if not wrong_df.empty:
            kc_order = wrong_df['know_name'].value_counts().index.tolist()
            weak_kcs = kc_order  # ä¸é™åˆ¶æ•°é‡
    
    return weak_kcs


# --- 4. æ‰¹é‡ç”Ÿæˆè¾…å¯¼å†…å®¹ ---

def save_results_batch(batch_results, results_path, is_first_batch=False):
    """
    æ‰¹é‡ä¿å­˜è¾…å¯¼å†…å®¹ç»“æœï¼ˆè¿½åŠ æ¨¡å¼ï¼Œä½¿ç”¨ pickleï¼‰
    
    Args:
        batch_results: å¾…ä¿å­˜çš„ç»“æœåˆ—è¡¨
        results_path: ç»“æœ pickle æ–‡ä»¶è·¯å¾„
        is_first_batch: æ˜¯å¦æ˜¯ç¬¬ä¸€æ‰¹ï¼ˆå†³å®šæ˜¯å¦åˆ›å»ºæ–°æ–‡ä»¶ï¼‰
    """
    if not batch_results:
        return
    
    batch_df = pd.DataFrame(batch_results)
    
    try:
        if is_first_batch or not os.path.exists(results_path):
            # é¦–æ¬¡ä¿å­˜ï¼Œåˆ›å»ºæ–°æ–‡ä»¶
            combined_df = batch_df
        else:
            # è¿½åŠ æ¨¡å¼ï¼Œè¯»å–å·²æœ‰æ•°æ®å¹¶åˆå¹¶
            existing_df = pd.read_pickle(results_path)
            combined_df = pd.concat([existing_df, batch_df], ignore_index=True)
        
        # ä¿å­˜ä¸º pickle
        combined_df.to_pickle(results_path)
        
        print(f"   ğŸ’¾ å·²ä¿å­˜ {len(batch_results)} æ¡ç»“æœåˆ°æ–‡ä»¶")
    except Exception as e:
        print(f"   âš ï¸  æ‰¹é‡ä¿å­˜å¤±è´¥: {e}")


async def generate_tutoring_for_single_kc(student_id, kc_name, test_question_ids, kc_to_questions_map, 
                                          question_text_map, kc_descriptions, question_choices_df):
    """
    ä¸ºå•ä¸ªå­¦ç”Ÿçš„å•ä¸ªçŸ¥è¯†ç‚¹ç”Ÿæˆè¾…å¯¼å†…å®¹ï¼ˆå¹¶å‘è°ƒç”¨å•å…ƒï¼‰
    
    Args:
        student_id: å­¦ç”ŸID
        kc_name: çŸ¥è¯†ç‚¹åç§°
        test_question_ids: æµ‹è¯•é›†é¢˜ç›®IDé›†åˆï¼ˆéœ€æ’é™¤ï¼‰
        å…¶ä»–å‚æ•°: æ•°æ®æ˜ å°„è¡¨
    
    Returns:
        dict: å•ä¸ªçŸ¥è¯†ç‚¹çš„è¾…å¯¼å†…å®¹è®°å½•ï¼Œå¤±è´¥æ—¶è¿”å›None
    """
    # 1. è·å–è¯¥çŸ¥è¯†ç‚¹çš„ä¾‹é¢˜ï¼ˆæ’é™¤æµ‹è¯•é›†ï¼Œå¯åŒ…å«è®­ç»ƒé›†ï¼‰
    picked = _select_three_questions_for_kc(
        kc_name, 
        kc_to_questions_map, 
        test_question_ids,
        question_text_map,
        question_choices_df,
        max_num=2
    )
    
    if not picked:
        # è¯¥çŸ¥è¯†ç‚¹æ²¡æœ‰å¯ç”¨ä¾‹é¢˜ï¼Œè·³è¿‡
        return None
    
    # 2. æ„å»ºå•ä¸ªçŸ¥è¯†ç‚¹çš„æç¤ºè¯
    kc_description = (kc_descriptions or {}).get(kc_name, '') or ''
    system_prompt, user_prompt = build_tutoring_prompt_single_kc(
        student_id,
        kc_name,
        kc_description,
        picked
    )
    
    # 3. è°ƒç”¨LLM
    try:
        raw_resp = await user_sys_call_with_model(
            user_prompt=user_prompt,
            system_prompt=system_prompt,
            model_name=MODEL_NAME
        )
        tutoring_content = raw_resp.strip()
    except Exception as e:
        print(f"   âŒ å­¦ç”Ÿ {student_id} çŸ¥è¯†ç‚¹ '{kc_name}' LLMè°ƒç”¨å¤±è´¥: {e}")
        raw_resp = f"LLM_CALL_FAILED: {e}"
        tutoring_content = ''
    
    # 4. æ„å»ºç»“æœè®°å½•
    example_q_ids = [ex['question_id'] for ex in picked]
    
    return {
        'student_id': student_id,
        'kc_name': kc_name,
        'tutoring_content': tutoring_content,
        'example_question_ids': json.dumps(example_q_ids),
        'llm_raw_response': raw_resp,
        'prompt_system': system_prompt,
        'prompt_user': user_prompt
    }


async def generate_tutoring_for_student(student_id, student_records_df, kc_to_questions_map, question_text_map, 
                                        kc_descriptions, question_choices_df, mastery_lookup=None, processed_pairs=None):
    """
    ä¸ºå•ä¸ªå­¦ç”Ÿç”Ÿæˆè¾…å¯¼å†…å®¹ï¼ˆä¼˜åŒ–ç‰ˆï¼šçŸ¥è¯†ç‚¹çº§åˆ«å¹¶å‘è°ƒç”¨LLMï¼‰
    
    Args:
        processed_pairs: å·²å®Œæˆçš„ (student_id, kc_name) é›†åˆï¼Œç”¨äºè·³è¿‡å·²ç”Ÿæˆçš„å†…å®¹
    
    Returns:
        list: è¯¥å­¦ç”Ÿæ‰€æœ‰è–„å¼±çŸ¥è¯†ç‚¹çš„è¾…å¯¼å†…å®¹è®°å½•åˆ—è¡¨
    """
    # 1. æ•°æ®åˆ’åˆ†ï¼ˆä¸ run_experiment.py ä¿æŒä¸€è‡´ï¼‰
    if len(student_records_df) > 10:
        train_df, test_df = train_test_split(
            student_records_df, 
            test_size=0.1, 
            random_state=42, 
            shuffle=True
        )
    else:
        train_df = student_records_df
        test_df = pd.DataFrame()
    
    # æå–æµ‹è¯•é›†é¢˜ç›®IDï¼ˆéœ€è¦æ’é™¤ï¼Œé¿å…æ³„éœ²ç­”æ¡ˆï¼‰
    test_question_ids = set(test_df['question_id'].tolist()) if not test_df.empty else set()
    
    # 2. è¯†åˆ«è–„å¼±çŸ¥è¯†ç‚¹
    weak_kcs = identify_weak_kcs(train_df, mastery_lookup, student_id)
    
    if not weak_kcs:
        return []
    
    # 3. è¿‡æ»¤å‡ºéœ€è¦ç”Ÿæˆçš„çŸ¥è¯†ç‚¹ï¼ˆè·³è¿‡å·²æœ‰çš„ï¼‰
    if processed_pairs:
        missing_kcs = [kc for kc in weak_kcs if (student_id, kc) not in processed_pairs]
        if not missing_kcs:
            # è¯¥å­¦ç”Ÿæ‰€æœ‰çŸ¥è¯†ç‚¹éƒ½å·²ç”Ÿæˆ
            return []
        weak_kcs = missing_kcs  # åªç”Ÿæˆç¼ºå¤±çš„çŸ¥è¯†ç‚¹
    
    # ğŸ”¥ 4. å¹¶å‘ç”Ÿæˆæ‰€æœ‰çŸ¥è¯†ç‚¹çš„è¾…å¯¼å†…å®¹ï¼ˆçŸ¥è¯†ç‚¹çº§åˆ«å¹¶å‘ï¼‰
    tasks = []
    for kc_name in weak_kcs:
        task = generate_tutoring_for_single_kc(
            student_id,
            kc_name,
            test_question_ids,
            kc_to_questions_map,
            question_text_map,
            kc_descriptions,
            question_choices_df
        )
        tasks.append(task)
    
    # å¹¶å‘æ‰§è¡Œæ‰€æœ‰çŸ¥è¯†ç‚¹çš„ç”Ÿæˆä»»åŠ¡
    kc_results = await asyncio.gather(*tasks, return_exceptions=True)
    
    # 5. æ”¶é›†æˆåŠŸçš„ç»“æœï¼ˆè¿‡æ»¤æ‰Noneå’Œå¼‚å¸¸ï¼‰
    results = []
    for i, result in enumerate(kc_results):
        if isinstance(result, Exception):
            kc_name = weak_kcs[i]
            print(f"   âŒ å­¦ç”Ÿ {student_id} çŸ¥è¯†ç‚¹ '{kc_name}' ç”Ÿæˆå¤±è´¥: {result}")
        elif result is not None:
            results.append(result)
    
    return results


async def main():
    parser = argparse.ArgumentParser(description="ä¸ºå­¦ç”Ÿç”Ÿæˆä¸ªæ€§åŒ–è¾…å¯¼å†…å®¹ã€‚")
    parser.add_argument("--students", type=int, default=10, help="è¦ç”Ÿæˆè¾…å¯¼å†…å®¹çš„å­¦ç”Ÿæ•°é‡ã€‚è®¾ç½®ä¸º-1åˆ™è¿è¡Œæ‰€æœ‰å­¦ç”Ÿã€‚é»˜è®¤10ä¸ªå­¦ç”Ÿã€‚")
    parser.add_argument("--student-ids", type=str, default=None, help="ä»¥é€—å·åˆ†éš”çš„å­¦ç”ŸIDåˆ—è¡¨ï¼ŒæŒ‡å®šæ—¶ä¼˜å…ˆä½¿ç”¨ã€‚")
    parser.add_argument("--concurrency", type=int, default=30, help="LLM è¯·æ±‚å¹¶å‘æ•°é‡ã€‚é»˜è®¤30ã€‚")
    parser.add_argument("--model", type=str, default="qwen-plus", help="ä½¿ç”¨çš„LLMæ¨¡å‹åç§°ã€‚é»˜è®¤qwen-plusã€‚")
    parser.add_argument("--use-mastery", action="store_true", help="ä½¿ç”¨æŒæ¡åº¦è¯„ä¼°æ•°æ®æ¥è¯†åˆ«è–„å¼±çŸ¥è¯†ç‚¹ï¼ˆå¦‚æœå¯ç”¨ï¼‰ã€‚")
    parser.add_argument("--spread-duration", type=int, default=60, help="å°†æ‰€æœ‰è¯·æ±‚å‡åŒ€åˆ†æ•£åˆ°æŒ‡å®šç§’æ•°å†…ã€‚é»˜è®¤60ç§’ã€‚è®¾ç½®ä¸º0åˆ™ç¦ç”¨ã€‚")
    args = parser.parse_args()
    
    # è®¾ç½®å…¨å±€æ¨¡å‹åç§°
    global MODEL_NAME
    MODEL_NAME = args.model

    if not user_sys_call_with_model:
        print("LLMå·¥å…·æ¨¡å—æœªèƒ½åŠ è½½ï¼Œè¯·æ£€æŸ¥é¡¹ç›®è·¯å¾„ã€‚è„šæœ¬é€€å‡ºã€‚")
        sys.exit(1)

    # 1. æ•°æ®åŠ è½½
    all_student_records, kcs_df, kc_to_questions_map, question_text_map, kc_descriptions, question_choices_df = load_and_preprocess_data(PROJECT_ROOT)

    # 2. é€‰å–å­¦ç”Ÿ
    if args.student_ids:
        available_ids = {int(sid) for sid in all_student_records.keys()}
        specified_ids = []
        for sid in args.student_ids.split(','):
            sid = sid.strip()
            if not sid:
                continue
            sid_int = int(sid)
            if sid_int not in available_ids:
                print(f"è­¦å‘Š: æŒ‡å®šå­¦ç”ŸID {sid_int} ä¸å­˜åœ¨äºæ•°æ®é›†ä¸­ï¼Œå·²å¿½ç•¥ã€‚")
                continue
            specified_ids.append(sid_int)
        if not specified_ids:
            print("æŒ‡å®šçš„å­¦ç”ŸIDåˆ—è¡¨ä¸æ•°æ®é›†ä¸åŒ¹é…ï¼Œé€€å‡ºã€‚")
            return
        student_ids = specified_ids
    else:
        student_ids = sorted(all_student_records.keys())
        if args.students != -1:
            student_ids = student_ids[:min(args.students, len(student_ids))]
    
    print(f"\nå°†ä¸º {len(student_ids)} åå­¦ç”Ÿç”Ÿæˆè¾…å¯¼å†…å®¹...")
    print(f"ä½¿ç”¨æ¨¡å‹: {MODEL_NAME}")
    if args.spread_duration > 0:
        print(f"å‰Šå³°å¡«è°·: å¼€å¯ ({args.spread_duration}ç§’)")
    
    # 3. åŠ è½½æŒæ¡åº¦è¯„ä¼°æ•°æ®ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    mastery_lookup = None
    if args.use_mastery:
        model_suffix = MODEL_NAME.replace('/', '_').replace('.', '_')
        mastery_path = os.path.join(
            PROJECT_ROOT,
            f'results/mastery_assessment_results_minimal_{model_suffix}.csv'
        )
        if os.path.exists(mastery_path):
            try:
                mastery_df = pd.read_csv(mastery_path)
                mastery_lookup = {}
                for student_id in student_ids:
                    student_mastery = mastery_df[mastery_df['student_id'] == student_id]
                    if not student_mastery.empty:
                        mastery_lookup[student_id] = {}
                        for _, row in student_mastery.iterrows():
                            kc_name = row['kc_name']
                            mastery_lookup[student_id][kc_name] = {
                                'mastery_level': row['mastery_level'],
                                'rationale': row.get('rationale', ''),
                                'suggestions': row.get('suggestions', '')
                            }
                print(f"âœ… å·²åŠ è½½æŒæ¡åº¦è¯„ä¼°æ•°æ®: {len(mastery_lookup)} ä¸ªå­¦ç”Ÿ")
            except Exception as e:
                print(f"âš ï¸  åŠ è½½æŒæ¡åº¦æ•°æ®å¤±è´¥: {e}")
        else:
            print(f"âš ï¸  æœªæ‰¾åˆ°æŒæ¡åº¦è¯„ä¼°æ•°æ®: {mastery_path}")

    # å‡†å¤‡è¾“å‡ºç›®å½•
    output_dir = os.path.join(os.path.dirname(__file__), '../results')
    os.makedirs(output_dir, exist_ok=True)
    
    # ç”Ÿæˆå¸¦æ¨¡å‹åç§°çš„æ–‡ä»¶åç¼€
    model_suffix = MODEL_NAME.replace('/', '_').replace('.', '_')
    
    # æ–‡ä»¶è·¯å¾„ï¼ˆä½¿ç”¨ pickle æ ¼å¼ï¼‰
    results_path = os.path.join(output_dir, f'tutoring_content_results_{model_suffix}.pkl')
    log_path = os.path.join(output_dir, f'tutoring_generation_logs_{model_suffix}.txt')
    
    # ğŸ”¥ æ£€æŸ¥å·²å®Œæˆçš„ (student_id, kc_name) å¯¹
    processed_pairs = set()
    if os.path.exists(results_path):
        try:
            # ä½¿ç”¨ pickle è¯»å–ï¼Œé€Ÿåº¦æ›´å¿«
            existing_df = pd.read_pickle(results_path)
            # æ„å»ºå·²æœ‰çš„ (student_id, kc_name) é›†åˆ
            for _, row in existing_df.iterrows():
                processed_pairs.add((row['student_id'], row['kc_name']))
            
            processed_students = set(existing_df['student_id'].unique())
            print(f"\nâœ… æ£€æµ‹åˆ°å·²æœ‰è¾…å¯¼å†…å®¹æ•°æ® (pklæ ¼å¼)")
            print(f"   å·²å®Œæˆå­¦ç”Ÿæ•°: {len(processed_students)}")
            print(f"   å·²å®Œæˆè¾…å¯¼å¯¹æ•°: {len(processed_pairs)}")
        except Exception as e:
            print(f"âš ï¸  è¯»å–å·²æœ‰æ•°æ®å¤±è´¥: {e}")
    
    # ğŸ”¥ è®¡ç®—éœ€è¦å¤„ç†çš„å­¦ç”Ÿï¼ˆæœ‰ç¼ºå¤±çŸ¥è¯†ç‚¹çš„å­¦ç”Ÿï¼‰
    # å…ˆè¯†åˆ«æ¯ä¸ªå­¦ç”Ÿçš„è–„å¼±çŸ¥è¯†ç‚¹
    student_weak_kcs_map = {}
    for student_id in student_ids:
        if student_id not in all_student_records:
            continue
        student_records_df = all_student_records[student_id]
        
        # æ•°æ®åˆ’åˆ†
        if len(student_records_df) > 10:
            train_df, _ = train_test_split(
                student_records_df, 
                test_size=0.1, 
                random_state=42, 
                shuffle=True
            )
        else:
            train_df = student_records_df
        
        # è¯†åˆ«è–„å¼±çŸ¥è¯†ç‚¹
        weak_kcs = identify_weak_kcs(train_df, mastery_lookup, student_id)
        student_weak_kcs_map[student_id] = weak_kcs
    
    # è®¡ç®—ç¼ºå¤±çš„è¾…å¯¼å¯¹
    missing_pairs = set()
    for student_id, weak_kcs in student_weak_kcs_map.items():
        for kc_name in weak_kcs:
            if (student_id, kc_name) not in processed_pairs:
                missing_pairs.add((student_id, kc_name))
    
    # æ‰¾å‡ºæœ‰ç¼ºå¤±çŸ¥è¯†ç‚¹çš„å­¦ç”Ÿ
    pending_students = []
    for student_id in student_ids:
        weak_kcs = student_weak_kcs_map.get(student_id, [])
        if not weak_kcs:
            continue
        # æ£€æŸ¥è¯¥å­¦ç”Ÿæ˜¯å¦æœ‰ç¼ºå¤±çš„çŸ¥è¯†ç‚¹
        has_missing = any((student_id, kc) not in processed_pairs for kc in weak_kcs)
        if has_missing:
            pending_students.append(student_id)
    
    if not pending_students:
        print(f"\nâœ… æ‰€æœ‰å­¦ç”Ÿçš„æ‰€æœ‰è–„å¼±çŸ¥è¯†ç‚¹è¾…å¯¼å†…å®¹å·²ç”Ÿæˆå®Œæˆï¼Œæ— éœ€ç»§ç»­å¤„ç†")
        return
    
    print(f"\nğŸ“ å¾…å¤„ç†æƒ…å†µ:")
    print(f"   â€¢ æœ‰ç¼ºå¤±çš„å­¦ç”Ÿæ•°: {len(pending_students)}")
    print(f"   â€¢ ç¼ºå¤±çš„è¾…å¯¼å¯¹æ•°: {len(missing_pairs)}")
    
    # æ˜¾ç¤ºéƒ¨åˆ†ç¼ºå¤±è¯¦æƒ…
    if len(pending_students) <= 10:
        print(f"\n   ç¼ºå¤±è¯¦æƒ…:")
        for sid in sorted(pending_students):
            weak_kcs = student_weak_kcs_map.get(sid, [])
            missing_kcs = [kc for kc in weak_kcs if (sid, kc) not in processed_pairs]
            print(f"   â€¢ å­¦ç”Ÿ {sid}: ç¼ºå¤± {len(missing_kcs)}/{len(weak_kcs)} ä¸ªçŸ¥è¯†ç‚¹")
    else:
        print(f"   ï¼ˆéƒ¨åˆ†å­¦ç”Ÿçš„ç¼ºå¤±è¯¦æƒ…ï¼‰:")
        for sid in sorted(pending_students)[:5]:
            weak_kcs = student_weak_kcs_map.get(sid, [])
            missing_kcs = [kc for kc in weak_kcs if (sid, kc) not in processed_pairs]
            print(f"   â€¢ å­¦ç”Ÿ {sid}: ç¼ºå¤± {len(missing_kcs)}/{len(weak_kcs)} ä¸ªçŸ¥è¯†ç‚¹")
        print(f"   ... è¿˜æœ‰ {len(pending_students) - 5} ä¸ªå­¦ç”Ÿæœªæ˜¾ç¤º")
    
    # 4. æ‰¹é‡ç”Ÿæˆè¾…å¯¼å†…å®¹ï¼ˆæ”¹ä¸º KC çº§åˆ«å¹¶å‘ï¼‰
    print(f"\n{'='*80}")
    print(f"ğŸš€ å¼€å§‹ç”Ÿæˆè¾…å¯¼å†…å®¹ï¼ˆKCçº§åˆ«å¹¶å‘ï¼‰".center(80))
    print(f"{'='*80}")
    
    all_results = []
    save_every_n_kcs = 100  # ğŸ”¥ æ”¹ä¸ºæ¯å®Œæˆ100ä¸ªKCä¿å­˜ä¸€æ¬¡
    kcs_since_last_save = 0
    is_first_batch = not os.path.exists(results_path)
    completed_kcs = 0
    
    # ä½¿ç”¨ Semaphore æ§åˆ¶å¹¶å‘
    semaphore = asyncio.Semaphore(args.concurrency)
    
    async def process_single_kc(student_id, kc_name, delay):
        """ğŸ”¥ å¤„ç†å•ä¸ª(å­¦ç”Ÿ, KC)å¯¹ - KCçº§åˆ«å¹¶å‘"""
        if delay > 0:
            await asyncio.sleep(delay)
        
        async with semaphore:
            try:
                student_records_df = all_student_records[student_id]
                
                # æ•°æ®åˆ’åˆ†
                if len(student_records_df) > 10:
                    train_df, test_df = train_test_split(
                        student_records_df, 
                        test_size=0.1, 
                        random_state=42, 
                        shuffle=True
                    )
                else:
                    train_df = student_records_df
                    test_df = pd.DataFrame()
                
                test_question_ids = set(test_df['question_id'].tolist()) if not test_df.empty else set()
                
                # ğŸ”¥ ä¸ºå•ä¸ªKCç”Ÿæˆè¾…å¯¼å†…å®¹ï¼ˆè¶…æ—¶120ç§’ï¼‰
                result = await asyncio.wait_for(
                    generate_tutoring_for_single_kc(
                        student_id,
                        kc_name,
                        test_question_ids,
                        kc_to_questions_map,
                        question_text_map,
                        kc_descriptions,
                        question_choices_df
                    ),
                    timeout=9999  # ğŸ”¥ å¢åŠ åˆ°180ç§’ï¼ˆ3åˆ†é’Ÿï¼‰
                )
                return (student_id, kc_name, result, None)
            except asyncio.TimeoutError:
                return (student_id, kc_name, None, "è¶…æ—¶(120s)")
            except Exception as e:
                return (student_id, kc_name, None, f"{type(e).__name__}: {str(e)}")
    
    # ğŸ”¥ åˆ›å»ºä»»åŠ¡ï¼šæŒ‰(å­¦ç”Ÿ, KC)å¯¹ç»´åº¦
    print(f"\nğŸ“¦ å‡†å¤‡å¼‚æ­¥ä»»åŠ¡ï¼ˆKCçº§åˆ«å¹¶å‘ï¼‰...")
    print(f"   â€¢ æ€»KCå¯¹æ•°: {len(missing_pairs)}")
    print(f"   â€¢ å¹¶å‘åº¦: {args.concurrency}")
    
    tasks = []
    if args.spread_duration > 0:
        # åœ¨æŒ‡å®šæ—¶é—´å†…å‡åŒ€åˆ†å¸ƒæ‰€æœ‰KCä»»åŠ¡
        max_spread = min(args.spread_duration, 300)  # æœ€å¤š5åˆ†é’Ÿ
        delay_per_kc = max_spread / len(missing_pairs)
        print(f"   â€¢ å‰Šå³°å¡«è°·: {max_spread}ç§’å†…å‡åŒ€åˆ†å¸ƒ (æ¯KCå»¶è¿Ÿ {delay_per_kc*1000:.1f}ms)")
        
        for i, (student_id, kc_name) in enumerate(missing_pairs):
            delay = i * delay_per_kc
            tasks.append(process_single_kc(student_id, kc_name, delay))
    else:
        print(f"   â€¢ å¹¶å‘æ¨¡å¼: æ— å»¶è¿Ÿï¼Œç«‹å³å¯åŠ¨")
        for student_id, kc_name in missing_pairs:
            tasks.append(process_single_kc(student_id, kc_name, 0))
    
    print(f"âœ… ä»»åŠ¡åˆ›å»ºå®Œæˆï¼Œå¼€å§‹å¹¶å‘æ‰§è¡Œ...\n")
    
    # ğŸ”¥ å¹¶å‘æ‰§è¡Œï¼šæ¯å®Œæˆä¸€ä¸ªKCæ›´æ–°ä¸€æ¬¡è¿›åº¦æ¡
    import time
    last_update_time = time.time()
    pbar = tqdm(total=len(missing_pairs), desc="ç”ŸæˆçŸ¥è¯†ç‚¹è¾…å¯¼", unit="KC", mininterval=0.1)
    
    # å¿ƒè·³ç›‘æ§
    async def heartbeat_monitor():
        nonlocal last_update_time
        while True:
            await asyncio.sleep(10)
            elapsed = time.time() - last_update_time
            if elapsed > 30:
                pbar.write(f"â° å¿ƒè·³æ£€æµ‹: å·² {elapsed:.0f}s æ— è¿›å±•...")
    
    heartbeat_task = asyncio.create_task(heartbeat_monitor())
    pbar.write("ğŸš€ è¿›åº¦æ¡å·²å¯åŠ¨ï¼Œå¼€å§‹å¤„ç†ä»»åŠ¡...")
    
    # ğŸ”¥ é€ä¸ªå¤„ç†å®Œæˆçš„ä»»åŠ¡
    for future in asyncio.as_completed(tasks):
        student_id, kc_name, result, error = await future
        last_update_time = time.time()
        
        if error:
            pbar.write(f"   âŒ å­¦ç”Ÿ{student_id}-KC[{kc_name}] å¤±è´¥: {error}")
        elif result:
            all_results.append(result)
            kcs_since_last_save += 1
        
        # ğŸ”¥ æ›´æ–°è¿›åº¦æ¡
        pbar.update(1)
        
        # ğŸ”¥ æ¯å®Œæˆ100ä¸ªKCä¿å­˜ä¸€æ¬¡
        if kcs_since_last_save >= save_every_n_kcs:
            save_results_batch(all_results, results_path, is_first_batch)
            pbar.write(f"ğŸ’¾ å·²ä¿å­˜ {len(all_results)} æ¡è®°å½•ï¼ˆ{kcs_since_last_save} ä¸ªKCï¼‰")
            all_results = []
            kcs_since_last_save = 0
            is_first_batch = False
    
    pbar.close()
    heartbeat_task.cancel()
    
    # ä¿å­˜å‰©ä½™ç»“æœ
    if all_results:
        save_results_batch(all_results, results_path, is_first_batch)
    
    print(f"\n{'='*80}")
    print(f"âœ… è¾…å¯¼å†…å®¹ç”Ÿæˆå®Œæˆ".center(80))
    print(f"{'='*80}")
    print(f"   ğŸ“ ç»“æœæ–‡ä»¶: {results_path}")
    print(f"   ğŸ“ æ—¥å¿—æ–‡ä»¶: {log_path}")
    
    # ç»Ÿè®¡ä¿¡æ¯
    if os.path.exists(results_path):
        final_df = pd.read_pickle(results_path)
        print(f"\nğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
        print(f"   â€¢ æ€»å­¦ç”Ÿæ•°: {final_df['student_id'].nunique()}")
        print(f"   â€¢ æ€»çŸ¥è¯†ç‚¹æ•°: {final_df['kc_name'].nunique()}")
        print(f"   â€¢ æ€»è®°å½•æ•°: {len(final_df)}")
        print(f"   â€¢ å¹³å‡æ¯å­¦ç”Ÿè¾…å¯¼çŸ¥è¯†ç‚¹æ•°: {len(final_df) / final_df['student_id'].nunique():.1f}")
        
        # æ˜¾ç¤ºæ–‡ä»¶å¤§å°
        pkl_size = os.path.getsize(results_path) / (1024 * 1024)
        print(f"   â€¢ æ–‡ä»¶å¤§å°: {pkl_size:.2f} MB")


if __name__ == "__main__":
    asyncio.run(main())

