import pandas as pd
import numpy as np
import json
import random
import os
import sys
import asyncio
import argparse
from tqdm import tqdm
from sklearn.model_selection import train_test_split

# --- 1. è®¾ç½®é¡¹ç›®è·¯å¾„ ---
def setup_project_path():
    """
    è·å–é¡¹ç›®æ ¹ç›®å½•ï¼ˆå»æ‰äº†æ—§çš„ yl_data_process æŸ¥æ‰¾é€»è¾‘ï¼‰
    """
    try:
        current_path = os.path.dirname(os.path.abspath(__file__))
    except NameError:
        current_path = os.getcwd()
    
    # é¡¹ç›®æ ¹ç›®å½•æ˜¯ Code æ–‡ä»¶å¤¹çš„çˆ¶ç›®å½•
    project_root = os.path.dirname(current_path)
    print(f"é¡¹ç›®æ ¹ç›®å½•: {project_root}")
    return project_root

# è®¾ç½®è·¯å¾„
PROJECT_ROOT = setup_project_path()

# å¯¼å…¥ LLM å·¥å…·å‡½æ•°
from llms.qwen import user_sys_call as user_sys_call_with_model

# --- Agent Model Config ---
MODEL_NAME = "gpt-3.5-turbo"  # é»˜è®¤ä½¿ç”¨ GPT-3.5-Turbo æ¨¡å‹


# --- 2. æ•°æ®åŠ è½½ä¸é¢„å¤„ç† ---
def load_and_prepare_data(project_root):
    """
    åŠ è½½æ‰€æœ‰ç›¸å…³æ•°æ®ï¼Œå¹¶åˆ›å»ºä¸€ä¸ªåŒ…å«å®Œæ•´å­¦ä¹ è®°å½•çš„ DataFrameã€‚
    è¿™ä¸ª DataFrame çš„æ¯ä¸€è¡Œä»£è¡¨å­¦ç”Ÿåœ¨æŸä¸ªé—®é¢˜ä¸Šä¸å•ä¸ªçŸ¥è¯†ç‚¹çš„äº¤äº’ã€‚
    """
    print("\n" + "="*20, "é˜¶æ®µ1: æ•°æ®åŠ è½½ä¸é¢„å¤„ç†", "="*20)
    data_path = os.path.join(project_root, 'data/')
    
    try:
        questions_df = pd.read_csv(os.path.join(data_path, "Questions.csv"))
        question_choices_df = pd.read_csv(os.path.join(data_path, "Question_Choices.csv"))
        q_kc_rels_df = pd.read_csv(os.path.join(data_path, "Question_KC_Relationships.csv"))
        transactions_df = pd.read_csv(os.path.join(data_path, "Transaction.csv"))
        kcs_df = pd.read_csv(os.path.join(data_path, "KCs.csv"))
        kc_rels_df = pd.read_csv(os.path.join(data_path, "KC_Relationships.csv"))
        print("æ‰€æœ‰æ•°æ®æ–‡ä»¶åŠ è½½æˆåŠŸï¼")
    except FileNotFoundError as e:
        print(f"åŠ è½½æ–‡ä»¶æ—¶å‡ºé”™: {e}")
        sys.exit(1)

    # åˆ›å»º KC ID -> KC Name çš„æ˜ å°„
    kc_id_to_name_map = kcs_df.set_index('id')['name'].to_dict()

    # 1. å°† Transaction å’Œ Question å…³è”
    trans_q_df = pd.merge(
        transactions_df, 
        questions_df[['id', 'question_text']], 
        left_on='question_id', 
        right_on='id', 
        how='left'
    ).drop(columns=['id_y']).rename(columns={'id_x': 'id'})

    # 2. å°† Question å’Œ KC å…³è” (ä¸€ä¸ªé—®é¢˜å¯èƒ½å…³è”å¤šä¸ªKC)
    q_kc_rels_df['kc_name'] = q_kc_rels_df['knowledgecomponent_id'].map(kc_id_to_name_map)
    
    # 3. å°†å®Œæ•´çš„ Transaction å’Œ KC ä¿¡æ¯å…³è”èµ·æ¥
    # è¿™ä¼šä½¿æ¯ä¸ª transaction è®°å½•æ ¹æ®å…¶å…³è”çš„ KC æ•°é‡è¿›è¡Œå¤åˆ¶
    full_log_df = pd.merge(
        trans_q_df,
        q_kc_rels_df[['question_id', 'kc_name']],
        on='question_id',
        how='left'
    )
    
    # æ•°æ®æ¸…æ´—
    full_log_df = full_log_df.dropna(subset=['kc_name'])
    full_log_df['score'] = full_log_df['answer_state'].astype(int)
    full_log_df = full_log_df.sort_values(by='start_time').reset_index(drop=True)
    
    print(f"æ•°æ®é¢„å¤„ç†å®Œæˆï¼Œç”Ÿæˆäº† {len(full_log_df)} æ¡åŒ…å«çŸ¥è¯†ç‚¹çš„å­¦ä¹ è®°å½•ã€‚")
    
    # å‡†å¤‡ KC ä¾èµ–å…³ç³»å›¾
    kc_rels_df['from_kc_name'] = kc_rels_df['from_knowledgecomponent_id'].map(kc_id_to_name_map)
    kc_rels_df['to_kc_name'] = kc_rels_df['to_knowledgecomponent_id'].map(kc_id_to_name_map)
    kc_graph = set(zip(kc_rels_df['from_kc_name'], kc_rels_df['to_kc_name']))

    return full_log_df, kcs_df, kc_graph, question_choices_df


# --- 3. æ™ºèƒ½ä½“æ ¸å¿ƒåŠŸèƒ½ ---

def save_results_batch(batch_results, results_path, is_first_batch=False):
    """
    æ‰¹é‡ä¿å­˜è¯„ä¼°ç»“æœï¼ˆè¿½åŠ æ¨¡å¼ï¼‰
    
    Args:
        batch_results: å¾…ä¿å­˜çš„ç»“æœåˆ—è¡¨
        results_path: ç»“æœCSVæ–‡ä»¶è·¯å¾„
        is_first_batch: æ˜¯å¦æ˜¯ç¬¬ä¸€æ‰¹ï¼ˆå†³å®šæ˜¯å¦å†™å…¥è¡¨å¤´ï¼‰
    """
    if not batch_results:
        return
    
    batch_df = pd.DataFrame(batch_results)
    
    try:
        if is_first_batch or not os.path.exists(results_path):
            # é¦–æ¬¡ä¿å­˜ï¼Œå†™å…¥è¡¨å¤´
            batch_df.to_csv(results_path, index=False, encoding='utf-8-sig', mode='w')
        else:
            # è¿½åŠ æ¨¡å¼ï¼Œä¸å†™è¡¨å¤´
            batch_df.to_csv(results_path, index=False, encoding='utf-8-sig', mode='a', header=False)
        
        print(f"   ğŸ’¾ å·²ä¿å­˜ {len(batch_results)} æ¡ç»“æœåˆ°æ–‡ä»¶")
    except Exception as e:
        print(f"   âš ï¸  æ‰¹é‡ä¿å­˜å¤±è´¥: {e}")


def generate_request_manifest(student_ids, full_log_df, kcs_df, kc_graph, question_choices_df, 
                               include_behavioral_data, manifest_path):
    """
    ç”Ÿæˆè¯·æ±‚æ¸…å•æ–‡ä»¶ï¼ˆä¸€æ¬¡æ€§å‡†å¤‡æ‰€æœ‰è¯·æ±‚ï¼Œç»“æœåˆ—ç•™ç©ºï¼‰
    
    Args:
        student_ids: å­¦ç”ŸIDåˆ—è¡¨
        full_log_df: å®Œæ•´å­¦ä¹ è®°å½•
        kcs_df: çŸ¥è¯†ç‚¹DataFrame
        kc_graph: çŸ¥è¯†ç‚¹ä¾èµ–å›¾
        question_choices_df: é¢˜ç›®é€‰é¡¹DataFrame
        include_behavioral_data: æ˜¯å¦åŒ…å«è¡Œä¸ºæ•°æ®
        manifest_path: æ¸…å•æ–‡ä»¶ä¿å­˜è·¯å¾„
    
    Returns:
        DataFrame: è¯·æ±‚æ¸…å•
    """
    print(f"\n{'='*80}")
    print(f"ğŸ”¨ ç”Ÿæˆè¯·æ±‚æ¸…å•".center(80))
    print(f"{'='*80}")
    
    manifest_records = []
    kc_info_map = kcs_df.set_index('name')['description'].to_dict()
    
    for student_id in tqdm(student_ids, desc="å‡†å¤‡è¯·æ±‚æ¸…å•"):
        student_records = full_log_df[full_log_df['student_id'] == student_id]
        
        # ä½¿ç”¨è®­ç»ƒé›†æ•°æ®
        if len(student_records) > 10:
            train_records, _ = train_test_split(
                student_records, 
                test_size=0.1, 
                random_state=42, 
                shuffle=True
            )
            student_records = train_records
        
        practiced_kcs = student_records['kc_name'].unique()
        
        for kc_name in practiced_kcs:
            trajectory_df = get_student_kc_trajectory(full_log_df, student_id, kc_name, use_train_only=True)
            
            if trajectory_df.empty:
                continue
            
            kc_description = kc_info_map.get(kc_name, "No description available.")
            prerequisites = [pre for pre, post in kc_graph if post == kc_name]
            system_prompt, user_prompt = build_mastery_prompt(
                student_id, kc_name, kc_description, trajectory_df, 
                question_choices_df, prerequisites, include_behavioral_data
            )
            
            manifest_records.append({
                'student_id': student_id,
                'kc_name': kc_name,
                'system_prompt': system_prompt,
                'user_prompt': user_prompt,
                'mastery_level': '',  # å¾…å¡«å……
                'rationale': '',      # å¾…å¡«å……
                'suggestions': '',    # å¾…å¡«å……
                'llm_raw_response': '' # å¾…å¡«å……
            })
    
    manifest_df = pd.DataFrame(manifest_records)
    manifest_df.to_csv(manifest_path, index=False, encoding='utf-8-sig')
    
    print(f"\nâœ… è¯·æ±‚æ¸…å•å·²ç”Ÿæˆ: {manifest_path}")
    print(f"   ğŸ“Š æ€»è¯·æ±‚æ•°: {len(manifest_df)}")
    print(f"   ğŸ‘¥ æ¶‰åŠå­¦ç”Ÿ: {manifest_df['student_id'].nunique()}")
    print(f"   ğŸ¯ æ¶‰åŠçŸ¥è¯†ç‚¹: {manifest_df['kc_name'].nunique()}")
    print(f"{'='*80}\n")
    
    return manifest_df


def load_request_manifest(manifest_path, results_path):
    """
    åŠ è½½è¯·æ±‚æ¸…å•ï¼Œç­›é€‰å‡ºæœªå®Œæˆçš„è¯·æ±‚
    
    Args:
        manifest_path: è¯·æ±‚æ¸…å•æ–‡ä»¶è·¯å¾„
        results_path: ç»“æœæ–‡ä»¶è·¯å¾„
    
    Returns:
        tuple: (å®Œæ•´æ¸…å•DataFrame, å¾…å¤„ç†çš„è¯·æ±‚åˆ—è¡¨)
    """
    if not os.path.exists(manifest_path):
        return None, []
    
    print(f"\n{'='*80}")
    print(f"ğŸ“‹ åŠ è½½è¯·æ±‚æ¸…å•".center(80))
    print(f"{'='*80}")
    
    try:
        manifest_df = pd.read_csv(manifest_path)
        print(f"   âœ… æ¸…å•åŠ è½½æˆåŠŸ: {len(manifest_df)} æ¡è¯·æ±‚")
        
        # æ£€æŸ¥å·²å®Œæˆçš„ç»“æœ
        processed_pairs = set()
        if os.path.exists(results_path):
            results_df = pd.read_csv(results_path)
            # ç­›é€‰å·²å®Œæˆçš„è®°å½•ï¼ˆmastery_level ä¸ä¸ºç©ºï¼‰
            completed_df = results_df[results_df['mastery_level'].notna() & (results_df['mastery_level'] != '')]
            processed_pairs = set(zip(completed_df['student_id'], completed_df['kc_name']))
            print(f"   âœ… å·²å®Œæˆè¯·æ±‚: {len(processed_pairs)} æ¡")
        
        # ç­›é€‰æœªå®Œæˆçš„è¯·æ±‚
        pending_requests = []
        for idx, row in manifest_df.iterrows():
            if (row['student_id'], row['kc_name']) not in processed_pairs:
                pending_requests.append({
                    'index': idx,  # è®°å½•åœ¨æ¸…å•ä¸­çš„ä½ç½®
                    'system_prompt': row['system_prompt'],
                    'user_prompt': row['user_prompt'],
                    'model_name': MODEL_NAME,
                    'context': {
                        'student_id': row['student_id'],
                        'kc_name': row['kc_name']
                    }
                })
        
        print(f"   ğŸ“ å¾…å¤„ç†è¯·æ±‚: {len(pending_requests)} æ¡")
        print(f"{'='*80}\n")
        
        return manifest_df, pending_requests
        
    except Exception as e:
        print(f"   âš ï¸  åŠ è½½æ¸…å•å¤±è´¥: {e}")
        return None, []

def get_student_kc_trajectory(full_log_df, student_id, kc_name, use_train_only=True):
    """
    ä¸ºæŒ‡å®šå­¦ç”Ÿå’ŒçŸ¥è¯†ç‚¹ï¼Œæå–å…¶å­¦ä¹ è½¨è¿¹ã€‚
    
    Args:
        full_log_df: å®Œæ•´çš„å­¦ä¹ è®°å½•DataFrame
        student_id: å­¦ç”ŸID
        kc_name: çŸ¥è¯†ç‚¹åç§°
        use_train_only: æ˜¯å¦åªä½¿ç”¨è®­ç»ƒé›†æ•°æ®ï¼ˆé»˜è®¤Trueï¼Œé¿å…æ•°æ®æ³„éœ²ï¼‰
    """
    student_df = full_log_df[full_log_df['student_id'] == student_id]
    
    # ğŸ”¥ å…³é”®ä¿®å¤ï¼šåªä½¿ç”¨è®­ç»ƒé›†æ•°æ®è¿›è¡ŒæŒæ¡åº¦è¯„ä¼°ï¼Œé¿å…æ•°æ®æ³„éœ²
    if use_train_only and len(student_df) > 10:  # ç¡®ä¿æœ‰è¶³å¤Ÿæ•°æ®è¿›è¡Œåˆ’åˆ†
        train_df, _ = train_test_split(
            student_df, 
            test_size=0.1, 
            random_state=42, 
            shuffle=True
        )
        student_df = train_df
    
    # ç­›é€‰å‡ºä¸ç›®æ ‡KCç›´æ¥ç›¸å…³çš„ç»ƒä¹ è®°å½•
    kc_trajectory_df = student_df[student_df['kc_name'] == kc_name].copy()
    
    # ä¸ºäº†æä¾›æ›´ä¸°å¯Œçš„ä¸Šä¸‹æ–‡ï¼Œæˆ‘ä»¬è¿˜éœ€è¦æ‰¾å‡ºæ¯æ¬¡ç»ƒä¹ ä¸­è¿˜æ¶‰åŠäº†å“ªäº›å…¶ä»–KC
    other_kcs_list = []
    for _, row in kc_trajectory_df.iterrows():
        # åœ¨è¯¥å­¦ç”Ÿçš„æ‰€æœ‰è®°å½•ä¸­ï¼Œæ‰¾åˆ°ä¸å½“å‰è®°å½•æ—¶é—´ã€é¢˜ç›®IDéƒ½ç›¸åŒçš„è®°å½•
        related_records = student_df[
            (student_df['start_time'] == row['start_time']) &
            (student_df['question_id'] == row['question_id'])
        ]
        # æå–é™¤ç›®æ ‡KCå¤–çš„å…¶ä»–KC
        other_kcs = list(related_records[related_records['kc_name'] != kc_name]['kc_name'].unique())
        other_kcs_list.append(other_kcs)
        
    kc_trajectory_df['other_kcs'] = other_kcs_list
    
    return kc_trajectory_df.sort_values(by='start_time')

def build_mastery_prompt(student_id, kc_name, kc_description, trajectory_df, question_choices_df, prerequisite_kcs=None, include_behavioral_data=True):
    """
    æ„å»ºç”¨äºè¯„ä¼°çŸ¥è¯†ç‚¹æŒæ¡ç¨‹åº¦çš„LLM Promptï¼ˆåŸºäºè€ƒè¯•è¡¨ç°æ•°æ®ï¼‰ã€‚
    è¿”å› (system_prompt, user_prompt)
    
    å‚æ•°:
        include_behavioral_data: æ˜¯å¦åŒ…å«è¡Œä¸ºæ•°æ®ï¼ˆå­—æ®µ6-12ï¼‰
            - True: å®Œæ•´ç‰ˆï¼ŒåŒ…å«æ‰€æœ‰å­—æ®µ
            - False: ç²¾ç®€ç‰ˆï¼ŒåªåŒ…å«å­—æ®µ1-5ï¼ˆåŸºç¡€ä¿¡æ¯+ç»“æœï¼‰
    """
    # 1. è§’è‰²æ‰®æ¼”æŒ‡ä»¤ -> System Prompt
    system_prompt = """You are an experienced educational assessment expert. Your task is to evaluate a student's mastery level of a specific knowledge component based on their exam performance data.

Focus on analyzing:
- Overall performance patterns across all questions
- Performance consistency and stability
- Handling of questions with different difficulties
- Behavioral signals (confidence, hint usage, hesitation)
- Performance on questions involving multiple knowledge components"""

    # 2. ç”¨æˆ·æŒ‡ä»¤ä¸èƒŒæ™¯ä¿¡æ¯
    user_prompt = "--- ASSESSMENT CONTEXT ---\n"
    user_prompt += f"Student ID: {student_id}\n"
    user_prompt += f"Knowledge Component: '{kc_name}'\n"
    if kc_description:
        user_prompt += f"Description: {kc_description}\n"
    
    # 3. æ ¸å¿ƒè¯æ®ï¼šè¯¦ç»†çš„è€ƒè¯•ç­”é¢˜è®°å½•
    user_prompt += f"\n--- EXAM PERFORMANCE RECORDS FOR '{kc_name}' ---\n"
    user_prompt += f"Total questions answered: {len(trajectory_df)}\n\n"
    
    if trajectory_df.empty:
        user_prompt += "No exam records found for this knowledge component.\n"
    else:
        for idx, (_, row) in enumerate(trajectory_df.iterrows(), 1):
            user_prompt += f"ã€Question {idx}ã€‘\n"
            user_prompt += f"  â€¢ Question ID: {row['question_id']}\n"
            
            # é¢˜ç›®å†…å®¹ï¼ˆæ ¸å¿ƒä¿¡æ¯ï¼‰
            if pd.notna(row.get('question_text')):
                question_text = str(row['question_text']).strip()
                # é™åˆ¶é•¿åº¦ï¼Œé¿å…promptè¿‡é•¿
                if len(question_text) > 150:
                    question_text = question_text[:150] + "..."
                user_prompt += f"  â€¢ Question Content: {question_text}\n"
            
            # é¢˜ç›®é€‰é¡¹
            question_id = row['question_id']
            choices = question_choices_df[question_choices_df['question_id'] == question_id]
            if not choices.empty:
                user_prompt += f"  â€¢ Answer Choices:\n"
                for choice_idx, choice_row in choices.iterrows():
                    choice_text = str(choice_row['choice_text']).strip()
                    is_correct = choice_row['is_correct']
                    choice_id = choice_row['id']
                    
                    # æ ‡è®°æ­£ç¡®ç­”æ¡ˆ
                    correct_mark = " [Correct Answer]" if is_correct else ""
                    
                    # æ ‡è®°å­¦ç”Ÿé€‰æ‹©
                    student_choice_mark = ""
                    if pd.notna(row.get('answer_choice_id')) and choice_id == row['answer_choice_id']:
                        student_choice_mark = " â† [Student's Choice]"
                    
                    user_prompt += f"    - {choice_text}{correct_mark}{student_choice_mark}\n"
            
            # å­¦ç”Ÿç­”æ¡ˆæ–‡æœ¬ï¼ˆå¦‚æœæœ‰ï¼‰
            if pd.notna(row.get('answer_text')) and str(row['answer_text']).strip():
                user_prompt += f"  â€¢ Student's Answer Text: {str(row['answer_text']).strip()}\n"
            
            user_prompt += f"  â€¢ Result: {'âœ“ Correct' if row['score'] == 1 else 'âœ— Incorrect'}\n"
            
            # å¦‚æœåŒ…å«è¡Œä¸ºæ•°æ®ï¼Œåˆ™æ·»åŠ å­—æ®µ6-12
            if include_behavioral_data:
                # é¢˜ç›®éš¾åº¦
                if pd.notna(row.get('difficulty')):
                    difficulty_map = {0: 'Very Easy', 1: 'Easy', 2: 'Medium', 3: 'Hard', 4: 'Very Hard'}
                    user_prompt += f"  â€¢ Question Difficulty: {difficulty_map.get(row['difficulty'], 'Unknown')} (Level {row['difficulty']})\n"
                
                # å­¦ç”Ÿæ„ŸçŸ¥éš¾åº¦
                if pd.notna(row.get('difficulty_feedback')):
                    perceived_map = {0: 'Very Easy', 1: 'Easy', 2: 'Medium', 3: 'Hard'}
                    user_prompt += f"  â€¢ Student's Perceived Difficulty: {perceived_map.get(row['difficulty_feedback'], 'Unknown')} (Level {row['difficulty_feedback']})\n"
                
                # ä¿¡å¿ƒåº¦
                if pd.notna(row.get('trust_feedback')):
                    confidence_map = {0: 'No confidence', 1: 'Low confidence', 2: 'Medium confidence', 3: 'High confidence'}
                    user_prompt += f"  â€¢ Confidence Level: {confidence_map.get(row['trust_feedback'], 'Unknown')} ({row['trust_feedback']}/3)\n"
                
                # æç¤ºä½¿ç”¨
                if pd.notna(row.get('hint_used')):
                    user_prompt += f"  â€¢ Used Hint: {'Yes' if row['hint_used'] else 'No'}\n"
                
                # é€‰æ‹©å˜æ›´æ¬¡æ•°ï¼ˆåæ˜ çŠ¹è±«ç¨‹åº¦ï¼‰
                if pd.notna(row.get('selection_change')):
                    changes = int(row['selection_change'])
                    user_prompt += f"  â€¢ Answer Changes: {changes}"
                    if changes > 2:
                        user_prompt += " (significant hesitation)"
                    elif changes > 0:
                        user_prompt += " (some hesitation)"
                    user_prompt += "\n"
                
                # ç­”é¢˜æ—¶é•¿
                if pd.notna(row.get('duration')) and row['duration'] > 0:
                    duration_sec = row['duration']
                    user_prompt += f"  â€¢ Time Spent: {duration_sec:.1f} seconds"
                    if duration_sec > 120:
                        user_prompt += " (took longer time)"
                    elif duration_sec < 10:
                        user_prompt += " (answered quickly)"
                    user_prompt += "\n"
                
                # å…³è”çš„å…¶ä»–çŸ¥è¯†ç‚¹
                if row.get('other_kcs'):
                    user_prompt += f"  â€¢ Other KCs in this question: {', '.join(row['other_kcs'])}\n"
            
            user_prompt += "\n"

    # 4. ä»»åŠ¡æŒ‡ä»¤
    user_prompt += """--- ASSESSMENT TASK ---

Based on the exam performance records above, evaluate the student's mastery level of this knowledge component.

Choose ONE mastery level from: [Novice, Developing, Proficient, Mastered]

Level Definitions:
- Novice: Limited understanding, frequent errors, low confidence
- Developing: Partial understanding, inconsistent performance, needs improvement
- Proficient: Solid understanding, mostly correct answers, occasional mistakes on complex questions
- Mastered: Comprehensive understanding, consistently correct, high confidence across all difficulty levels

Provide:
1. Your chosen mastery level
2. Detailed rationale citing specific question performances and behavioral patterns
3. Actionable suggestions for improvement (if applicable)

--- OUTPUT FORMAT ---
Please structure your response exactly as follows:

Mastery Level: <Your chosen level>

Rationale: <Detailed explanation with specific evidence from the exam records>

Suggestions: <Actionable recommendations for the student>
"""
    
    return system_prompt, user_prompt

def parse_llm_response(text):
    """
    ä»LLMçš„å“åº”æ–‡æœ¬ä¸­è§£æå‡ºç»“æ„åŒ–æ•°æ®ã€‚
    """
    parsed_data = {'Mastery Level': 'N/A', 'Rationale': 'N/A', 'Suggestions': 'N/A'}
    if not isinstance(text, str):
        return parsed_data

    current_key = None
    for line in text.strip().split('\n'):
        line = line.strip()
        if line.startswith("Mastery Level:"):
            current_key = 'Mastery Level'
            parsed_data[current_key] = line.split(":", 1)[1].strip()
        elif line.startswith("Rationale:"):
            current_key = 'Rationale'
            parsed_data[current_key] = line.split(":", 1)[1].strip()
        elif line.startswith("Suggestions:"):
            current_key = 'Suggestions'
            parsed_data[current_key] = line.split(":", 1)[1].strip()
        elif current_key and current_key in parsed_data:
            # è¿½åŠ å¤šè¡Œå†…å®¹åˆ°ä¸Šä¸€ä¸ªé”®
            parsed_data[current_key] += " " + line
            
    return parsed_data

async def prepare_student_requests(student_id, full_log_df, kcs_df, kc_graph, question_choices_df, include_behavioral_data=True, processed_pairs=None):
    """
    å‡†å¤‡å•ä¸ªå­¦ç”Ÿçš„æ‰€æœ‰çŸ¥è¯†ç‚¹è¯„ä¼°è¯·æ±‚ï¼ˆä¸å®é™…å‘é€ï¼‰
    
    å‚æ•°:
        include_behavioral_data: æ˜¯å¦åŒ…å«è¡Œä¸ºæ•°æ®ï¼ˆå­—æ®µ6-12ï¼‰
        processed_pairs: å·²å¤„ç†çš„ (student_id, kc_name) é›†åˆï¼Œç”¨äºè·³è¿‡å·²è¯„ä¼°çš„è®°å½•
    
    ğŸ”¥ é‡è¦ï¼šæ­¤å‡½æ•°ç°åœ¨åªä½¿ç”¨è®­ç»ƒé›†æ•°æ®è¿›è¡ŒæŒæ¡åº¦è¯„ä¼°ï¼Œé¿å…æ•°æ®æ³„éœ²
    
    è¿”å›: (student_id, llm_requestsåˆ—è¡¨)
    """
    if processed_pairs is None:
        processed_pairs = set()
    
    print(f"\n{'='*60}")
    print(f"ğŸ“ å­¦ç”Ÿ {student_id} - å‡†å¤‡è¯·æ±‚")
    print(f"{'='*60}")
    
    student_records = full_log_df[full_log_df['student_id'] == student_id]
    
    # ğŸ”¥ å…³é”®ä¿®å¤ï¼šåªä½¿ç”¨è®­ç»ƒé›†æ•°æ®ï¼Œä¸run_experiment.pyä¿æŒä¸€è‡´
    if len(student_records) > 10:  # ç¡®ä¿æœ‰è¶³å¤Ÿæ•°æ®è¿›è¡Œåˆ’åˆ†
        train_records, _ = train_test_split(
            student_records, 
            test_size=0.1, 
            random_state=42, 
            shuffle=True
        )
        student_records = train_records
        
    practiced_kcs = student_records['kc_name'].unique()
    
    print(f"ğŸ“Š å­¦ç”Ÿ {student_id} ç»Ÿè®¡:")
    print(f"   â€¢ ç»ƒä¹ è¿‡çš„çŸ¥è¯†ç‚¹æ•°: {len(practiced_kcs)}")
    print(f"   â€¢ è®­ç»ƒé›†è®°å½•æ•°: {len(student_records)}")
    
    kc_info_map = kcs_df.set_index('name')['description'].to_dict()

    llm_requests = []
    skipped_count = 0
    for kc_name in practiced_kcs:
        # æ£€æŸ¥æ˜¯å¦å·²å¤„ç†è¿‡è¯¥å­¦ç”Ÿ-çŸ¥è¯†ç‚¹å¯¹
        if (student_id, kc_name) in processed_pairs:
            skipped_count += 1
            continue
        
        # ğŸ”¥ å…³é”®ä¿®å¤ï¼šä¼ é€’use_train_only=Trueç¡®ä¿åªä½¿ç”¨è®­ç»ƒé›†æ•°æ®
        trajectory_df = get_student_kc_trajectory(full_log_df, student_id, kc_name, use_train_only=True)
        
        if trajectory_df.empty:
            continue

        kc_description = kc_info_map.get(kc_name, "No description available.")
        prerequisites = [pre for pre, post in kc_graph if post == kc_name]
        system_prompt, user_prompt = build_mastery_prompt(student_id, kc_name, kc_description, trajectory_df, question_choices_df, prerequisites, include_behavioral_data)
        
        llm_requests.append({
            "system_prompt": system_prompt,
            "user_prompt": user_prompt,
            "model_name": MODEL_NAME,
            "context": {"student_id": student_id, "kc_name": kc_name}
        })
    
    if skipped_count > 0:
        print(f"â­ï¸  è·³è¿‡å·²å¤„ç†çš„çŸ¥è¯†ç‚¹: {skipped_count} ä¸ª")
    print(f"âœ… å­¦ç”Ÿ {student_id} å‡†å¤‡äº† {len(llm_requests)} ä¸ªæ–°è¯·æ±‚")
    print(f"{'='*60}\n")
    
    return (student_id, llm_requests)


# --- 4. å®éªŒä¸»å¾ªç¯ ---

# ğŸ”¥ æ•°æ®æ³„éœ²ä¿®å¤è¯´æ˜ï¼š
# 1. æ·»åŠ äº†train_test_splitå¯¼å…¥
# 2. ä¿®æ”¹get_student_kc_trajectoryå‡½æ•°ï¼Œæ·»åŠ use_train_onlyå‚æ•°ï¼Œé»˜è®¤åªä½¿ç”¨è®­ç»ƒé›†æ•°æ®
# 3. ä¿®æ”¹run_mastery_assessment_for_studentå‡½æ•°ï¼Œç¡®ä¿æ•°æ®åˆ’åˆ†ä¸run_experiment.pyä¸€è‡´
# 4. ä½¿ç”¨ç›¸åŒçš„å‚æ•°ï¼štest_size=0.1, random_state=42, shuffle=True
# 5. è¿™æ ·ç¡®ä¿æŒæ¡åº¦è¯„ä¼°åªåŸºäºè®­ç»ƒé›†ï¼Œé¿å…äº†æ•°æ®æ³„éœ²åˆ°æµ‹è¯•é›†

async def main():
    parser = argparse.ArgumentParser(description="è¿è¡Œå­¦ç”ŸçŸ¥è¯†ç‚¹æŒæ¡åº¦è¯„ä¼°ã€‚")
    parser.add_argument("--students", type=int, default=10, help="è¦è¯„ä¼°çš„å­¦ç”Ÿæ•°é‡ã€‚è®¾ç½®ä¸º-1åˆ™è¿è¡Œæ‰€æœ‰å­¦ç”Ÿã€‚é»˜è®¤10ä¸ªå­¦ç”Ÿã€‚")
    parser.add_argument("--student-ids", type=str, default=None, help="ä»¥é€—å·åˆ†éš”çš„å­¦ç”ŸIDåˆ—è¡¨ï¼ŒæŒ‡å®šæ—¶ä¼˜å…ˆä½¿ç”¨ã€‚")
    parser.add_argument("--concurrency", type=int, default=30, help="LLM è¯·æ±‚å¹¶å‘æ•°é‡ï¼ˆæ¨¡å‹è¯·æ±‚ç»´åº¦ï¼‰ã€‚é»˜è®¤30ã€‚")
    parser.add_argument("--mode", type=str, default="both", choices=["full", "minimal", "both"], help="è¯„ä¼°æ¨¡å¼ï¼šfull(å®Œæ•´è¡Œä¸ºæ•°æ®), minimal(ç²¾ç®€ç‰ˆ), both(ä¸¤ç§éƒ½è¿è¡Œ)ã€‚é»˜è®¤bothã€‚")
    parser.add_argument("--spread-duration", type=int, default=60, help="å°†æ‰€æœ‰è¯·æ±‚å‡åŒ€åˆ†æ•£åˆ°æŒ‡å®šç§’æ•°å†…ï¼Œå®ç°å‰Šå³°å¡«è°·ã€‚é»˜è®¤60ç§’ã€‚è®¾ç½®ä¸º0åˆ™ç¦ç”¨ã€‚")
    parser.add_argument("--model", type=str, default="gpt-3.5-turbo", help="ä½¿ç”¨çš„LLMæ¨¡å‹åç§°ã€‚é»˜è®¤gpt-3.5-turboã€‚")
    args = parser.parse_args()
    
    # è®¾ç½®å…¨å±€æ¨¡å‹åç§°
    global MODEL_NAME
    MODEL_NAME = args.model

    if not user_sys_call_with_model:
        print("LLMå·¥å…·æ¨¡å—æœªèƒ½åŠ è½½ï¼Œè¯·æ£€æŸ¥é¡¹ç›®è·¯å¾„ã€‚è„šæœ¬é€€å‡ºã€‚")
        sys.exit(1)

    # 1. æ•°æ®åŠ è½½
    full_log_df, kcs_df, kc_graph, question_choices_df = load_and_prepare_data(PROJECT_ROOT)

    # 2. é€‰å–å­¦ç”Ÿ
    if args.student_ids:
        available_ids = {str(sid) for sid in full_log_df['student_id'].unique()}
        specified_ids = []
        for sid in args.student_ids.split(','):
            sid = sid.strip()
            if not sid:
                continue
            if sid not in available_ids:
                print(f"è­¦å‘Š: æŒ‡å®šå­¦ç”ŸID {sid} ä¸å­˜åœ¨äºæ•°æ®é›†ä¸­ï¼Œå·²å¿½ç•¥ã€‚")
                continue
            specified_ids.append(int(sid))
        if not specified_ids:
            print("æŒ‡å®šçš„å­¦ç”ŸIDåˆ—è¡¨ä¸æ•°æ®é›†ä¸åŒ¹é…ï¼Œé€€å‡ºã€‚")
            return
        student_ids = specified_ids
    else:
        student_ids = sorted(full_log_df['student_id'].unique())
        if args.students != -1:
            student_ids = student_ids[:min(args.students, len(student_ids))]
    
    print(f"\nå°†å¯¹ {len(student_ids)} åå­¦ç”Ÿè¿›è¡Œè¯„ä¼°...")
    print(f"è¯„ä¼°æ¨¡å¼: {args.mode}")
    print(f"ä½¿ç”¨æ¨¡å‹: {MODEL_NAME}")
    if args.spread_duration > 0:
        print(f"å‰Šå³°å¡«è°·: å¼€å¯ ({args.spread_duration}ç§’/å­¦ç”Ÿ)")
    
    # ç”Ÿæˆå¸¦æ¨¡å‹åç§°çš„æ–‡ä»¶åç¼€
    model_suffix = MODEL_NAME.replace('/', '_').replace('.', '_')

    # å‡†å¤‡è¾“å‡ºç›®å½•
    output_dir = os.path.join(os.path.dirname(__file__), '../results')
    os.makedirs(output_dir, exist_ok=True)
    
    # å®šä¹‰è¿è¡Œæ¨¡å¼
    modes_to_run = []
    if args.mode == "both":
        modes_to_run = [("full", True), ("minimal", False)]
    elif args.mode == "full":
        modes_to_run = [("full", True)]
    else:  # minimal
        modes_to_run = [("minimal", False)]
    
    # å¯¹æ¯ç§æ¨¡å¼è¿è¡Œè¯„ä¼°
    for mode_name, include_behavioral in modes_to_run:
        print("\n" + "="*80)
        print(f"è¿è¡Œè¯„ä¼°æ¨¡å¼: {mode_name.upper()} ({'åŒ…å«è¡Œä¸ºæ•°æ®' if include_behavioral else 'ä»…åŸºç¡€æ•°æ®'})".center(80))
        print("="*80)
        
        # å‡†å¤‡é”™è¯¯æ—¥å¿—æ–‡ä»¶ï¼ˆåªè®°å½•å¤±è´¥çš„è¯·æ±‚ï¼‰
        error_log_path = os.path.join(output_dir, f'assessment_errors_{mode_name}_{model_suffix}.txt')
        print(f"å¤±è´¥çš„è¯·æ±‚å°†è®°å½•åˆ°: {error_log_path}")

        # æ–‡ä»¶è·¯å¾„
        results_path = os.path.join(output_dir, f'mastery_assessment_results_{mode_name}_{model_suffix}.csv')
        manifest_path = os.path.join(output_dir, f'mastery_assessment_manifest_{mode_name}_{model_suffix}.csv')
        
        # ğŸ”¥ æ–°é€»è¾‘ï¼šæ£€æŸ¥è¯·æ±‚æ¸…å•æ˜¯å¦å­˜åœ¨
        print(f"\né˜¶æ®µ2: æ£€æŸ¥/ç”Ÿæˆè¯·æ±‚æ¸…å•")
        
        manifest_df, all_requests = load_request_manifest(manifest_path, results_path)
        
        if manifest_df is None:
            # æ¸…å•ä¸å­˜åœ¨ï¼Œç”Ÿæˆæ–°æ¸…å•
            print(f"   â„¹ï¸  æœªæ‰¾åˆ°è¯·æ±‚æ¸…å•ï¼Œå¼€å§‹ç”Ÿæˆ...")
            manifest_df = generate_request_manifest(
                student_ids, full_log_df, kcs_df, kc_graph, 
                question_choices_df, include_behavioral, manifest_path
            )
            # é‡æ–°åŠ è½½ä»¥è·å–å¾…å¤„ç†è¯·æ±‚
            manifest_df, all_requests = load_request_manifest(manifest_path, results_path)
        
        if len(all_requests) == 0:
            print(f"\nâœ… æ‰€æœ‰è¯„ä¼°å·²å®Œæˆï¼Œæ— éœ€ç»§ç»­å¤„ç†")
            continue
        
        print(f"\nğŸš€ å‡†å¤‡å‘é€ {len(all_requests)} ä¸ªå¾…å¤„ç†è¯·æ±‚")
        
        # 4. ç»Ÿä¸€æ‰¹é‡å‘é€æ‰€æœ‰è¯·æ±‚ï¼ˆLLM è¯·æ±‚ç»´åº¦å¹¶å‘ + å‰Šå³°å¡«è°· + æ‰¹é‡ä¿å­˜ï¼‰
        print(f"\né˜¶æ®µ3: æ‰¹é‡å‘é€è¯·æ±‚å¹¶å®æ—¶ä¿å­˜")
        print(f"   âš¡ å¹¶å‘é™åˆ¶: {args.concurrency} ä¸ªè¯·æ±‚")
        print(f"   ğŸŒŠ å‰Šå³°å¡«è°·: {'å¼€å¯' if args.spread_duration > 0 else 'å…³é—­'} ({args.spread_duration}ç§’)" if args.spread_duration > 0 else "   ğŸŒŠ å‰Šå³°å¡«è°·: å…³é—­")
        print(f"   ğŸ’¾ æ‰¹é‡ä¿å­˜: æ¯ 30 ä¸ªå­¦ç”Ÿä¿å­˜ä¸€æ¬¡")
        
        llm_results = []
        if all_requests:
            # ğŸ”¥ ä½¿ç”¨ Semaphore æ§åˆ¶çœŸæ­£çš„å¹¶å‘æ•°é‡
            semaphore = asyncio.Semaphore(args.concurrency)
            
            # ğŸ”¥ å®æ—¶ä¿å­˜ï¼šæ‰¹é‡ç¼“å­˜å’Œå®šæ—¶ä¿å­˜
            batch_results = []
            batch_lock = asyncio.Lock()
            total_saved = 0
            is_first_batch = not os.path.exists(results_path)
            BATCH_SIZE = 100  # æ¯100ä¸ªç»“æœä¿å­˜ä¸€æ¬¡
            
            # è¿›åº¦è·Ÿè¸ª
            completed_count = 0
            total_count = len(all_requests)
            lock = asyncio.Lock()
            
            # ä½¿ç”¨å‰Šå³°å¡«è°·æ¨¡å¼ï¼ˆé»˜è®¤å¼€å¯ï¼‰
            if args.spread_duration > 0:
                delay_per_request = args.spread_duration / len(all_requests)
                print(f"   â±ï¸  è¯·æ±‚é—´éš”: {delay_per_request:.2f} ç§’")
                
                async def delayed_request(req, index, delay):
                    """æ·»åŠ å»¶è¿Ÿåæ‰§è¡Œè¯·æ±‚ï¼ˆå¸¦å¹¶å‘æ§åˆ¶ + å®æ—¶ä¿å­˜ï¼‰"""
                    nonlocal completed_count, total_saved, is_first_batch
                    
                    context = req.get('context', {})
                    student_id = context.get('student_id', 'Unknown')
                    kc_name = context.get('kc_name', 'Unknown')
                    
                    # å…ˆå»¶è¿Ÿï¼ˆå‰Šå³°å¡«è°·ï¼‰
                    if delay > 0:
                        await asyncio.sleep(delay)
                    
                    # å†é€šè¿‡ä¿¡å·é‡æ§åˆ¶å¹¶å‘
                    async with semaphore:
                        try:
                            result = await user_sys_call_with_model(
                                user_prompt=req.get('user_prompt', ''),
                                system_prompt=req.get('system_prompt', ''),
                                model_name=req.get('model_name', MODEL_NAME)
                            )
                            raw_resp = result
                            error = None
                        except Exception as e:
                            error_msg = str(e) if str(e) else f"{type(e).__name__}: (ç©ºé”™è¯¯ä¿¡æ¯)"
                            raw_resp = f"LLM_CALL_FAILED: {error_msg}"
                            error = error_msg
                        
                        # ğŸ”¥ ç«‹å³å¤„ç†å¹¶ä¿å­˜ç»“æœ
                        parsed_resp = parse_llm_response(raw_resp)
                        result_record = {
                            'student_id': context['student_id'],
                            'kc_name': context['kc_name'],
                            'mastery_level': parsed_resp['Mastery Level'],
                            'rationale': parsed_resp['Rationale'],
                            'suggestions': parsed_resp['Suggestions'],
                            'llm_raw_response': raw_resp,
                            'prompt_system': req['system_prompt'],
                            'prompt_user': req['user_prompt']
                        }
                        
                        # åŠ å…¥æ‰¹æ¬¡ç¼“å­˜
                        async with batch_lock:
                            batch_results.append(result_record)
                            
                            # è¾¾åˆ°æ‰¹æ¬¡å¤§å°ï¼Œç«‹å³ä¿å­˜
                            if len(batch_results) >= BATCH_SIZE:
                                save_results_batch(batch_results, results_path, is_first_batch)
                                total_saved += len(batch_results)
                                print(f"   ğŸ’¾ å·²ä¿å­˜ {len(batch_results)} æ¡ç»“æœ | ç´¯è®¡: {total_saved}/{total_count}")
                                batch_results.clear()
                                is_first_batch = False
                        
                        # æ›´æ–°è¿›åº¦
                        async with lock:
                            completed_count += 1
                            if error:
                                error_detail = error[:100] if len(error) > 100 else error
                                print(f"   âŒ [{index+1}/{total_count}] å¤±è´¥ - å­¦ç”Ÿ{student_id} KC='{kc_name}' | {completed_count}/{total_count} ({completed_count*100//total_count}%)")
                            else:
                                if completed_count % 50 == 0 or completed_count == total_count:
                                    print(f"   âœ… [{index+1}/{total_count}] æˆåŠŸ | {completed_count}/{total_count} ({completed_count*100//total_count}%)")
                        
                        return {"index": index, "result": raw_resp, "error": error}
                
                # åˆ›å»ºæ‰€æœ‰å»¶è¿Ÿä»»åŠ¡
                tasks = []
                for i, req in enumerate(all_requests):
                    delay = i * delay_per_request
                    tasks.append(delayed_request(req, i, delay))
                
                # å¹¶å‘æ‰§è¡Œæ‰€æœ‰ä»»åŠ¡
                results = await asyncio.gather(*tasks, return_exceptions=True)
                
                # ä¿å­˜å‰©ä½™ç»“æœ
                async with batch_lock:
                    if batch_results:
                        save_results_batch(batch_results, results_path, is_first_batch)
                        total_saved += len(batch_results)
                        print(f"   ğŸ’¾ ä¿å­˜æœ€åä¸€æ‰¹ {len(batch_results)} æ¡ç»“æœ | æ€»è®¡: {total_saved}/{total_count}")
                        batch_results.clear()
                
                # å¤„ç†å¼‚å¸¸ç»“æœ
                for i, result in enumerate(results):
                    if isinstance(result, Exception):
                        llm_results.append({"index": i, "result": None, "error": str(result)})
                    else:
                        llm_results.append(result)
            else:
                # ä¸ä½¿ç”¨å‰Šå³°å¡«è°·ï¼Œç›´æ¥å¹¶å‘ï¼ˆæ­¤æ¨¡å¼ä¸‹ä¹Ÿéœ€è¦å¹¶å‘æ§åˆ¶ + å®æ—¶ä¿å­˜ï¼‰
                async def controlled_request(req, index):
                    """å¸¦å¹¶å‘æ§åˆ¶çš„è¯·æ±‚ï¼ˆå®æ—¶ä¿å­˜ï¼‰"""
                    nonlocal completed_count, total_saved, is_first_batch
                    
                    context = req.get('context', {})
                    student_id = context.get('student_id', 'Unknown')
                    kc_name = context.get('kc_name', 'Unknown')
                    
                    async with semaphore:
                        try:
                            result = await user_sys_call_with_model(
                                user_prompt=req.get('user_prompt', ''),
                                system_prompt=req.get('system_prompt', ''),
                                model_name=req.get('model_name', MODEL_NAME)
                            )
                            raw_resp = result
                            error = None
                        except Exception as e:
                            error_msg = str(e) if str(e) else f"{type(e).__name__}: (ç©ºé”™è¯¯ä¿¡æ¯)"
                            raw_resp = f"LLM_CALL_FAILED: {error_msg}"
                            error = error_msg
                        
                        # ğŸ”¥ ç«‹å³å¤„ç†å¹¶ä¿å­˜ç»“æœ
                        parsed_resp = parse_llm_response(raw_resp)
                        result_record = {
                            'student_id': context['student_id'],
                            'kc_name': context['kc_name'],
                            'mastery_level': parsed_resp['Mastery Level'],
                            'rationale': parsed_resp['Rationale'],
                            'suggestions': parsed_resp['Suggestions'],
                            'llm_raw_response': raw_resp,
                            'prompt_system': req['system_prompt'],
                            'prompt_user': req['user_prompt']
                        }
                        
                        # åŠ å…¥æ‰¹æ¬¡ç¼“å­˜
                        async with batch_lock:
                            batch_results.append(result_record)
                            
                            # è¾¾åˆ°æ‰¹æ¬¡å¤§å°ï¼Œç«‹å³ä¿å­˜
                            if len(batch_results) >= BATCH_SIZE:
                                save_results_batch(batch_results, results_path, is_first_batch)
                                total_saved += len(batch_results)
                                print(f"   ğŸ’¾ å·²ä¿å­˜ {len(batch_results)} æ¡ç»“æœ | ç´¯è®¡: {total_saved}/{total_count}")
                                batch_results.clear()
                                is_first_batch = False
                        
                        # æ›´æ–°è¿›åº¦
                        async with lock:
                            completed_count += 1
                            if error:
                                error_detail = error[:100] if len(error) > 100 else error
                                print(f"   âŒ [{index+1}/{total_count}] å¤±è´¥ - å­¦ç”Ÿ{student_id} KC='{kc_name}' | {completed_count}/{total_count} ({completed_count*100//total_count}%)")
                            else:
                                if completed_count % 50 == 0 or completed_count == total_count:
                                    print(f"   âœ… [{index+1}/{total_count}] æˆåŠŸ | {completed_count}/{total_count} ({completed_count*100//total_count}%)")
                        
                        return {"index": index, "result": raw_resp, "error": error}
                
                tasks = [controlled_request(req, i) for i, req in enumerate(all_requests)]
                results = await asyncio.gather(*tasks, return_exceptions=True)
                
                # ä¿å­˜å‰©ä½™ç»“æœ
                async with batch_lock:
                    if batch_results:
                        save_results_batch(batch_results, results_path, is_first_batch)
                        total_saved += len(batch_results)
                        print(f"   ğŸ’¾ ä¿å­˜æœ€åä¸€æ‰¹ {len(batch_results)} æ¡ç»“æœ | æ€»è®¡: {total_saved}/{total_count}")
                        batch_results.clear()
                
                for i, result in enumerate(results):
                    if isinstance(result, Exception):
                        llm_results.append({"index": i, "result": None, "error": str(result)})
                    else:
                        llm_results.append(result)
        
        # 5. å¤„ç†æ‰€æœ‰ç»“æœï¼ˆæ‰¹é‡ä¿å­˜æ¨¡å¼ï¼šæŒ‰å­¦ç”Ÿåˆ†ç»„ï¼‰
        print(f"\né˜¶æ®µ4: å¤„ç†è¯„ä¼°ç»“æœï¼ˆè¾¹å¤„ç†è¾¹ä¿å­˜ï¼‰")
        
        # ç»Ÿè®¡æˆåŠŸå’Œå¤±è´¥
        success_count = sum(1 for r in llm_results if r.get('error') is None)
        fail_count = len(llm_results) - success_count
        
        print(f"ğŸ“ˆ æ€»ä½“è¯·æ±‚å®Œæˆ:")
        print(f"   âœ… æˆåŠŸ: {success_count}/{len(llm_results)}")
        print(f"   âŒ å¤±è´¥: {fail_count}/{len(llm_results)}")
        
        # ğŸ”¥ æ‰¹é‡ä¿å­˜é€»è¾‘ï¼šæ¯30ä¸ªå­¦ç”Ÿä¿å­˜ä¸€æ¬¡
        STUDENTS_PER_BATCH = 30
        batch_results = []
        total_saved = 0
        is_first_batch = not os.path.exists(results_path)
        
        # æŒ‰å­¦ç”Ÿåˆ†ç»„ç»“æœ
        student_results_map = {}  # {student_id: [results]}
        
        for i, result in enumerate(tqdm(llm_results, desc="å¤„ç†ç»“æœ")):
            raw_resp = result.get('result')
            error = result.get('error')
            original_request = all_requests[i]
            context = original_request['context']
            student_id = context['student_id']
            
            if error:
                raw_resp = f"LLM_CALL_FAILED: {error}"
                
                # åªè®°å½•å¤±è´¥çš„è¯·æ±‚
                try:
                    with open(error_log_path, "a", encoding="utf-8") as f:
                        f.write(f"--- å¤±è´¥è¯·æ±‚ - å­¦ç”Ÿ {context['student_id']}, çŸ¥è¯†ç‚¹ '{context['kc_name']}' ---\n")
                        f.write("--- SYSTEM PROMPT ---\n")
                        f.write(original_request['system_prompt'] + "\n\n")
                        f.write("--- USER PROMPT ---\n")
                        f.write(original_request['user_prompt'] + "\n\n")
                        f.write("--- é”™è¯¯ä¿¡æ¯ ---\n")
                        f.write(str(error) + "\n")
                        f.write("="*80 + "\n\n")
                except Exception as e:
                    print(f"å†™å…¥é”™è¯¯æ—¥å¿—æ—¶å‡ºé”™: {e}")

            parsed_resp = parse_llm_response(raw_resp)
            result_record = {
                'student_id': context['student_id'],
                'kc_name': context['kc_name'],
                'mastery_level': parsed_resp['Mastery Level'],
                'rationale': parsed_resp['Rationale'],
                'suggestions': parsed_resp['Suggestions'],
                'llm_raw_response': raw_resp,
                'prompt_system': original_request['system_prompt'],
                'prompt_user': original_request['user_prompt']
            }
            
            # æŒ‰å­¦ç”Ÿåˆ†ç»„
            if student_id not in student_results_map:
                student_results_map[student_id] = []
            student_results_map[student_id].append(result_record)
            
            batch_results.append(result_record)
            
            # ğŸ”¥ æ¯30ä¸ªå­¦ç”Ÿä¿å­˜ä¸€æ¬¡
            if len(student_results_map) >= STUDENTS_PER_BATCH:
                save_results_batch(batch_results, results_path, is_first_batch)
                total_saved += len(batch_results)
                completed_students = len(student_results_map)
                print(f"   ğŸ’¾ å·²ä¿å­˜ {completed_students} ä¸ªå­¦ç”Ÿçš„è¯„ä¼°ç»“æœ")
                print(f"   ğŸ“Š ç´¯è®¡å·²ä¿å­˜: {total_saved}/{len(llm_results)} æ¡è®°å½•")
                batch_results = []
                student_results_map = {}
                is_first_batch = False
        
        # ä¿å­˜å‰©ä½™ç»“æœ
        if batch_results:
            save_results_batch(batch_results, results_path, is_first_batch)
            total_saved += len(batch_results)
            remaining_students = len(student_results_map)
            if remaining_students > 0:
                print(f"   ğŸ’¾ å·²ä¿å­˜å‰©ä½™ {remaining_students} ä¸ªå­¦ç”Ÿçš„è¯„ä¼°ç»“æœ")
            print(f"   ğŸ“Š ç´¯è®¡å·²ä¿å­˜: {total_saved}/{len(llm_results)} æ¡è®°å½•")
        
        print(f"\nâœ… {mode_name.upper()} è¯„ä¼°å®Œæˆ")
        print(f"   ğŸ“ ç»“æœæ–‡ä»¶: {results_path}")
        print(f"   ğŸ“Š æœ¬æ¬¡å¤„ç†: {total_saved} æ¡")
        print(f"   ğŸ“‹ è¯·æ±‚æ¸…å•: {manifest_path}")
        print(f"   ğŸ“ é”™è¯¯æ—¥å¿—: {error_log_path}")
    
    print("\n" + "="*80)
    print("ğŸ‰ æ‰€æœ‰è¯„ä¼°æ¨¡å¼è¿è¡Œå®Œæˆï¼".center(80))
    print("="*80)


if __name__ == "__main__":
    asyncio.run(main())
